training: False
_parameters: OrderedDict()
_buffers: OrderedDict()
_non_persistent_buffers_set: set()
_backward_pre_hooks: OrderedDict()
_backward_hooks: OrderedDict()
_is_full_backward_hook: None
_forward_hooks: OrderedDict()
_forward_hooks_with_kwargs: OrderedDict()
_forward_hooks_always_called: OrderedDict()
_forward_pre_hooks: OrderedDict()
_forward_pre_hooks_with_kwargs: OrderedDict()
_state_dict_hooks: OrderedDict()
_state_dict_pre_hooks: OrderedDict()
_load_state_dict_pre_hooks: OrderedDict()
_load_state_dict_post_hooks: OrderedDict()
_modules: OrderedDict({'features': Sequential(
  (0): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (1): Attention_ConvNN_Attn_Branching(
    (branch1): Sequential(
      (0): Attention2d(
        (shuffle_layer): PixelShuffle(upscale_factor=2)
        (unshuffle_layer): PixelUnshuffle(downscale_factor=2)
        (Attention1d): Attention1d(
          (shuffle_layer): PixelShuffle1D()
          (unshuffle_layer): PixelUnshuffle1D()
          (multi_head_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (conv1x1): Conv1d(12, 16, kernel_size=(1,), stride=(1,))
        )
        (flatten): Flatten(start_dim=2, end_dim=-1)
        (pointwise_conv): Conv2d(4, 2, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
    )
    (branch2): Sequential(
      (0): Conv2d_NN_Attn(
        (shuffle_layer): PixelShuffle(upscale_factor=2)
        (unshuffle_layer): PixelUnshuffle(downscale_factor=2)
        (conv1d_layer): Conv1d(12, 16, kernel_size=(9,), stride=(9,))
        (flatten): Flatten(start_dim=2, end_dim=-1)
        (w_q): Linear(in_features=64, out_features=64, bias=False)
        (w_k): Linear(in_features=256, out_features=256, bias=False)
        (w_v): Linear(in_features=256, out_features=256, bias=False)
        (w_o): Linear(in_features=256, out_features=256, bias=False)
        (pointwise_conv): Conv2d(4, 2, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
    )
  )
  (2): ReLU(inplace=True)
  (3): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (4): Attention_ConvNN_Attn_Branching(
    (branch1): Sequential(
      (0): Attention2d(
        (shuffle_layer): PixelShuffle(upscale_factor=2)
        (unshuffle_layer): PixelUnshuffle(downscale_factor=2)
        (Attention1d): Attention1d(
          (shuffle_layer): PixelShuffle1D()
          (unshuffle_layer): PixelUnshuffle1D()
          (multi_head_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)
          )
          (conv1x1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (flatten): Flatten(start_dim=2, end_dim=-1)
        (pointwise_conv): Conv2d(8, 6, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
    )
    (branch2): Sequential(
      (0): Conv2d_NN_Attn(
        (shuffle_layer): PixelShuffle(upscale_factor=2)
        (unshuffle_layer): PixelUnshuffle(downscale_factor=2)
        (conv1d_layer): Conv1d(32, 32, kernel_size=(9,), stride=(9,))
        (flatten): Flatten(start_dim=2, end_dim=-1)
        (w_q): Linear(in_features=64, out_features=64, bias=False)
        (w_k): Linear(in_features=256, out_features=256, bias=False)
        (w_v): Linear(in_features=256, out_features=256, bias=False)
        (w_o): Linear(in_features=256, out_features=256, bias=False)
        (pointwise_conv): Conv2d(8, 6, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
    )
  )
  (5): ReLU(inplace=True)
  (6): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (7): Attention_ConvNN_Attn_Branching(
    (branch1): Sequential(
      (0): Attention2d(
        (shuffle_layer): PixelShuffle(upscale_factor=2)
        (unshuffle_layer): PixelUnshuffle(downscale_factor=2)
        (Attention1d): Attention1d(
          (shuffle_layer): PixelShuffle1D()
          (unshuffle_layer): PixelUnshuffle1D()
          (multi_head_attention): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (conv1x1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (flatten): Flatten(start_dim=2, end_dim=-1)
        (pointwise_conv): Conv2d(16, 14, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
    )
    (branch2): Sequential(
      (0): Conv2d_NN_Attn(
        (shuffle_layer): PixelShuffle(upscale_factor=2)
        (unshuffle_layer): PixelUnshuffle(downscale_factor=2)
        (conv1d_layer): Conv1d(64, 64, kernel_size=(9,), stride=(9,))
        (flatten): Flatten(start_dim=2, end_dim=-1)
        (w_q): Linear(in_features=64, out_features=64, bias=False)
        (w_k): Linear(in_features=256, out_features=256, bias=False)
        (w_v): Linear(in_features=256, out_features=256, bias=False)
        (w_o): Linear(in_features=256, out_features=256, bias=False)
        (pointwise_conv): Conv2d(16, 14, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ReLU()
    )
  )
  (8): ReLU(inplace=True)
), 'pool': AdaptiveAvgPool2d(output_size=(1, 1)), 'flatten': Flatten(start_dim=1, end_dim=-1), 'classifier': Sequential(
  (0): Dropout(p=0.1, inplace=False)
  (1): Linear(in_features=32, out_features=10, bias=True)
)})
args: Namespace(layer='Attention/ConvNN_Attn', num_layers=3, channels=[8, 16, 32], K=9, kernel_size=3, sampling_type='random', num_samples=64, sample_padding=0, num_heads=4, shuffle_pattern='BA', shuffle_scale=2, magnitude_type='similarity', coordinate_encoding=False, dataset='cifar10', data_path='./Data', batch_size=64, num_epochs=50, use_amp=False, clip_grad_norm=None, criterion='CrossEntropy', optimizer='adamw', momentum=0.9, weight_decay=1e-06, lr=0.001, lr_step=20, lr_gamma=0.1, scheduler='step', device='cuda', seed=0, output_dir='./Output/ACN/B_Attention_ConvNNAttention_Random', resize=False, num_classes=10, img_size=(3, 32, 32), total_params=678342, trainable_params=678342)
model: All Convolutional Network
name: All Convolutional Network Attention/ConvNN_Attn
