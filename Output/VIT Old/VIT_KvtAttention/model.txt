training: False
_parameters: OrderedDict()
_buffers: OrderedDict()
_non_persistent_buffers_set: set()
_backward_pre_hooks: OrderedDict()
_backward_hooks: OrderedDict()
_is_full_backward_hook: None
_forward_hooks: OrderedDict()
_forward_hooks_with_kwargs: OrderedDict()
_forward_hooks_always_called: OrderedDict()
_forward_pre_hooks: OrderedDict()
_forward_pre_hooks_with_kwargs: OrderedDict()
_state_dict_hooks: OrderedDict()
_state_dict_pre_hooks: OrderedDict()
_load_state_dict_pre_hooks: OrderedDict()
_load_state_dict_post_hooks: OrderedDict()
_modules: OrderedDict({'patch_embedding': PatchEmbedding(
  (linear_projection): Conv2d(3, 8, kernel_size=(16, 16), stride=(16, 16))
  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
  (flatten): Flatten(start_dim=2, end_dim=-1)
), 'positional_encoding': PositionalEncoding(), 'transformer_encoder': Sequential(
  (0): TransformerEncoder(
    (attention): MultiHeadKvtAttention(
      (qkv): Linear(in_features=8, out_features=24, bias=False)
      (attn_drop): Dropout(p=0.1, inplace=False)
      (proj): Linear(in_features=8, out_features=8, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=32, out_features=8, bias=True)
    )
  )
  (1): TransformerEncoder(
    (attention): MultiHeadKvtAttention(
      (qkv): Linear(in_features=8, out_features=24, bias=False)
      (attn_drop): Dropout(p=0.1, inplace=False)
      (proj): Linear(in_features=8, out_features=8, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=32, out_features=8, bias=True)
    )
  )
  (2): TransformerEncoder(
    (attention): MultiHeadKvtAttention(
      (qkv): Linear(in_features=8, out_features=24, bias=False)
      (attn_drop): Dropout(p=0.1, inplace=False)
      (proj): Linear(in_features=8, out_features=8, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
    (mlp): Sequential(
      (0): Linear(in_features=8, out_features=32, bias=True)
      (1): GELU(approximate='none')
      (2): Dropout(p=0.1, inplace=False)
      (3): Linear(in_features=32, out_features=8, bias=True)
    )
  )
), 'classifier': Sequential(
  (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
  (1): Linear(in_features=8, out_features=10, bias=True)
)})
args: Namespace(layer='KvtAttention', patch_size=16, num_layers=3, num_heads=4, d_model=8, dropout=0.1, attention_dropout=0.1, K=9, kernel_size=3, num_samples=0, magnitude_type='similarity', dataset='cifar10', data_path='./Data', batch_size=64, num_epochs=10, use_amp=False, clip_grad_norm=None, criterion='CrossEntropy', optimizer='adamw', momentum=0.9, weight_decay=1e-06, lr=0.001, lr_step=20, lr_gamma=0.1, scheduler='step', device='cuda', seed=0, output_dir='./Output/VIT/VIT_KvtAttention', resize=True, num_classes=10, img_size=(3, 224, 224), model='VIT', total_params=8826, trainable_params=8826)
model: VIT
d_model: 8
img_size: (224, 224)
n_classes: 10
n_heads: 4
patch_size: (16, 16)
n_channels: 3
n_layers: 3
n_patches: 196
dropout: 0.1
max_seq_length: 197
device: cuda
name: VIT KvtAttention
