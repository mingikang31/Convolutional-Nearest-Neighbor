layer: LocalAttention
patch_size: 16
num_layers: 4
num_heads: 3
d_hidden: 48
d_mlp: 192
dropout: 0.1
attention_dropout: 0.1
K: 9
num_samples: -1
sampling_type: all
sample_padding: 0
magnitude_type: similarity
coordinate_encoding: False
dataset: cifar10
data_path: ./Data
batch_size: 64
num_epochs: 50
use_amp: False
clip_grad_norm: None
criterion: CrossEntropy
optimizer: adamw
momentum: 0.9
weight_decay: 1e-06
lr: 0.001
lr_step: 20
lr_gamma: 0.1
scheduler: step
device: cuda
seed: 0
output_dir: ./Output/Final_results/ViT-Tiny-Tiny-Tiny/CIFAR10/LocalAttention
resize: True
num_classes: 10
img_size: (3, 224, 224)
model: VIT
total_params: 260458
trainable_params: 260458
