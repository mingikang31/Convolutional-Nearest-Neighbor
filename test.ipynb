{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874e806e",
   "metadata": {},
   "source": [
    "# ConvNN Prime New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d8bc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dd24947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _calculate_similarity_matrix(self, matrix):\n",
    "    # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    return similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix: torch.Size([1, 2, 4])\n",
      "Input Matrix: tensor([[[1., 2., 3., 4.],\n",
      "         [5., 6., 7., 8.]]])\n",
      "\n",
      "Similarity Matrix: torch.Size([1, 4, 4])\n",
      "Similarity Matrix: tensor([[[1.0000, 0.9923, 0.9785, 0.9648],\n",
      "         [0.9923, 1.0000, 0.9965, 0.9899],\n",
      "         [0.9785, 0.9965, 1.0000, 0.9983],\n",
      "         [0.9648, 0.9899, 0.9983, 1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "matrix_x = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8]]], dtype=torch.float32)\n",
    "\n",
    "print(\"Input Matrix:\", matrix_x.shape)\n",
    "print(\"Input Matrix:\", matrix_x)\n",
    "print()\n",
    "\n",
    "a = _calculate_similarity_matrix(None, matrix_x)\n",
    "print(\"Similarity Matrix:\", a.shape)\n",
    "print(\"Similarity Matrix:\", a)\n",
    "\n",
    "# Input Matrix: torch.Size([1, 2, 4])\n",
    "# Input Matrix: tensor(\n",
    "#     [\n",
    "#         [\n",
    "#             [1., 2., 3., 4.],\n",
    "#             [5., 6., 7., 8.]\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Similarity Matrix: torch.Size([1, 4, 4])\n",
    "# Similarity Matrix: tensor(\n",
    "#     [\n",
    "#         [      1.      2.      3.      4.\n",
    "#          1. [1.0000, 0.9923, 0.9785, 0.9648],\n",
    "#          2. [0.9923, 1.0000, 0.9965, 0.9899],\n",
    "#          3. [0.9785, 0.9965, 1.0000, 0.9983],\n",
    "#          4. [0.9648, 0.9899, 0.9983, 1.0000]\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Prime Matrix: torch.Size([1, 2, 8])\n",
    "# Prime Matrix: tensor(\n",
    "#     [\n",
    "#         [.     1.      2.      3.      4.\n",
    "#             [1., 2., 2., 3., 3., 4., 4., 3.],\n",
    "#             [5., 6., 6., 7., 7., 8., 8., 7.]\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# New Prime Matrix: torch.Size([1, 2, 8])\n",
    "# New Prime Matrix: tensor(\n",
    "#     [\n",
    "#         [.          1.              2.              3.              4.\n",
    "#             [1.0000, 1.9846, 2.0000, 2.9896, 3.0000, 3.9931, 4.0000, 2.9948],\n",
    "#             [5.0000, 5.9537, 6.0000, 6.9758, 7.0000, 7.9862, 8.0000, 6.9879]\n",
    "#         ]\n",
    "#     ]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f78f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67cc042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "    b, c, t = matrix.shape\n",
    "    _, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "    \n",
    "    matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "    prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "    prime = prime.view(b, c, -1)\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43357349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime Matrix: torch.Size([1, 2, 8])\n",
      "Prime Matrix: tensor([[[1., 2., 2., 3., 3., 4., 4., 3.],\n",
      "         [5., 6., 6., 7., 7., 8., 8., 7.]]])\n"
     ]
    }
   ],
   "source": [
    "prime = _prime(None, matrix_x, a, K=2, maximum=True)\n",
    "print(\"Prime Matrix:\", prime.shape)\n",
    "print(\"Prime Matrix:\", prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _prime_new(self, matrix, magnitude_matrix, K, maximum):\n",
    "    b, c, t = matrix.shape\n",
    "    topk_values, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "    topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)    \n",
    "\n",
    "    matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "    prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "    prime = topk_values_exp * prime\n",
    "    \n",
    "    prime = prime.view(b, c, -1)\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0671aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Prime Matrix: torch.Size([1, 2, 8])\n",
      "New Prime Matrix: tensor([[[1.0000, 1.9846, 2.0000, 2.9896, 3.0000, 3.9931, 4.0000, 2.9948],\n",
      "         [5.0000, 5.9537, 6.0000, 6.9758, 7.0000, 7.9862, 8.0000, 6.9879]]])\n"
     ]
    }
   ],
   "source": [
    "new_prime = _prime_new(None, matrix_x, a, K=2, maximum=True)\n",
    "print(\"New Prime Matrix:\", new_prime.shape)\n",
    "print(\"New Prime Matrix:\", new_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93912674",
   "metadata": {},
   "source": [
    "### I. New Prime from Farias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "785792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prime(self, v, qk, K, maximum):\n",
    "    b, c, t = v.shape \n",
    "    topk_values, topk_indices = torch.topk(qk, k=K, dim=-1, largest = maximum)\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "    topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "    v_expanded = v.unsqueeze(-1).expand(b, c, t, K)\n",
    "    prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "    prime = topk_values_exp * prime\n",
    "    prime = prime.reshape(b, c, -1)\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23508fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1899b65b",
   "metadata": {},
   "source": [
    "# Coordinate Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3fce27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa1d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8671829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_coordinate_encoding( x):\n",
    "    b, c, t = x.shape \n",
    "    cache_key = f\"{t}_{x.device}\"\n",
    "    if cache_key in coordinate_cache:\n",
    "        coords_vec = coordinate_cache[cache_key]\n",
    "    else:\n",
    "        coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1)\n",
    "        coordinate_cache[cache_key] = coords_vec\n",
    "\n",
    "    expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1)\n",
    "    x_with_coords = torch.cat((x, expanded_coords), dim=1)  \n",
    "    return x_with_coords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bada92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([256, 3, 10])\n",
      "With Coordinate Encoding shape: torch.Size([256, 4, 10])\n",
      "Coordinate Cache Size: 1\n",
      "Coordinate Cache Keys: ['10_cpu']\n",
      "coordinate cache: tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n",
      "x_with_coords: tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(256, 3, 10)\n",
    "x_with_coords = _add_coordinate_encoding(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"With Coordinate Encoding shape: {x_with_coords.shape}\")\n",
    "\n",
    "print(f\"Coordinate Cache Size: {len(coordinate_cache)}\")\n",
    "print(f\"Coordinate Cache Keys: {list(coordinate_cache.keys())}\")\n",
    "\n",
    "print(f\"coordinate cache: {coordinate_cache['10_cpu'][0, ]}\")\n",
    "\n",
    "print(f\"x_with_coords: {x_with_coords[0, 3, :]}\")  # Print the first channel of the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6791ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_cache = {} \n",
    "def _add_coordinate_encoding( x):\n",
    "    b, c, t = x.shape \n",
    "    cache_key = f\"{b}_{t}_{x.device}\"\n",
    "    if cache_key in coordinate_cache:\n",
    "        expanded_coords = coordinate_cache[cache_key]\n",
    "    else:\n",
    "        coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1)\n",
    "        expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1)\n",
    "        coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "\n",
    "    x_with_coords = torch.cat((x, expanded_coords), dim=1)  \n",
    "    return x_with_coords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46532a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 3, 10])\n",
      "With Coordinate Encoding shape: torch.Size([64, 4, 10])\n",
      "Coordinate Cache Size: 2\n",
      "Coordinate Cache Keys: ['256_10_cpu', '64_10_cpu']\n",
      "coordinate cache: tensor([[-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "          0.7778,  1.0000]])\n",
      "x_with_coords: tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64, 3, 10)\n",
    "x_with_coords = _add_coordinate_encoding(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"With Coordinate Encoding shape: {x_with_coords.shape}\")\n",
    "\n",
    "print(f\"Coordinate Cache Size: {len(coordinate_cache)}\")\n",
    "print(f\"Coordinate Cache Keys: {list(coordinate_cache.keys())}\")\n",
    "\n",
    "print(f\"coordinate cache: {coordinate_cache['64_10_cpu'][0, ]}\")\n",
    "\n",
    "print(f\"x_with_coords: {x_with_coords[0, 3, :]}\")  # Print the first channel of the first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066b810",
   "metadata": {},
   "source": [
    "## ConvNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ConvNN\n",
    "Total parameters: 185,150\n",
    "Trainable parameters: 185,150\n",
    "\n",
    "Input shape: torch.Size([64, 197, 64])\n",
    "After Permute: torch.Size([64, 64, 197])\n",
    "After Split_head: torch.Size([64, 4, 197, 16])\n",
    "After Batch_Combine: torch.Size([256, 16, 197]) ### [256, 17, 197] added coordinate encoding\n",
    "After Conv1d: torch.Size([256, 16, 197]) ###    [256, 17, 197] added coordinate encoding\n",
    "After batch_split: torch.Size([64, 4, 197, 16]) ### [64, 4, 197, 17] added coordinate encoding\n",
    "After Combine_Heads: torch.Size([64, 64, 197]) ### [64, 68, 197] added coordinate encoding\n",
    "\n",
    "Output shape: torch.Size([64, 100])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bf967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2659c0f1",
   "metadata": {},
   "source": [
    "## ConvNNAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d351b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ConvNNAttention\n",
    "Total parameters: 83,716\n",
    "Trainable parameters: 83,716\n",
    "\n",
    "Input shape: torch.Size([64, 197, 64])\n",
    "After Split_head: torch.Size([64, 4, 197, 16])\n",
    "After Batch_Combine: torch.Size([256, 16, 197])\n",
    "After Conv1d: torch.Size([256, 16, 197])\n",
    "After permute: torch.Size([256, 197, 16])\n",
    "After Batch_Split: torch.Size([64, 4, 197, 16])\n",
    "After Combine_Heads: torch.Size([64, 197, 64])\n",
    "\n",
    "Output shape: torch.Size([64, 100])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b2cea",
   "metadata": {},
   "source": [
    "╰─$ python -u \"/Users/mingikang/Developer/Convolutional-Nearest-Neighbor/vit.py\"\n",
    "Regular Attention\n",
    "Total parameters: 70,228\n",
    "Trainable parameters: 70,228\n",
    "Output shape: torch.Size([64, 100])\n",
    "\n",
    "ConvNN\n",
    "Total parameters: 218,546\n",
    "Trainable parameters: 218,546\n",
    "Output shape: torch.Size([64, 100])\n",
    "\n",
    "ConvNNAttention\n",
    "Total parameters: 71,930\n",
    "Trainable parameters: 71,930\n",
    "Output shape: torch.Size([64, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b31576",
   "metadata": {},
   "source": [
    "╰─$ python -u \"/Users/mingikang/Developer/Convolutional-Nearest-Neighbor/vit.py\"\n",
    "Regular Attention\n",
    "Total parameters: 70,228\n",
    "Trainable parameters: 70,228\n",
    "Output shape: torch.Size([64, 100])\n",
    "\n",
    "ConvNN\n",
    "Total parameters: 218,295\n",
    "Trainable parameters: 218,295\n",
    "Output shape: torch.Size([64, 100])\n",
    "\n",
    "ConvNNAttention\n",
    "Total parameters: 71,679\n",
    "Trainable parameters: 71,679\n",
    "Output shape: torch.Size([64, 100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
