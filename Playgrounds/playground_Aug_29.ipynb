{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "91a28c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bcc5e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_cache = {}\n",
    "def _add_coordinate_encoding(x):\n",
    "    b, _, h, w = x.shape\n",
    "    cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "    if cache_key in coordinate_cache:\n",
    "        expanded_grid = coordinate_cache[cache_key]\n",
    "    else:\n",
    "        y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "        x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "        y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "        grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "        expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "        coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "    x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "    return x_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0fac47c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "ex = torch.rand(1, 3, 5, 5) \n",
    "ex_coord = _add_coordinate_encoding(ex)\n",
    "print(ex_coord.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "98fa5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "coord = ex_coord[:, -2:, :, :]\n",
    "print(coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "94fa2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_similarity_matrix(matrix):\n",
    "    # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e368ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 25])\n",
      "tensor([[[ 1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "          -1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00],\n",
      "         [ 9.4868e-01,  1.0000e+00,  8.9443e-01,  6.0000e-01,  3.1623e-01,\n",
      "           8.0000e-01,  9.4868e-01,  8.9443e-01,  3.1623e-01,  1.4263e-08,\n",
      "           4.4721e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -4.4721e-01,\n",
      "          -1.4263e-08, -3.1623e-01, -8.9443e-01, -9.4868e-01, -8.0000e-01,\n",
      "          -3.1623e-01, -6.0000e-01, -8.9443e-01, -1.0000e+00, -9.4868e-01],\n",
      "         [ 7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "          -7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01],\n",
      "         [ 3.1623e-01,  6.0000e-01,  8.9443e-01,  1.0000e+00,  9.4868e-01,\n",
      "           1.4263e-08,  3.1623e-01,  8.9443e-01,  9.4868e-01,  8.0000e-01,\n",
      "          -4.4721e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  4.4721e-01,\n",
      "          -8.0000e-01, -9.4868e-01, -8.9443e-01, -3.1623e-01, -1.4263e-08,\n",
      "          -9.4868e-01, -1.0000e+00, -8.9443e-01, -6.0000e-01, -3.1623e-01],\n",
      "         [ 1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08],\n",
      "         [ 9.4868e-01,  8.0000e-01,  4.4721e-01,  1.4263e-08, -3.1623e-01,\n",
      "           1.0000e+00,  9.4868e-01,  4.4721e-01, -3.1623e-01, -6.0000e-01,\n",
      "           8.9443e-01,  8.9443e-01,  0.0000e+00, -8.9443e-01, -8.9443e-01,\n",
      "           6.0000e-01,  3.1623e-01, -4.4721e-01, -9.4868e-01, -1.0000e+00,\n",
      "           3.1623e-01, -1.4263e-08, -4.4721e-01, -8.0000e-01, -9.4868e-01],\n",
      "         [ 1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "          -1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00],\n",
      "         [ 7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "          -7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01],\n",
      "         [ 1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08],\n",
      "         [-3.1623e-01,  1.4263e-08,  4.4721e-01,  8.0000e-01,  9.4868e-01,\n",
      "          -6.0000e-01, -3.1623e-01,  4.4721e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -8.9443e-01, -8.9443e-01,  0.0000e+00,  8.9443e-01,  8.9443e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -4.4721e-01,  3.1623e-01,  6.0000e-01,\n",
      "          -9.4868e-01, -8.0000e-01, -4.4721e-01, -1.4263e-08,  3.1623e-01],\n",
      "         [ 7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           1.0000e+00,  1.0000e+00,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01],\n",
      "         [ 7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           1.0000e+00,  1.0000e+00,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -1.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01],\n",
      "         [-7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -1.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01],\n",
      "         [ 3.1623e-01, -1.4263e-08, -4.4721e-01, -8.0000e-01, -9.4868e-01,\n",
      "           6.0000e-01,  3.1623e-01, -4.4721e-01, -9.4868e-01, -1.0000e+00,\n",
      "           8.9443e-01,  8.9443e-01,  0.0000e+00, -8.9443e-01, -8.9443e-01,\n",
      "           1.0000e+00,  9.4868e-01,  4.4721e-01, -3.1623e-01, -6.0000e-01,\n",
      "           9.4868e-01,  8.0000e-01,  4.4721e-01,  1.4263e-08, -3.1623e-01],\n",
      "         [-1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08],\n",
      "         [-7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01],\n",
      "         [-1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "           1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00],\n",
      "         [-9.4868e-01, -8.0000e-01, -4.4721e-01, -1.4263e-08,  3.1623e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -4.4721e-01,  3.1623e-01,  6.0000e-01,\n",
      "          -8.9443e-01, -8.9443e-01,  0.0000e+00,  8.9443e-01,  8.9443e-01,\n",
      "          -6.0000e-01, -3.1623e-01,  4.4721e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -3.1623e-01,  1.4263e-08,  4.4721e-01,  8.0000e-01,  9.4868e-01],\n",
      "         [-1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08],\n",
      "         [-3.1623e-01, -6.0000e-01, -8.9443e-01, -1.0000e+00, -9.4868e-01,\n",
      "          -1.4263e-08, -3.1623e-01, -8.9443e-01, -9.4868e-01, -8.0000e-01,\n",
      "           4.4721e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -4.4721e-01,\n",
      "           8.0000e-01,  9.4868e-01,  8.9443e-01,  3.1623e-01,  1.4263e-08,\n",
      "           9.4868e-01,  1.0000e+00,  8.9443e-01,  6.0000e-01,  3.1623e-01],\n",
      "         [-7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01],\n",
      "         [-9.4868e-01, -1.0000e+00, -8.9443e-01, -6.0000e-01, -3.1623e-01,\n",
      "          -8.0000e-01, -9.4868e-01, -8.9443e-01, -3.1623e-01, -1.4263e-08,\n",
      "          -4.4721e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  4.4721e-01,\n",
      "           1.4263e-08,  3.1623e-01,  8.9443e-01,  9.4868e-01,  8.0000e-01,\n",
      "           3.1623e-01,  6.0000e-01,  8.9443e-01,  1.0000e+00,  9.4868e-01],\n",
      "         [-1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "           1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "coord = nn.Flatten(start_dim=2)(ex_coord[:, -2:, :, :])\n",
    "print(coord.shape)\n",
    "coord_mat = _calculate_similarity_matrix(coord)\n",
    "print(coord_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "653c3e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9262, 0.0754, 0.3860, 0.1060, 0.4902],\n",
      "         [0.5279, 0.6429, 0.4504, 0.5828, 0.9255],\n",
      "         [0.0707, 0.1662, 0.6738, 0.5658, 0.4525]]])\n",
      "\n",
      "tensor([[[1.0000, 0.5895, 0.6704, 0.5094, 0.7995],\n",
      "         [0.5895, 1.0000, 0.7178, 0.8708, 0.9275],\n",
      "         [0.6704, 0.7178, 1.0000, 0.9310, 0.8895],\n",
      "         [0.5094, 0.8708, 0.9310, 1.0000, 0.9067],\n",
      "         [0.7995, 0.9275, 0.8895, 0.9067, 1.0000]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = torch.rand(1, 3, 5)\n",
    "print(ex)\n",
    "print()\n",
    "sim_mat = _calculate_similarity_matrix(ex)\n",
    "print(sim_mat)\n",
    "print() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "201eeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_similarity_matrix(matrix):\n",
    "    # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "44884b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1415, 0.4209, 0.6430, 0.4458, 0.9746],\n",
      "         [0.4209, 0.4466, 0.4306, 0.4767, 0.7071],\n",
      "         [0.6430, 0.4306, 0.8058, 0.6846, 0.9109],\n",
      "         [0.4458, 0.4767, 0.6846, 0.6710, 0.8473],\n",
      "         [0.9746, 0.7071, 0.9109, 0.8473, 1.3015]]])\n",
      "\n",
      "tensor([[[0.6545, 0.3707, 0.4029, 0.3104, 0.4499],\n",
      "         [0.2413, 0.3933, 0.2698, 0.3319, 0.3265],\n",
      "         [0.3686, 0.3793, 0.5050, 0.4767, 0.4206],\n",
      "         [0.2556, 0.4198, 0.4290, 0.4672, 0.3912],\n",
      "         [0.5587, 0.6227, 0.5708, 0.5899, 0.6009]]])\n",
      "\n",
      "tensor([[[1.0000, 0.5895, 0.6704, 0.5094, 0.7995],\n",
      "         [0.5895, 1.0000, 0.7178, 0.8708, 0.9275],\n",
      "         [0.6704, 0.7178, 1.0000, 0.9310, 0.8895],\n",
      "         [0.5094, 0.8708, 0.9310, 1.0000, 0.9067],\n",
      "         [0.7995, 0.9275, 0.8895, 0.9067, 1.0000]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim_mat = ex.transpose(2, 1) @ ex\n",
    "print(sim_mat)\n",
    "print()\n",
    "sim_mat = nn.functional.normalize(sim_mat, p=2, dim=1)\n",
    "print(sim_mat)\n",
    "print()\n",
    "sim_mat = _calculate_similarity_matrix(ex)\n",
    "print(sim_mat)\n",
    "print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ac3f3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5beae31",
   "metadata": {},
   "source": [
    "# SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6cb045c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_NN(nn.Module): \n",
    "    \"\"\"Convolution 2D Nearest Neighbor Layer\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                K,\n",
    "                stride, \n",
    "                sampling_type, \n",
    "                num_samples, \n",
    "                sample_padding,\n",
    "                shuffle_pattern, \n",
    "                shuffle_scale, \n",
    "                magnitude_type,\n",
    "                coordinate_encoding\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            K (int): Number of Nearest Neighbors for consideration.\n",
    "            stride (int): Stride size.\n",
    "            sampling_type (str): Sampling type: \"all\", \"random\", \"spatial\".\n",
    "            num_samples (int): Number of samples to consider. -1 for all samples.\n",
    "            shuffle_pattern (str): Shuffle pattern: \"B\", \"A\", \"BA\".\n",
    "            shuffle_scale (int): Shuffle scale factor.\n",
    "            magnitude_type (str): Distance or Similarity.\n",
    "        \"\"\"\n",
    "        super(Conv2d_NN, self).__init__()\n",
    "        \n",
    "        # Assertions \n",
    "        assert K == stride, \"Error: K must be same as stride. K == stride.\"\n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Error: shuffle_pattern must be one of ['B', 'A', 'BA', 'NA']\"\n",
    "        assert magnitude_type in [\"distance\", \"similarity\"], \"Error: magnitude_type must be one of ['distance', 'similarity']\"\n",
    "        assert sampling_type in [\"all\", \"random\", \"spatial\"], \"Error: sampling_type must be one of ['all', 'random', 'spatial']\"\n",
    "        assert int(num_samples) > 0 or int(num_samples) == -1, \"Error: num_samples must be greater than 0 or -1 for all samples\"\n",
    "        assert (sampling_type == \"all\" and int(num_samples) == -1) or (sampling_type != \"all\" and isinstance(num_samples, int)), \"Error: num_samples must be -1 for 'all' sampling or an integer for 'random' and 'spatial' sampling\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.stride = stride\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples if num_samples != -1 else 'all'  # -1 for all samples\n",
    "        self.sample_padding = sample_padding if sampling_type == \"spatial\" else 0\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'similarity' else False\n",
    "        self.INF_DISTANCE = 1e10\n",
    "        self.NEG_INF_DISTANCE = -1e10\n",
    "\n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        self.in_channels = in_channels + 2 if self.coordinate_encoding else in_channels\n",
    "        self.out_channels = out_channels # + 2 if self.coordinate_encoding else out_channels\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Adjust Channels for PixelShuffle\n",
    "        self.in_channels_1d = self.in_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_1d = self.out_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        # Conv1d Layer\n",
    "        self.in_channels_1d = 1\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=self.in_channels_1d, \n",
    "                                      out_channels=self.out_channels_1d, \n",
    "                                      kernel_size=self.K, \n",
    "                                      stride=self.stride, \n",
    "                                      padding=0)\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "\n",
    "        # # Pointwise Convolution Layer\n",
    "        # self.pointwise_conv = nn.Conv2d(in_channels=self.out_channels,\n",
    "        #                                  out_channels=self.out_channels - 2,\n",
    "        #                                  kernel_size=1,\n",
    "        #                                  stride=1,\n",
    "        #                                  padding=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        # Coordinate Channels (optional) + Unshuffle + Flatten \n",
    "        x = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "        print(\"x padded: \")\n",
    "        print(x)\n",
    "        print()\n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        print(\"x after coordinate encoding: \")\n",
    "        print(x)\n",
    "        x_2d = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "        x = self.flatten(x_2d)\n",
    "        print(\"x: \")\n",
    "        print(x)\n",
    "        print() \n",
    "        print(x.shape)\n",
    "        \n",
    "        if self.sampling_type == \"all\":    \n",
    "            # ConvNN Algorithm \n",
    "            x_dist = x[:, -2:, :]\n",
    "            print(\"x_dist: \")\n",
    "            print(x_dist)\n",
    "            print(x_dist.shape)\n",
    "\n",
    "            matrix_magnitude = self._calculate_distance_matrix(x_dist, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix(x_dist)\n",
    "            print(\"matrix_magnitude: \")\n",
    "            print(matrix_magnitude)\n",
    "            x = x[:, 0, :].unsqueeze(1)\n",
    "            print(x)\n",
    "            print(x.shape)\n",
    "            prime = self._prime(x, matrix_magnitude, self.K, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == \"random\":\n",
    "            # Select random samples\n",
    "            rand_idx = torch.randperm(x.shape[2], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, :, rand_idx]\n",
    "\n",
    "            # ConvNN Algorithm \n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(rand_idx), device=x.device)\n",
    "            matrix_magnitude[:, rand_idx, range_idx] = self.INF_DISTANCE if self.magnitude_type == 'distance' else self.NEG_INF_DISTANCE\n",
    "            prime = self._prime_N(x, matrix_magnitude, self.K, rand_idx, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type == \"spatial\":\n",
    "            # Get spatial sampled indices\n",
    "            x_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[2] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            y_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[3] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "            x_idx_flat, y_idx_flat = x_grid.flatten(), y_grid.flatten()\n",
    "            width = x_2d.shape[2] \n",
    "            flat_indices = y_idx_flat * width + x_idx_flat  \n",
    "            x_sample = x[:, :, flat_indices]\n",
    "\n",
    "            # ConvNN Algorithm\n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(flat_indices), device=x.device)\n",
    "            matrix_magnitude[:, flat_indices, range_idx] = self.INF_DISTANCE if self.magnitude_type == 'distance' else self.NEG_INF_DISTANCE\n",
    "            prime = self._prime_N(x, matrix_magnitude, self.K, flat_indices, self.maximum)\n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial'].\")\n",
    "        print(\"\\n prime: \")\n",
    "        print(prime.shape)\n",
    "        print(prime)\n",
    "        # Post-Processing \n",
    "        x_conv = self.conv1d_layer(prime) \n",
    "        \n",
    "        # Unflatten + Shuffle\n",
    "        unflatten = nn.Unflatten(dim=2, unflattened_size=x_2d.shape[2:])\n",
    "        x = unflatten(x_conv)  # [batch_size, out_channels\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x\n",
    "        # x = self.pointwise_conv(x) if self.coordinate_encoding else x\n",
    "        return x\n",
    "\n",
    "    def _calculate_distance_matrix(self, matrix, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared.transpose(2, 1) - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=-1.0, max=1.0) \n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix \n",
    "        \n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_distance_matrix_N(self, matrix, matrix_sample, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True).permute(0, 2, 1)\n",
    "        norm_squared_sample = torch.sum(matrix_sample ** 2, dim=1, keepdim=True).transpose(2, 1).permute(0, 2, 1)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix_sample)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared_sample - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=-1.0, max=1.0) \n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "\n",
    "        return dist_matrix\n",
    "    \n",
    "    # def _calculate_similarity_matrix(self, matrix):\n",
    "    #     # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    #     norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    #     similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    #     similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    #     return similarity_matrix\n",
    "\n",
    "    def _calculate_similarity_matrix(self, matrix, sigma=0.1):\n",
    "        \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "        b, c, t = matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "        \n",
    "        # Calculate pairwise Euclidean distances between coordinates\n",
    "        coord_expanded_1 = matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "        coord_expanded_2 = matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "        \n",
    "        # Euclidean distance between coordinates\n",
    "        coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "        coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1) + 1e-8)  # [B, T, T]\n",
    "        \n",
    "        # Convert distance to similarity using Gaussian kernel\n",
    "        similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix_N(self, matrix, matrix_sample):\n",
    "        # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_sample)\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        _, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "        \n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "        print(\"prime: \")\n",
    "        print(prime)\n",
    "        print()\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "        \n",
    "    def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        \n",
    "        _, topk_indices = torch.topk(magnitude_matrix, k=K - 1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indices back to original matrix positions\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords ### Last two channels are coordinate channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "09e41aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x padded: \n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 1., 2., 3., 0.],\n",
      "          [0., 4., 5., 6., 0.],\n",
      "          [0., 7., 8., 9., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "\n",
      "x after coordinate encoding: \n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  2.0000,  3.0000,  0.0000],\n",
      "          [ 0.0000,  4.0000,  5.0000,  6.0000,  0.0000],\n",
      "          [ 0.0000,  7.0000,  8.0000,  9.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]]])\n",
      "x: \n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
      "           2.0000,  3.0000,  0.0000,  0.0000,  4.0000,  5.0000,  6.0000,\n",
      "           0.0000,  0.0000,  7.0000,  8.0000,  9.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000, -0.5000,\n",
      "           0.0000,  0.5000,  1.0000, -1.0000, -0.5000,  0.0000,  0.5000,\n",
      "           1.0000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000,\n",
      "          -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000]]])\n",
      "\n",
      "torch.Size([1, 3, 25])\n",
      "x_dist: \n",
      "tensor([[[-1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000, -0.5000,\n",
      "           0.0000,  0.5000,  1.0000, -1.0000, -0.5000,  0.0000,  0.5000,\n",
      "           1.0000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000,\n",
      "          -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000]]])\n",
      "torch.Size([1, 2, 25])\n",
      "matrix_magnitude: \n",
      "tensor([[[1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00],\n",
      "         [7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00],\n",
      "         [3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44],\n",
      "         [0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1., 2., 3., 0., 0., 4., 5., 6., 0., 0., 7.,\n",
      "          8., 9., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 25])\n",
      "prime: \n",
      "tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
      "          [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
      "          [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
      "          [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
      "          [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
      "          [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
      "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "          [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
      "          [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
      "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "          [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
      "          [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
      "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "          [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
      "          [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
      "          [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
      "          [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
      "          [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
      "          [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
      "          [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
      "\n",
      "\n",
      " prime: \n",
      "torch.Size([1, 1, 225])\n",
      "tensor([[[0., 0., 0., 1., 0., 0., 4., 2., 5., 0., 0., 0., 1., 0., 2., 0., 4.,\n",
      "          3., 0., 0., 0., 2., 3., 1., 5., 0., 0., 0., 0., 0., 3., 0., 2., 6.,\n",
      "          0., 1., 0., 0., 0., 3., 0., 0., 6., 2., 5., 0., 0., 0., 1., 0., 4.,\n",
      "          0., 2., 5., 1., 4., 0., 2., 0., 5., 0., 0., 0., 2., 1., 0., 3., 5.,\n",
      "          0., 0., 6., 4., 3., 6., 0., 0., 2., 5., 0., 0., 0., 0., 0., 0., 3.,\n",
      "          6., 0., 0., 2., 5., 0., 0., 4., 0., 7., 1., 5., 0., 0., 4., 5., 1.,\n",
      "          0., 7., 0., 2., 0., 8., 5., 6., 8., 4., 2., 7., 1., 9., 3., 6., 0.,\n",
      "          5., 3., 9., 8., 2., 0., 0., 0., 0., 6., 0., 9., 3., 0., 5., 0., 0.,\n",
      "          0., 0., 7., 4., 0., 8., 0., 5., 7., 0., 8., 0., 4., 5., 0., 0., 0.,\n",
      "          8., 5., 0., 7., 9., 4., 0., 0., 6., 9., 0., 0., 8., 6., 0., 0., 0.,\n",
      "          5., 0., 0., 9., 0., 0., 6., 8., 0., 3., 0., 0., 0., 7., 0., 0., 8.,\n",
      "          4., 5., 0., 0., 0., 7., 8., 0., 4., 0., 9., 0., 0., 0., 8., 9., 7.,\n",
      "          0., 5., 0., 0., 0., 0., 9., 0., 8., 0., 6., 5., 0., 0., 0., 9., 0.,\n",
      "          0., 8., 6., 5.]]])\n",
      "torch.Size([1, 6, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe results may be different because of the different positions of the kernel window from ConvNN. \\nExample: \\nConvolution = \\ntensor([[0., 0., 0.],\\n        [0., 1., 2.],\\n        [0., 4., 5.]])\\nFlattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\\n\\nConvNN = \\ntensor([[0., 0., 0.],\\n        [0., 1., 2.],\\n        [0., 4., 5.]])\\nTopk:      [1.0, 4.0, 0.0, 2.0, 0.0, 5.0, 0.0, 0.0, 0.0],\\n'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = torch.Tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "convnn = Conv2d_NN(in_channels=1, out_channels=6, K=9, stride=9, sampling_type='all', num_samples=-1, sample_padding=0, shuffle_pattern='NA', shuffle_scale=0.0, magnitude_type='similarity', coordinate_encoding=True)\n",
    "out = convnn(ex)\n",
    "print(out.shape)\n",
    "\n",
    "\"\"\"\n",
    "tensor([[[0., 0., 0., 0., 0., \n",
    "          0., 1., 2., 3., 0., \n",
    "          0., 4., 5., 6., 0., \n",
    "          0., 7., 8., 9., 0., \n",
    "          0., 0., 0., 0., 0.]]])\n",
    "\n",
    "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],  \n",
    "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "          [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "          \n",
    "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "          [9., 0., 0., 8., 6., 0., 0., 0., 5.]]]])\n",
    "          \n",
    "Window 0 (position [0, 0]):\n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 1., 2.],\n",
    "        [0., 4., 5.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\n",
    "\n",
    "Window 1 (position [0, 1]):\n",
    "tensor([[0., 0., 0.],\n",
    "        [1., 2., 3.],\n",
    "        [4., 5., 6.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
    "\n",
    "Window 2 (position [0, 2]):\n",
    "tensor([[0., 0., 0.],\n",
    "        [2., 3., 0.],\n",
    "        [5., 6., 0.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 2.0, 3.0, 0.0, 5.0, 6.0, 0.0]\n",
    "\n",
    "Window 3 (position [1, 0]):\n",
    "tensor([[0., 1., 2.],\n",
    "        [0., 4., 5.],\n",
    "        [0., 7., 8.]])\n",
    "Flattened: [0.0, 1.0, 2.0, 0.0, 4.0, 5.0, 0.0, 7.0, 8.0]\n",
    "\n",
    "Window 4 (position [1, 1]):\n",
    "tensor([[1., 2., 3.],\n",
    "        [4., 5., 6.],\n",
    "        [7., 8., 9.]])\n",
    "Flattened: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "\n",
    "Window 5 (position [1, 2]):\n",
    "tensor([[2., 3., 0.],\n",
    "        [5., 6., 0.],\n",
    "        [8., 9., 0.]])\n",
    "Flattened: [2.0, 3.0, 0.0, 5.0, 6.0, 0.0, 8.0, 9.0, 0.0]\n",
    "\n",
    "Window 6 (position [2, 0]):\n",
    "tensor([[0., 4., 5.],\n",
    "        [0., 7., 8.],\n",
    "        [0., 0., 0.]])\n",
    "Flattened: [0.0, 4.0, 5.0, 0.0, 7.0, 8.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "Window 7 (position [2, 1]):\n",
    "tensor([[4., 5., 6.],\n",
    "        [7., 8., 9.],\n",
    "        [0., 0., 0.]])\n",
    "Flattened: [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "Window 8 (position [2, 2]):\n",
    "tensor([[5., 6., 0.],\n",
    "        [8., 9., 0.],\n",
    "        [0., 0., 0.]])\n",
    "Flattened: [5.0, 6.0, 0.0, 8.0, 9.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "          \n",
    "torch.Size([1, 1, 25])\n",
    "prime: \n",
    "tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
    "          [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
    "          [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
    "          [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
    "          [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
    "          [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
    "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "          [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
    "          [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
    "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "          [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
    "          [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
    "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "          [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "          [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
    "          [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
    "          [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
    "          [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
    "          [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
    "          [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
    "\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The results may be different because of the different positions of the kernel window from ConvNN. \n",
    "Example: \n",
    "Convolution = \n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 1., 2.],\n",
    "        [0., 4., 5.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\n",
    "\n",
    "ConvNN = \n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 1., 2.],\n",
    "        [0., 4., 5.]])\n",
    "Topk:      [1.0, 4.0, 0.0, 2.0, 0.0, 5.0, 0.0, 0.0, 0.0],\n",
    "\"\"\"\n",
    "\n",
    "# SOLVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "66289130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-channel tensor shape: torch.Size([1, 3, 25, 9])\n",
      "Original shape: torch.Size([1, 3, 25, 9])\n",
      "Filtered shape: torch.Size([1, 3, 9, 9])\n",
      "Kept row indices: tensor([ 6,  7,  8, 11, 12, 13, 16, 17, 18])\n",
      "\n",
      "Filtered first channel:\n",
      "tensor([[1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "        [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "        [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "        [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "        [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "        [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "        [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "        [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "        [9., 0., 0., 8., 6., 0., 0., 0., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def filter_non_zero_starting_rows_multichannel(tensor):\n",
    "    \"\"\"\n",
    "    Filter rows based on the first element of the first channel being non-zero\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor of shape [B, C, num_rows, row_length]\n",
    "    \n",
    "    Returns:\n",
    "        Filtered tensor with only rows where first channel's first element != 0\n",
    "    \"\"\"\n",
    "    # Get the shape\n",
    "    b, c, num_rows, row_length = tensor.shape\n",
    "    \n",
    "    # Create mask based on first channel only\n",
    "    # tensor[:, 0, :, 0] gets first element of each row in first channel\n",
    "    mask = tensor[:, 0, :, 0] != 0  # Shape: [b, num_rows]\n",
    "    \n",
    "    # Get indices of non-zero starting rows\n",
    "    non_zero_indices = torch.where(mask[0])[0]  # [0] because batch dimension\n",
    "    \n",
    "    # Select rows from ALL channels\n",
    "    filtered_tensor = tensor[:, :, non_zero_indices, :]\n",
    "    \n",
    "    return filtered_tensor, non_zero_indices\n",
    "\n",
    "# Test with your example (assuming you have multiple channels)\n",
    "# For demonstration, let's create a multi-channel version\n",
    "prime_single_channel = torch.tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
    "                                      [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
    "                                      [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
    "                                      [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
    "                                      [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
    "                                      [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
    "                                      [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "                                      [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "                                      [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "                                      [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
    "                                      [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
    "                                      [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "                                      [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "                                      [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "                                      [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
    "                                      [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
    "                                      [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "                                      [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "                                      [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "                                      [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
    "                                      [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
    "                                      [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
    "                                      [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
    "                                      [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
    "                                      [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
    "\n",
    "# Create a multi-channel version for demonstration\n",
    "# Let's say we have 3 channels\n",
    "prime_multi_channel = torch.cat([\n",
    "    prime_single_channel,  # Channel 0 (your original data)\n",
    "    torch.rand_like(prime_single_channel),  # Channel 1 (random data)\n",
    "    torch.rand_like(prime_single_channel)   # Channel 2 (random data)\n",
    "], dim=1)\n",
    "\n",
    "print(f\"Multi-channel tensor shape: {prime_multi_channel.shape}\")\n",
    "\n",
    "# Filter based on first channel only\n",
    "filtered_tensor, kept_indices = filter_non_zero_starting_rows_multichannel(prime_multi_channel)\n",
    "\n",
    "print(f\"Original shape: {prime_multi_channel.shape}\")\n",
    "print(f\"Filtered shape: {filtered_tensor.shape}\")\n",
    "print(f\"Kept row indices: {kept_indices}\")\n",
    "\n",
    "# Show the filtered first channel\n",
    "print(\"\\nFiltered first channel:\")\n",
    "print(filtered_tensor[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "379f3a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 1, 25, 9])\n",
      "Filtered shape: torch.Size([1, 1, 9, 9])\n",
      "\n",
      "Filtered tensor:\n",
      "tensor([[[[1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "          [9., 0., 0., 8., 6., 0., 0., 0., 5.]]]])\n",
      "\n",
      "Method 2 - Filtered shape: torch.Size([1, 1, 9, 9])\n",
      "Filtered tensor (Method 2):\n",
      "tensor([[[[1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "          [9., 0., 0., 8., 6., 0., 0., 0., 5.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your original tensor\n",
    "prime = torch.tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
    "                        [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
    "                        [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
    "                        [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
    "                        [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
    "                        [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
    "                        [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "                        [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "                        [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "                        [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
    "                        [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
    "                        [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "                        [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "                        [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "                        [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
    "                        [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
    "                        [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "                        [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "                        [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "                        [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
    "                        [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
    "                        [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
    "                        [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
    "                        [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
    "                        [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
    "\n",
    "# Method 1: Create a mask for rows that don't start with 0\n",
    "def filter_non_zero_starting_rows(tensor):\n",
    "    # Get the shape\n",
    "    b, c, num_rows, row_length = tensor.shape\n",
    "    \n",
    "    # Create mask for rows where first element is not 0\n",
    "    mask = tensor[:, :, :, 0] != 0  # Shape: [b, c, num_rows]\n",
    "    \n",
    "    # Get indices of non-zero starting rows\n",
    "    non_zero_indices = torch.where(mask[0, 0])[0]\n",
    "    \n",
    "    # Select only the rows that don't start with 0\n",
    "    filtered_tensor = tensor[:, :, non_zero_indices, :]\n",
    "    \n",
    "    return filtered_tensor\n",
    "\n",
    "# Apply the filter\n",
    "filtered_prime = filter_non_zero_starting_rows(prime)\n",
    "\n",
    "print(f\"Original shape: {prime.shape}\")\n",
    "print(f\"Filtered shape: {filtered_prime.shape}\")\n",
    "print(\"\\nFiltered tensor:\")\n",
    "print(filtered_prime)\n",
    "\n",
    "# Method 2: More concise using boolean indexing\n",
    "def filter_non_zero_starting_rows_concise(tensor):\n",
    "    # Reshape to 2D for easier indexing\n",
    "    reshaped = tensor.squeeze()  # Remove batch and channel dimensions\n",
    "    \n",
    "    # Create mask for rows that don't start with 0\n",
    "    mask = reshaped[:, 0] != 0\n",
    "    \n",
    "    # Filter rows\n",
    "    filtered_rows = reshaped[mask]\n",
    "    \n",
    "    # Restore original dimensions\n",
    "    return filtered_rows.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Alternative method\n",
    "filtered_prime_v2 = filter_non_zero_starting_rows_concise(prime)\n",
    "print(f\"\\nMethod 2 - Filtered shape: {filtered_prime_v2.shape}\")\n",
    "print(\"Filtered tensor (Method 2):\")\n",
    "print(filtered_prime_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce720f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bd542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c56fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baad55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "21a0df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 1., 1., 1., 0.],\n",
      "          [0., 1., 1., 1., 0.],\n",
      "          [0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(1, 1, 3, 3) \n",
    "print(ones)\n",
    "ones = F.pad(ones, (1, 1, 1, 1), mode='constant', value=0)\n",
    "print(ones)\n",
    "print(ones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0a6c9f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 3x3 windows: 9 (3x3)\n",
      "Each window has 9 elements (3x3 = 9)\n",
      "\n",
      "Window 0 (position [0, 0]):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 1., 2.],\n",
      "        [0., 4., 5.]])\n",
      "Flattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\n",
      "\n",
      "Window 1 (position [0, 1]):\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Flattened: [0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "\n",
      "Window 2 (position [0, 2]):\n",
      "tensor([[0., 0., 0.],\n",
      "        [2., 3., 0.],\n",
      "        [5., 6., 0.]])\n",
      "Flattened: [0.0, 0.0, 0.0, 2.0, 3.0, 0.0, 5.0, 6.0, 0.0]\n",
      "\n",
      "Window 3 (position [1, 0]):\n",
      "tensor([[0., 1., 2.],\n",
      "        [0., 4., 5.],\n",
      "        [0., 7., 8.]])\n",
      "Flattened: [0.0, 1.0, 2.0, 0.0, 4.0, 5.0, 0.0, 7.0, 8.0]\n",
      "\n",
      "Window 4 (position [1, 1]):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "Flattened: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "\n",
      "Window 5 (position [1, 2]):\n",
      "tensor([[2., 3., 0.],\n",
      "        [5., 6., 0.],\n",
      "        [8., 9., 0.]])\n",
      "Flattened: [2.0, 3.0, 0.0, 5.0, 6.0, 0.0, 8.0, 9.0, 0.0]\n",
      "\n",
      "Window 6 (position [2, 0]):\n",
      "tensor([[0., 4., 5.],\n",
      "        [0., 7., 8.],\n",
      "        [0., 0., 0.]])\n",
      "Flattened: [0.0, 4.0, 5.0, 0.0, 7.0, 8.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Window 7 (position [2, 1]):\n",
      "tensor([[4., 5., 6.],\n",
      "        [7., 8., 9.],\n",
      "        [0., 0., 0.]])\n",
      "Flattened: [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Window 8 (position [2, 2]):\n",
      "tensor([[5., 6., 0.],\n",
      "        [8., 9., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Flattened: [5.0, 6.0, 0.0, 8.0, 9.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_3x3_windows_from_flattened(x_flat, original_shape=(5, 5)):\n",
    "    \"\"\"\n",
    "    Extract all 3x3 convolutional windows from a flattened tensor\n",
    "    \n",
    "    Args:\n",
    "        x_flat: Flattened tensor [1, 1, H*W]\n",
    "        original_shape: Original 2D shape (H, W)\n",
    "    \n",
    "    Returns:\n",
    "        windows: All 3x3 windows in 1D format [num_windows, 9]\n",
    "    \"\"\"\n",
    "    h, w = original_shape\n",
    "    \n",
    "    # Reshape back to 2D\n",
    "    x_2d = x_flat.reshape(1, 1, h, w)\n",
    "    \n",
    "    # Use unfold to get 3x3 patches\n",
    "    # unfold(dim, size, step) extracts sliding windows\n",
    "    patches = x_2d.unfold(2, 3, 1).unfold(3, 3, 1)  # [1, 1, H-2, W-2, 3, 3]\n",
    "    \n",
    "    # Reshape to get each window as a 1D vector\n",
    "    num_windows_h, num_windows_w = patches.shape[2], patches.shape[3]\n",
    "    windows = patches.reshape(num_windows_h * num_windows_w, 9)\n",
    "    \n",
    "    return windows, num_windows_h, num_windows_w\n",
    "\n",
    "# Your input tensor\n",
    "x_flat = torch.tensor([[[0., 0., 0., 0., 0., \n",
    "                        0., 1., 2., 3., 0., \n",
    "                        0., 4., 5., 6., 0., \n",
    "                        0., 7., 8., 9., 0., \n",
    "                        0., 0., 0., 0., 0.]]])\n",
    "\n",
    "# Extract all 3x3 windows\n",
    "windows, num_h, num_w = get_3x3_windows_from_flattened(x_flat)\n",
    "\n",
    "print(f\"Number of 3x3 windows: {windows.shape[0]} ({num_h}x{num_w})\")\n",
    "print(f\"Each window has {windows.shape[1]} elements (3x3 = 9)\")\n",
    "print()\n",
    "\n",
    "# Display all windows\n",
    "for i, window in enumerate(windows):\n",
    "    row = i // num_w\n",
    "    col = i % num_w\n",
    "    print(f\"Window {i} (position [{row}, {col}]):\")\n",
    "    print(window.reshape(3, 3))\n",
    "    print(\"Flattened:\", window.tolist())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9350537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef682c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b86a180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coordinate-based similarity approach ===\n",
      "Input tensor:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 3., 0.],\n",
      "        [0., 4., 5., 6., 0.],\n",
      "        [0., 7., 8., 9., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Original shape: torch.Size([1, 1, 5, 5])\n",
      "Padded shape: torch.Size([1, 1, 7, 7])\n",
      "Flattened coordinate shape: torch.Size([1, 2, 49])\n",
      "Similarity matrix shape: torch.Size([1, 49, 49])\n",
      "Neighbors for center position (value 5):\n",
      "tensor([0., 0., 0., 0., 0., 3., 0., 0., 0.])\n",
      "\n",
      "Neighbors for position with value 1:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 3.])\n",
      "\n",
      "=== Direct 3x3 patch approach ===\n",
      "3x3 patch around value 5:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [3., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _add_coordinate_encoding(x):\n",
    "    \"\"\"Add coordinate channels to input tensor\"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords = torch.linspace(-1, 1, h, device=x.device)\n",
    "    x_coords = torch.linspace(-1, 1, w, device=x.device)\n",
    "    \n",
    "    y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack coordinates and add batch dimension\n",
    "    coord_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0)  # [1, 2, H, W]\n",
    "    coord_grid = coord_grid.expand(b, -1, -1, -1)  # [B, 2, H, W]\n",
    "    \n",
    "    return coord_grid\n",
    "\n",
    "def _calculate_coordinate_similarity_matrix(coord_matrix, sigma=0.5):\n",
    "    \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "    b, c, t = coord_matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "    \n",
    "    # Calculate pairwise Euclidean distances between coordinates\n",
    "    coord_expanded_1 = coord_matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "    coord_expanded_2 = coord_matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "    \n",
    "    # Euclidean distance between coordinates\n",
    "    coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "    coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1))  # [B, T, T]\n",
    "    \n",
    "    # Convert distance to similarity using Gaussian kernel\n",
    "    similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def get_spatial_neighbors(x, K=9, sigma=0.5):\n",
    "    \"\"\"\n",
    "    Get K nearest spatial neighbors using coordinate-based similarity\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor [B, C, H, W]\n",
    "        K: Number of neighbors to select\n",
    "        sigma: Standard deviation for Gaussian similarity kernel\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Add padding\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    \n",
    "    # Get coordinate encoding for the padded tensor\n",
    "    coord_grid = _add_coordinate_encoding(x_padded)  # [B, 2, H+2, W+2]\n",
    "    \n",
    "    # Flatten spatial dimensions\n",
    "    x_flat = x_padded.flatten(2)  # [B, C, (H+2)*(W+2)]\n",
    "    coord_flat = coord_grid.flatten(2)  # [B, 2, (H+2)*(W+2)]\n",
    "    \n",
    "    print(f\"Original shape: {x.shape}\")\n",
    "    print(f\"Padded shape: {x_padded.shape}\")\n",
    "    print(f\"Flattened coordinate shape: {coord_flat.shape}\")\n",
    "    \n",
    "    # Calculate similarity matrix based on coordinates\n",
    "    similarity_matrix = _calculate_coordinate_similarity_matrix(coord_flat, sigma=sigma)\n",
    "    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "    \n",
    "    # Get top K neighbors for each position\n",
    "    topk_values, topk_indices = torch.topk(similarity_matrix, k=K, dim=2, largest=True)\n",
    "    \n",
    "    # Expand indices to match all channels\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, -1, K)  # [B, C, T, K]\n",
    "    \n",
    "    # Gather the neighboring values\n",
    "    x_expanded = x_flat.unsqueeze(-1).expand(b, c, -1, K)\n",
    "    neighbors = torch.gather(x_expanded, dim=2, index=topk_indices_exp)\n",
    "    \n",
    "    return neighbors, topk_indices, similarity_matrix\n",
    "\n",
    "# Test with your example\n",
    "def test_spatial_neighbors():\n",
    "    # Create test tensor - your example reshaped to 2D\n",
    "    x = torch.tensor([[[\n",
    "        [0., 0., 0., 0., 0.],\n",
    "        [0., 1., 2., 3., 0.],\n",
    "        [0., 4., 5., 6., 0.],\n",
    "        [0., 7., 8., 9., 0.],\n",
    "        [0., 0., 0., 0., 0.]\n",
    "    ]]])\n",
    "    \n",
    "    print(\"Input tensor:\")\n",
    "    print(x.squeeze())\n",
    "    print()\n",
    "    \n",
    "    # Get 9 nearest neighbors\n",
    "    neighbors, indices, sim_matrix = get_spatial_neighbors(x, K=9, sigma=0.3)\n",
    "    \n",
    "    # Look at neighbors for the position containing value '5' (center of 3x3 grid)\n",
    "    # In the padded 5x5 grid, value '5' should be at position (2,2)\n",
    "    # In flattened coordinates: 2*5 + 2 = 12\n",
    "    center_pos = 12  # This should correspond to value '5'\n",
    "    \n",
    "    print(f\"Neighbors for center position (value 5):\")\n",
    "    center_neighbors = neighbors[0, 0, center_pos, :]  # [K]\n",
    "    print(center_neighbors)\n",
    "    \n",
    "    # Let's also check a few other positions\n",
    "    print(f\"\\nNeighbors for position with value 1:\")\n",
    "    pos_1 = 6  # Position of value '1'\n",
    "    neighbors_1 = neighbors[0, 0, pos_1, :]\n",
    "    print(neighbors_1)\n",
    "    \n",
    "    return neighbors, indices, sim_matrix\n",
    "\n",
    "# Alternative approach: Direct coordinate-based neighbor selection\n",
    "def get_3x3_neighbors_direct(x):\n",
    "    \"\"\"\n",
    "    Directly get 3x3 neighbors without similarity matrix calculation\n",
    "    This mimics standard convolution receptive field\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Pad the input\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    \n",
    "    # Use unfold to get 3x3 patches\n",
    "    patches = x_padded.unfold(2, 3, 1).unfold(3, 3, 1)  # [B, C, H, W, 3, 3]\n",
    "    patches = patches.reshape(b, c, h*w, 9)  # [B, C, H*W, 9]\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Test both approaches\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Coordinate-based similarity approach ===\")\n",
    "    neighbors, indices, sim_matrix = test_spatial_neighbors()\n",
    "    \n",
    "    print(\"\\n=== Direct 3x3 patch approach ===\")\n",
    "    x = torch.tensor([[[\n",
    "        [0., 0., 0., 0., 0.],\n",
    "        [0., 1., 2., 3., 0.],\n",
    "        [0., 4., 5., 6., 0.],\n",
    "        [0., 7., 8., 9., 0.],\n",
    "        [0., 0., 0., 0., 0.]\n",
    "    ]]])\n",
    "    \n",
    "    patches = get_3x3_neighbors_direct(x)\n",
    "    \n",
    "    # Get patch for center position (1, 1) in original coordinates\n",
    "    # This corresponds to value '5'\n",
    "    center_patch = patches[0, 0, 1*3 + 1, :]  # 3x3 grid, position (1,1)\n",
    "    print(\"3x3 patch around value 5:\")\n",
    "    print(center_patch.reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "56ea00d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coordinate-based similarity approach ===\n",
      "Input tensor:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 3., 0.],\n",
      "        [0., 4., 5., 6., 0.],\n",
      "        [0., 7., 8., 9., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Original shape: torch.Size([1, 1, 5, 5])\n",
      "Padded shape: torch.Size([1, 1, 7, 7])\n",
      "Flattened coordinate shape: torch.Size([1, 2, 49])\n",
      "Similarity matrix shape: torch.Size([1, 49, 49])\n",
      "Padded and flattened tensor:\n",
      "Shape: torch.Size([1, 1, 49])\n",
      "Values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2.,\n",
      "        3., 0., 0., 0., 0., 4., 5., 6., 0., 0., 0., 0., 7., 8., 9., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Value 5.0 found at flattened position 24 (2D: row=3, col=3)\n",
      "Value 1.0 found at flattened position 16 (2D: row=2, col=2)\n",
      "\n",
      "Neighbors for center position (value 5) at position 24:\n",
      "Values: tensor([5., 2., 4., 6., 8., 1., 3., 7., 9.])\n",
      "Indices: tensor([24, 17, 23, 25, 31, 16, 18, 30, 32])\n",
      "Neighbor positions in 2D grid:\n",
      "  Index 24: (row=3, col=3), value=5.0\n",
      "  Index 17: (row=2, col=3), value=2.0\n",
      "  Index 23: (row=3, col=2), value=4.0\n",
      "  Index 25: (row=3, col=4), value=6.0\n",
      "  Index 31: (row=4, col=3), value=8.0\n",
      "  Index 16: (row=2, col=2), value=1.0\n",
      "  Index 18: (row=2, col=4), value=3.0\n",
      "  Index 30: (row=4, col=2), value=7.0\n",
      "  Index 32: (row=4, col=4), value=9.0\n",
      "\n",
      "Neighbors for position with value 1 at position 16:\n",
      "Values: tensor([1., 4., 2., 0., 0., 5., 0., 0., 0.])\n",
      "Indices: tensor([16, 23, 17, 15,  9, 24, 22,  8, 10])\n",
      "Neighbor positions in 2D grid:\n",
      "  Index 16: (row=2, col=2), value=1.0\n",
      "  Index 23: (row=3, col=2), value=4.0\n",
      "  Index 17: (row=2, col=3), value=2.0\n",
      "  Index 15: (row=2, col=1), value=0.0\n",
      "  Index 9: (row=1, col=2), value=0.0\n",
      "  Index 24: (row=3, col=3), value=5.0\n",
      "  Index 22: (row=3, col=1), value=0.0\n",
      "  Index 8: (row=1, col=1), value=0.0\n",
      "  Index 10: (row=1, col=3), value=0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _add_coordinate_encoding(x):\n",
    "    \"\"\"Add coordinate channels to input tensor\"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords = torch.linspace(-1, 1, h, device=x.device)\n",
    "    x_coords = torch.linspace(-1, 1, w, device=x.device)\n",
    "    \n",
    "    y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack coordinates and add batch dimension\n",
    "    coord_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0)  # [1, 2, H, W]\n",
    "    coord_grid = coord_grid.expand(b, -1, -1, -1)  # [B, 2, H, W]\n",
    "    \n",
    "    return coord_grid\n",
    "\n",
    "def _calculate_coordinate_similarity_matrix(coord_matrix, sigma=0.3):\n",
    "    \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "    b, c, t = coord_matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "    \n",
    "    # Calculate pairwise Euclidean distances between coordinates\n",
    "    coord_expanded_1 = coord_matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "    coord_expanded_2 = coord_matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "    \n",
    "    # Euclidean distance between coordinates\n",
    "    coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "    coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1) + 1e-8)  # [B, T, T]\n",
    "    \n",
    "    # Convert distance to similarity using Gaussian kernel\n",
    "    similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def get_spatial_neighbors(x, K=9, sigma=0.3):\n",
    "    \"\"\"\n",
    "    Get K nearest spatial neighbors using coordinate-based similarity\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Add padding\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    padded_h, padded_w = x_padded.shape[2], x_padded.shape[3]\n",
    "    \n",
    "    # Get coordinate encoding for the padded tensor\n",
    "    coord_grid = _add_coordinate_encoding(x_padded)  # [B, 2, H+2, W+2]\n",
    "    \n",
    "    # Flatten spatial dimensions\n",
    "    x_flat = x_padded.flatten(2)  # [B, C, (H+2)*(W+2)]\n",
    "    coord_flat = coord_grid.flatten(2)  # [B, 2, (H+2)*(W+2)]\n",
    "    \n",
    "    print(f\"Original shape: {x.shape}\")\n",
    "    print(f\"Padded shape: {x_padded.shape}\")\n",
    "    print(f\"Flattened coordinate shape: {coord_flat.shape}\")\n",
    "    \n",
    "    # Calculate similarity matrix based on coordinates\n",
    "    similarity_matrix = _calculate_coordinate_similarity_matrix(coord_flat, sigma=sigma)\n",
    "    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "    \n",
    "    # Get top K neighbors for each position\n",
    "    topk_values, topk_indices = torch.topk(similarity_matrix, k=K, dim=2, largest=True)\n",
    "    \n",
    "    # Expand indices to match all channels\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, -1, K)  # [B, C, T, K]\n",
    "    \n",
    "    # Gather the neighboring values\n",
    "    x_expanded = x_flat.unsqueeze(-1).expand(b, c, -1, K)\n",
    "    neighbors = torch.gather(x_expanded, dim=2, index=topk_indices_exp)\n",
    "    \n",
    "    return neighbors, topk_indices, similarity_matrix, x_flat, padded_h, padded_w\n",
    "\n",
    "def find_value_position(x_flat, value, padded_h, padded_w):\n",
    "    \"\"\"Find the flattened position of a specific value\"\"\"\n",
    "    # Find where the value occurs\n",
    "    positions = (x_flat[0, 0, :] == value).nonzero(as_tuple=False).flatten()\n",
    "    if len(positions) > 0:\n",
    "        pos = positions[0].item()\n",
    "        # Convert back to 2D coordinates for verification\n",
    "        row = pos // padded_w\n",
    "        col = pos % padded_w\n",
    "        print(f\"Value {value} found at flattened position {pos} (2D: row={row}, col={col})\")\n",
    "        return pos\n",
    "    else:\n",
    "        print(f\"Value {value} not found\")\n",
    "        return None\n",
    "\n",
    "def test_spatial_neighbors():\n",
    "    # Create test tensor - your example\n",
    "    x = torch.tensor([[[\n",
    "        [0., 0., 0., 0., 0.],\n",
    "        [0., 1., 2., 3., 0.],\n",
    "        [0., 4., 5., 6., 0.],\n",
    "        [0., 7., 8., 9., 0.],\n",
    "        [0., 0., 0., 0., 0.]\n",
    "    ]]])\n",
    "    \n",
    "    print(\"Input tensor:\")\n",
    "    print(x.squeeze())\n",
    "    print()\n",
    "    \n",
    "    # Get 9 nearest neighbors\n",
    "    neighbors, indices, sim_matrix, x_flat, padded_h, padded_w = get_spatial_neighbors(x, K=9, sigma=0.3)\n",
    "    \n",
    "    print(\"Padded and flattened tensor:\")\n",
    "    print(\"Shape:\", x_flat.shape)\n",
    "    print(\"Values:\", x_flat[0, 0, :])\n",
    "    print()\n",
    "    \n",
    "    # Find positions of values 5 and 1\n",
    "    pos_5 = find_value_position(x_flat, 5.0, padded_h, padded_w)\n",
    "    pos_1 = find_value_position(x_flat, 1.0, padded_h, padded_w)\n",
    "    \n",
    "    if pos_5 is not None:\n",
    "        print(f\"\\nNeighbors for center position (value 5) at position {pos_5}:\")\n",
    "        center_neighbors = neighbors[0, 0, pos_5, :]\n",
    "        print(\"Values:\", center_neighbors)\n",
    "        print(\"Indices:\", indices[0, pos_5, :])\n",
    "        \n",
    "        # Show the neighbor positions in 2D grid for verification\n",
    "        print(\"Neighbor positions in 2D grid:\")\n",
    "        for i, idx in enumerate(indices[0, pos_5, :]):\n",
    "            row = idx.item() // padded_w\n",
    "            col = idx.item() % padded_w\n",
    "            val = center_neighbors[i].item()\n",
    "            print(f\"  Index {idx.item()}: (row={row}, col={col}), value={val}\")\n",
    "    \n",
    "    if pos_1 is not None:\n",
    "        print(f\"\\nNeighbors for position with value 1 at position {pos_1}:\")\n",
    "        neighbors_1 = neighbors[0, 0, pos_1, :]\n",
    "        print(\"Values:\", neighbors_1)\n",
    "        print(\"Indices:\", indices[0, pos_1, :])\n",
    "        \n",
    "        # Show the neighbor positions in 2D grid for verification\n",
    "        print(\"Neighbor positions in 2D grid:\")\n",
    "        for i, idx in enumerate(indices[0, pos_1, :]):\n",
    "            row = idx.item() // padded_w\n",
    "            col = idx.item() % padded_w\n",
    "            val = neighbors_1[i].item()\n",
    "            print(f\"  Index {idx.item()}: (row={row}, col={col}), value={val}\")\n",
    "    \n",
    "    return neighbors, indices, sim_matrix\n",
    "\n",
    "def get_3x3_neighbors_direct(x):\n",
    "    \"\"\"\n",
    "    Directly get 3x3 neighbors without similarity matrix calculation\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Pad the input\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    \n",
    "    # Use unfold to get 3x3 patches\n",
    "    patches = x_padded.unfold(2, 3, 1).unfold(3, 3, 1)  # [B, C, H, W, 3, 3]\n",
    "    patches = patches.reshape(b, c, h*w, 9)  # [B, C, H*W, 9]\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Test both approaches\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Coordinate-based similarity approach ===\")\n",
    "    neighbors, indices, sim_matrix = test_spatial_neighbors()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b85b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a5fe483",
   "metadata": {},
   "source": [
    "# MyTopK function\n",
    "- need to make selecting Ks more stable (same pattern for when comparing same similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "f0822b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k values: tensor([0.5000, 0.5000])\n",
      "Original indices of top-k values: tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor\n",
    "data = torch.tensor([0.1, 0.5, 0.2, 0.5, 0.3])\n",
    "k = 2\n",
    "\n",
    "# Get sorted values and indices with stable tie-breaking\n",
    "sorted_values, sorted_indices = torch.sort(data, descending=True, stable=True)\n",
    "\n",
    "# Select the top-k elements and their original indices\n",
    "topk_values = sorted_values[:k]\n",
    "topk_original_indices = sorted_indices[:k]\n",
    "\n",
    "print(f\"Top-k values: {topk_values}\")\n",
    "print(f\"Original indices of top-k values: {topk_original_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a7eefe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k values: tensor([0.5000, 0.5000])\n",
      "Original indices of top-k values: tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor\n",
    "data = torch.tensor([0.1, 0.5, 0.2, 0.5, 0.3])\n",
    "k = 2\n",
    "\n",
    "topk_values, topk_indices = torch.topk(data, k, largest=True)\n",
    "print(f\"Top-k values: {topk_values}\")\n",
    "print(f\"Original indices of top-k values: {topk_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f067a",
   "metadata": {},
   "source": [
    "## New Prime_n and calculate_similarity_n \n",
    "Sept 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "55c65f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a80a4fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 36])\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(8, 3, 1024)\n",
    "mat2 = torch.randn(8, 3, 36)\n",
    "sigma = 0.1\n",
    "\n",
    "mat1_exp = mat1.unsqueeze(3)  # [B, 3, T, 1]\n",
    "mat2_exp = mat2.unsqueeze(2)  # [B, 3, 1, t]\n",
    "\n",
    "mat_diff = mat1_exp - mat2_exp # [B, 3, T, t]\n",
    "\n",
    "mat_diff = torch.sqrt(torch.sum(mat_diff ** 2, dim=1) + 1e-8)\n",
    "\n",
    "sim_mat = torch.exp(-mat_diff ** 2/(2 * sigma **2))\n",
    "\n",
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ae6b8dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_similarity_matrix_N(self, matrix, matrix_sample):\n",
    "    # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "    similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_sample)\n",
    "    similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fcc115f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 36])\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(8, 3, 1024)\n",
    "mat2 = torch.randn(8, 3, 36)\n",
    "\n",
    "sim_mat = _calculate_similarity_matrix_N(None, mat1, mat2)\n",
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3d88ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "    b, c, t = matrix.shape\n",
    "    \n",
    "    _, topk_indices = torch.topk(magnitude_matrix, k=K - 1, dim=2, largest=maximum)\n",
    "    tk = topk_indices.shape[-1]\n",
    "    assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "    # Map sample indices back to original matrix positions\n",
    "    mapped_tensor = rand_idx[topk_indices]\n",
    "    token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "    final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "    indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "    # Gather matrix values and apply similarity weighting\n",
    "    matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "    prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "    prime = prime.view(b, c, -1)\n",
    "    return prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd4d50",
   "metadata": {},
   "source": [
    "# Sanity Sep 4 - Thursday \n",
    "- need to make sure after 1 layer of Conv2d == ConvNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7e632d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc1ed2",
   "metadata": {},
   "source": [
    "### I. Same Filter Initialization\n",
    "- Kernel size = 3 with 1s and k = 0 with 1s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "17a427a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_NN_sanity(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            K,\n",
    "            stride, \n",
    "            padding, \n",
    "            sampling_type, \n",
    "            num_samples, \n",
    "            sample_padding, # NOT IN USE AS OF NOW\n",
    "            shuffle_pattern, \n",
    "            shuffle_scale, \n",
    "            magnitude_type,\n",
    "            coordinate_encoding\n",
    "                ):\n",
    "        super(Conv2d_NN_sanity, self).__init__()\n",
    "\n",
    "        assert K == stride, \"K must be equal to stride for ConvNN.\"\n",
    "\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples\n",
    "        self.sample_padding = sample_padding\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers \n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        \n",
    "        # Conv1d Layer \n",
    "        self.conv1d_layer = nn.Conv1d(\n",
    "            in_channels=self.in_channels, # + 2, ## CHANGE IF NEEDED\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "        ones = torch.ones(1, 3, 9)\n",
    "        self.conv1d_layer = F.conv1d()\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=2) \n",
    "        self.unflatten = None\n",
    "\n",
    "\n",
    "        # Shapes of tensors\n",
    "        self.og_shape = None \n",
    "        self.pad_shape = None\n",
    "\n",
    "        init_h, init_w = None, None \n",
    "        padded_h, padded_w = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.og_shape:\n",
    "            self.og_shape = x.shape\n",
    "        print(\"Original x shape: \", self.og_shape)\n",
    "        x = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0) if self.padding > 0 else x\n",
    "        \n",
    "        if not self.pad_shape:\n",
    "            self.pad_shape = x.shape\n",
    "        print(\"Padded x shape: \", self.pad_shape)\n",
    "\n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        print(\"coor shape: \", x.shape)\n",
    "        x = self.flatten(x)\n",
    "        print(\"flattened shape: \", x.shape)\n",
    "\n",
    "        x_dist = x[:, -2:, :]\n",
    "        x = x[:, :-2, :] \n",
    "\n",
    "        if self.sampling_type == \"all\":\n",
    "            similarity_matrix = self._calculate_similarity_matrix(x_dist)\n",
    "            prime = self._prime(x, similarity_matrix, self.K, maximum=True)\n",
    "        print(\"prime shape: \", prime.shape)\n",
    "        x = self.conv1d_layer(prime)\n",
    "        print(\"conv1d shape: \", x.shape)\n",
    "        # print(x.shape)\n",
    "        if not self.unflatten:\n",
    "            self.unflatten = nn.Unflatten(dim=2, unflattened_size=self.og_shape[2:])\n",
    "\n",
    "        x = self.unflatten(x)\n",
    "        print(\"unflattened shape: \", x.shape)\n",
    "        # print(x.shape)\n",
    "\n",
    "        print(\"final shape: \", x.shape)\n",
    "\n",
    "        print(\"sleeping for 2 seconds\")\n",
    "        time.sleep(2)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _calculate_similarity_matrix(self, matrix, sigma=0.1):\n",
    "        \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "        b, c, t = matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "\n",
    "        ### TODO CHANGE IF NOT USING DISTANCE ANYMORE\n",
    "        # coord_matrix = matrix[:, -2:, :]\n",
    "        coord_matrix = matrix\n",
    "\n",
    "        # Calculate pairwise Euclidean distances between coordinates\n",
    "        coord_expanded_1 = coord_matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "        coord_expanded_2 = coord_matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "\n",
    "        # Euclidean distance between coordinates\n",
    "        coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "        coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1) + 1e-8)  # [B, T, T]\n",
    "        \n",
    "        # Convert distance to similarity using Gaussian kernel\n",
    "        similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        \"\"\" ORIGINAL\n",
    "        _, topk_indices = torch.topk(magnitude_matrix.detach(), k=K, dim=2, largest=maximum)\n",
    "        print(\"Top-k Indices\")\n",
    "        print(topk_indices.shape)\n",
    "        \"\"\"\n",
    "        # New My TopK\n",
    "        _, sorted_indices = torch.sort(magnitude_matrix.detach(), dim=2, descending=True, stable=True)\n",
    "        topk_indices = sorted_indices[:, :, :K]\n",
    "        \n",
    "        # End of My TopK\n",
    "        \n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "        # prime, _ = self.filter_non_zero_starting_rows_multichannel(prime)\n",
    "        # b, c, num_filtered_rows, k = prime.shape\n",
    "        print()\n",
    "        print(\"With Padding Ks\")\n",
    "        print(prime.shape)\n",
    "        print(prime)\n",
    "        print()\n",
    "        if self.padding > 0:\n",
    "            prime = prime.view(b, c, self.pad_shape[-2], self.pad_shape[-1], K)\n",
    "            print(\"Prime with Padded shape:\")\n",
    "            print(prime.shape)\n",
    "            print(prime)\n",
    "            print()\n",
    "            prime = prime[:, :, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "            print(\"Without Padding Ks\")\n",
    "            print(prime.shape)\n",
    "            print(prime)\n",
    "\n",
    "            prime = prime.reshape(b, c, K * self.og_shape[-2] * self.og_shape[-1])\n",
    "        else: \n",
    "            prime = prime.view(b, c, -1)\n",
    "        \n",
    "        return prime\n",
    "\n",
    "    def filter_non_zero_starting_rows_multichannel(self, tensor):\n",
    "        \"\"\"\n",
    "        Filter rows based on the first element of the first channel being non-zero\n",
    "        \n",
    "        Args:\n",
    "            tensor: Input tensor of shape [B, C, num_rows, row_length]\n",
    "        \n",
    "        Returns:\n",
    "            Filtered tensor with only rows where first channel's first element != 0\n",
    "        \"\"\"\n",
    "        # Get the shape\n",
    "        b, c, num_rows, row_length = tensor.shape\n",
    "        \n",
    "        # Create mask based on first channel only\n",
    "        # tensor[:, 0, :, 0] gets first element of each row in first channel\n",
    "            \n",
    "        mask = tensor[:, 0, :, 0].detach() != 0  # Shape: [b, num_rows]\n",
    "        \n",
    "        # Get indices of non-zero starting rows\n",
    "        non_zero_indices = torch.where(mask[0])[0]  # [0] because batch dimension\n",
    "    \n",
    "        # Select rows from ALL channels\n",
    "        filtered_tensor = tensor[:, :, non_zero_indices, :]\n",
    "        \n",
    "        return filtered_tensor, non_zero_indices\n",
    "\n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "                x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "                y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "                grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "                expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "                self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa706d",
   "metadata": {},
   "source": [
    "# Initialize weights as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f580a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize weight as 1 and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "45ca200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "ex = torch.Tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3, 4, 5],\n",
    "                [5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14],\n",
    "                [15, 16, 17, 18, 19],\n",
    "                [20, 21, 22, 23, 24]\n",
    "            ], \n",
    "            [\n",
    "                [1, 2, 3, 4, 5],\n",
    "                [5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14], \n",
    "                [15, 16, 17, 18, 19],\n",
    "                [20, 21, 22, 23, 24]\n",
    "            ], \n",
    "            [\n",
    "                [1, 2, 3, 4, 5],\n",
    "                [5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14], \n",
    "                [15, 16, 17, 18, 19],\n",
    "                [20, 21, 22, 23, 24]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "print(ex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5268293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: torch.Size([1, 1, 5, 5])\n",
      "tensor([[[[ 42.,  72.,  90., 108.,  78.],\n",
      "          [105., 171., 198., 225., 159.],\n",
      "          [189., 297., 324., 351., 243.],\n",
      "          [279., 432., 459., 486., 333.],\n",
      "          [216., 333., 351., 369., 252.]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "conv2d.weight.data.fill_(1.0)\n",
    "\n",
    "conv2d_out = conv2d(ex) \n",
    "print(\"Conv2d output shape:\", conv2d_out.shape)\n",
    "print(conv2d_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2d_NN_sanity(nn.Module):\n",
    "    def __init__(self, \n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            K,\n",
    "            stride, \n",
    "            padding, \n",
    "            sampling_type, \n",
    "            num_samples, \n",
    "            sample_padding, # NOT IN USE AS OF NOW\n",
    "            shuffle_pattern, \n",
    "            shuffle_scale, \n",
    "            magnitude_type,\n",
    "            coordinate_encoding\n",
    "                ):\n",
    "        super(Conv2d_NN_sanity, self).__init__()\n",
    "\n",
    "        assert K == stride, \"K must be equal to stride for ConvNN.\"\n",
    "\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples\n",
    "        self.sample_padding = sample_padding\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers \n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        \n",
    "        # Conv1d Layer \n",
    "        self.conv1d_layer = nn.Conv1d(\n",
    "            in_channels=self.in_channels, # + 2, ## CHANGE IF NEEDED\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.stride,\n",
    "            bias=False\n",
    "        )\n",
    "        self.conv1d_layer.weight.data.fill_(1.0)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=2) \n",
    "        self.unflatten = None\n",
    "\n",
    "\n",
    "        # Shapes of tensors\n",
    "        self.og_shape = None \n",
    "        self.pad_shape = None\n",
    "\n",
    "        init_h, init_w = None, None \n",
    "        padded_h, padded_w = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.og_shape:\n",
    "            self.og_shape = x.shape\n",
    "        # print(\"Original x shape: \", self.og_shape)\n",
    "        x = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0) if self.padding > 0 else x\n",
    "        \n",
    "        if not self.pad_shape:\n",
    "            self.pad_shape = x.shape\n",
    "        # print(\"Padded x shape: \", self.pad_shape)\n",
    "\n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        # print(\"coor shape: \", x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print(\"flattened shape: \", x.shape)\n",
    "\n",
    "        x_dist = x[:, -2:, :]\n",
    "        x = x[:, :-2, :] \n",
    "        if self.sampling_type == \"all\":\n",
    "            similarity_matrix = self._calculate_similarity_matrix(x_dist)\n",
    "            prime = self._prime(x, similarity_matrix, self.K, maximum=True)\n",
    "            # print(\"prime shape: \", prime.shape)\n",
    "        x = self.conv1d_layer(prime)\n",
    "        # print(\"conv1d shape: \", x.shape)\n",
    "        # print(x.shape)\n",
    "        if not self.unflatten:\n",
    "            self.unflatten = nn.Unflatten(dim=2, unflattened_size=self.og_shape[2:])\n",
    "\n",
    "        x = self.unflatten(x)\n",
    "        # print(\"unflattened shape: \", x.shape)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # print(\"final shape: \", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _calculate_similarity_matrix(self, matrix, sigma=0.1):\n",
    "        \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "        b, c, t = matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "\n",
    "        ### TODO CHANGE IF NOT USING DISTANCE ANYMORE\n",
    "        # coord_matrix = matrix[:, -2:, :]\n",
    "        coord_matrix = matrix\n",
    "\n",
    "        # Calculate pairwise Euclidean distances between coordinates\n",
    "        coord_expanded_1 = coord_matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "        coord_expanded_2 = coord_matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "\n",
    "        # Euclidean distance between coordinates\n",
    "        coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "        coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1) + 1e-8)  # [B, T, T]\n",
    "        \n",
    "        # Convert distance to similarity using Gaussian kernel\n",
    "        similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        \"\"\" ORIGINAL\n",
    "        _, topk_indices = torch.topk(magnitude_matrix.detach(), k=K, dim=2, largest=maximum)\n",
    "        print(\"Top-k Indices\")\n",
    "        print(topk_indices.shape)\n",
    "        \"\"\"\n",
    "        # New My TopK\n",
    "        _, sorted_indices = torch.sort(magnitude_matrix.detach(), dim=2, descending=True, stable=True)\n",
    "        topk_indices = sorted_indices[:, :, :K]\n",
    "        \n",
    "        # End of My TopK\n",
    "        \n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "        # prime, _ = self.filter_non_zero_starting_rows_multichannel(prime)\n",
    "        # b, c, num_filtered_rows, k = prime.shape\n",
    "        # print()\n",
    "        # print(\"With Padding Ks\")\n",
    "        # print(prime.shape)\n",
    "        # print(prime)\n",
    "        # print()\n",
    "        if self.padding > 0:\n",
    "            prime = prime.view(b, c, self.pad_shape[-2], self.pad_shape[-1], K)\n",
    "            # print(\"Prime with Padded shape:\")\n",
    "            # print(prime.shape)\n",
    "            # print(prime)\n",
    "            # print()\n",
    "            prime = prime[:, :, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "            # print(\"Without Padding Ks\")\n",
    "            # print(prime.shape)\n",
    "            # print(prime)\n",
    "\n",
    "            prime = prime.reshape(b, c, K * self.og_shape[-2] * self.og_shape[-1])\n",
    "        else: \n",
    "            prime = prime.view(b, c, -1)\n",
    "        \n",
    "        return prime\n",
    "\n",
    "    def filter_non_zero_starting_rows_multichannel(self, tensor):\n",
    "        \"\"\"\n",
    "        Filter rows based on the first element of the first channel being non-zero\n",
    "        \n",
    "        Args:\n",
    "            tensor: Input tensor of shape [B, C, num_rows, row_length]\n",
    "        \n",
    "        Returns:\n",
    "            Filtered tensor with only rows where first channel's first element != 0\n",
    "        \"\"\"\n",
    "        # Get the shape\n",
    "        b, c, num_rows, row_length = tensor.shape\n",
    "        \n",
    "        # Create mask based on first channel only\n",
    "        # tensor[:, 0, :, 0] gets first element of each row in first channel\n",
    "            \n",
    "        mask = tensor[:, 0, :, 0].detach() != 0  # Shape: [b, num_rows]\n",
    "        \n",
    "        # Get indices of non-zero starting rows\n",
    "        non_zero_indices = torch.where(mask[0])[0]  # [0] because batch dimension\n",
    "    \n",
    "        # Select rows from ALL channels\n",
    "        filtered_tensor = tensor[:, :, non_zero_indices, :]\n",
    "        \n",
    "        return filtered_tensor, non_zero_indices\n",
    "\n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "                x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "                y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "                grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "                expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "                self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f9c40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "04e0d6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNN output shape: torch.Size([1, 1, 5, 5])\n",
      "tensor([[[[ 42.,  72.,  90., 108.,  78.],\n",
      "          [105., 171., 198., 225., 159.],\n",
      "          [189., 297., 324., 351., 243.],\n",
      "          [279., 432., 459., 486., 333.],\n",
      "          [216., 333., 351., 369., 252.]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "convnn = Conv2d_NN_sanity(\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    K=9,\n",
    "    stride=9,\n",
    "    padding=1,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=1,\n",
    "    sample_padding=0,\n",
    "    shuffle_pattern=\"none\",\n",
    "    shuffle_scale=1.0,\n",
    "    magnitude_type=\"none\",\n",
    "    coordinate_encoding=True\n",
    ")\n",
    "\n",
    "convnn_out = convnn(ex)\n",
    "print(\"ConvNN output shape:\", convnn_out.shape)\n",
    "print(convnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3bbdbd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Without Bias\\n\\nConv2d output shape: torch.Size([1, 1, 5, 5])\\ntensor([[[[ 42.,  72.,  90., 108.,  78.],\\n          [105., 171., 198., 225., 159.],\\n          [189., 297., 324., 351., 243.],\\n          [279., 432., 459., 486., 333.],\\n          [216., 333., 351., 369., 252.]]]], grad_fn=<ConvolutionBackward0>)\\n\\nConvNN output shape: torch.Size([1, 1, 5, 5])\\ntensor([[[[ 42.,  72.,  90., 108.,  78.],\\n          [105., 171., 198., 225., 159.],\\n          [189., 297., 324., 351., 243.],\\n          [279., 432., 459., 486., 333.],\\n          [216., 333., 351., 369., 252.]]]], grad_fn=<ViewBackward0>)\\n\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Without Bias\n",
    "\n",
    "Conv2d output shape: torch.Size([1, 1, 5, 5])\n",
    "tensor([[[[ 42.,  72.,  90., 108.,  78.],\n",
    "          [105., 171., 198., 225., 159.],\n",
    "          [189., 297., 324., 351., 243.],\n",
    "          [279., 432., 459., 486., 333.],\n",
    "          [216., 333., 351., 369., 252.]]]], grad_fn=<ConvolutionBackward0>)\n",
    "\n",
    "ConvNN output shape: torch.Size([1, 1, 5, 5])\n",
    "tensor([[[[ 42.,  72.,  90., 108.,  78.],\n",
    "          [105., 171., 198., 225., 159.],\n",
    "          [189., 297., 324., 351., 243.],\n",
    "          [279., 432., 459., 486., 333.],\n",
    "          [216., 333., 351., 369., 252.]]]], grad_fn=<ViewBackward0>)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6b660f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With Bias \\n\\nConv2d output shape: torch.Size([1, 1, 5, 5])\\ntensor([[[[ 42.1061,  72.1061,  90.1061, 108.1061,  78.1061],\\n          [105.1061, 171.1061, 198.1061, 225.1061, 159.1061],\\n          [189.1061, 297.1061, 324.1061, 351.1061, 243.1061],\\n          [279.1061, 432.1061, 459.1061, 486.1061, 333.1061],\\n          [216.1061, 333.1061, 351.1061, 369.1061, 252.1061]]]],\\n       grad_fn=<ConvolutionBackward0>)\\n\\nConvNN output shape: torch.Size([1, 1, 5, 5])\\ntensor([[[[ 41.9098,  71.9098,  89.9098, 107.9098,  77.9098],\\n          [104.9098, 170.9098, 197.9098, 224.9098, 158.9098],\\n          [188.9098, 296.9098, 323.9098, 350.9098, 242.9098],\\n          [278.9098, 431.9098, 458.9098, 485.9098, 332.9098],\\n          [215.9098, 332.9098, 350.9098, 368.9098, 251.9098]]]],\\n       grad_fn=<ViewBackward0>)\\n\\n'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"With Bias \n",
    "\n",
    "Conv2d output shape: torch.Size([1, 1, 5, 5])\n",
    "tensor([[[[ 42.1061,  72.1061,  90.1061, 108.1061,  78.1061],\n",
    "          [105.1061, 171.1061, 198.1061, 225.1061, 159.1061],\n",
    "          [189.1061, 297.1061, 324.1061, 351.1061, 243.1061],\n",
    "          [279.1061, 432.1061, 459.1061, 486.1061, 333.1061],\n",
    "          [216.1061, 333.1061, 351.1061, 369.1061, 252.1061]]]],\n",
    "       grad_fn=<ConvolutionBackward0>)\n",
    "\n",
    "ConvNN output shape: torch.Size([1, 1, 5, 5])\n",
    "tensor([[[[ 41.9098,  71.9098,  89.9098, 107.9098,  77.9098],\n",
    "          [104.9098, 170.9098, 197.9098, 224.9098, 158.9098],\n",
    "          [188.9098, 296.9098, 323.9098, 350.9098, 242.9098],\n",
    "          [278.9098, 431.9098, 458.9098, 485.9098, 332.9098],\n",
    "          [215.9098, 332.9098, 350.9098, 368.9098, 251.9098]]]],\n",
    "       grad_fn=<ViewBackward0>)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "32f38161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(conv2d.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d06ac30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConv2d - [1, 3, 3, 3]\\ntensor([[[[1., 1., 1.],\\n          [1., 1., 1.],\\n          [1., 1., 1.]],\\n\\n         [[1., 1., 1.],\\n          [1., 1., 1.],\\n          [1., 1., 1.]],\\n\\n         [[1., 1., 1.],\\n          [1., 1., 1.],\\n          [1., 1., 1.]]]])\\n\\n\\nConv1d - [1, 3, 9]\\ntensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1.],\\n         [1., 1., 1., 1., 1., 1., 1., 1., 1.],\\n         [1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\\n'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conv2d - [1, 3, 3, 3]\n",
    "tensor([[[[1., 1., 1.],\n",
    "          [1., 1., 1.],\n",
    "          [1., 1., 1.]],\n",
    "\n",
    "         [[1., 1., 1.],\n",
    "          [1., 1., 1.],\n",
    "          [1., 1., 1.]],\n",
    "\n",
    "         [[1., 1., 1.],\n",
    "          [1., 1., 1.],\n",
    "          [1., 1., 1.]]]])\n",
    "\n",
    "\n",
    "Conv1d - [1, 3, 9]\n",
    "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "         [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "         [1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf9a24",
   "metadata": {},
   "source": [
    "## II. Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325869b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.Tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3, 4, 5],\n",
    "                [5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14],\n",
    "                [15, 16, 17, 18, 19],\n",
    "                [20, 21, 22, 23, 24]\n",
    "            ], \n",
    "            [\n",
    "                [1, 2, 3, 4, 5],\n",
    "                [5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14], \n",
    "                [15, 16, 17, 18, 19],\n",
    "                [20, 21, 22, 23, 24]\n",
    "            ], \n",
    "            [\n",
    "                [1, 2, 3, 4, 5],\n",
    "                [5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14], \n",
    "                [15, 16, 17, 18, 19],\n",
    "                [20, 21, 22, 23, 24]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "label = torch.Tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30]\n",
    "            ], \n",
    "            [\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30], \n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30]\n",
    "            ], \n",
    "            [\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30], \n",
    "                [30, 30, 30, 30, 30],\n",
    "                [30, 30, 30, 30, 30]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "28630a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 42.,  72.,  90., 108.,  78.],\n",
      "          [105., 171., 198., 225., 159.],\n",
      "          [189., 297., 324., 351., 243.],\n",
      "          [279., 432., 459., 486., 333.],\n",
      "          [216., 333., 351., 369., 252.]]]], grad_fn=<ConvolutionBackward0>)\n",
      "Initial Loss: 62190.0\n",
      "Conv2d Weights Gradient: tensor([[[[3697.4399, 4549.6802, 4012.3201],\n",
      "          [5495.0400, 6628.0801, 5750.3999],\n",
      "          [4866.0000, 5803.6797, 4986.9600]],\n",
      "\n",
      "         [[3697.4399, 4549.6802, 4012.3201],\n",
      "          [5495.0400, 6628.0801, 5750.3999],\n",
      "          [4866.0000, 5803.6797, 4986.9600]],\n",
      "\n",
      "         [[3697.4399, 4549.6802, 4012.3201],\n",
      "          [5495.0400, 6628.0801, 5750.3999],\n",
      "          [4866.0000, 5803.6797, 4986.9600]]]])\n",
      "Conv2d Weights: Parameter containing:\n",
      "tensor([[[[-35.9744, -44.4968, -39.1232],\n",
      "          [-53.9504, -65.2808, -56.5040],\n",
      "          [-47.6600, -57.0368, -48.8696]],\n",
      "\n",
      "         [[-35.9744, -44.4968, -39.1232],\n",
      "          [-53.9504, -65.2808, -56.5040],\n",
      "          [-47.6600, -57.0368, -48.8696]],\n",
      "\n",
      "         [[-35.9744, -44.4968, -39.1232],\n",
      "          [-53.9504, -65.2808, -56.5040],\n",
      "          [-47.6600, -57.0368, -48.8696]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Update weights for Conv2d \n",
    "criteria1 = nn.MSELoss()\n",
    "optimizer1 = torch.optim.SGD(conv2d.parameters(), lr=0.01)\n",
    "\n",
    "output = conv2d(input) \n",
    "print(output)\n",
    "loss = criteria1(output, label)\n",
    "print(f\"Initial Loss: {loss.item()}\")\n",
    "loss.backward()\n",
    "optimizer1.step()\n",
    "\n",
    "print(\"Conv2d Weights Gradient:\", conv2d.weight.grad)\n",
    "print(\"Conv2d Weights:\", conv2d.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "29fa6377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 42.,  72.,  90., 108.,  78.],\n",
      "          [105., 171., 198., 225., 159.],\n",
      "          [189., 297., 324., 351., 243.],\n",
      "          [279., 432., 459., 486., 333.],\n",
      "          [216., 333., 351., 369., 252.]]]], grad_fn=<ViewBackward0>)\n",
      "Initial Loss: 62190.0\n",
      "Conv2d Weights Gradient: tensor([[[6628.0801, 6015.6001, 7020.4805, 5494.0801, 3068.6401, 5698.0801,\n",
      "          4763.0400, 4502.4004, 2599.2000],\n",
      "         [6628.0801, 6015.6001, 7020.4805, 5494.0801, 3068.6401, 5698.0801,\n",
      "          4763.0400, 4502.4004, 2599.2000],\n",
      "         [6628.0801, 6015.6001, 7020.4805, 5494.0801, 3068.6401, 5698.0801,\n",
      "          4763.0400, 4502.4004, 2599.2002]]])\n",
      "Conv2d Weights: Parameter containing:\n",
      "tensor([[[-65.2808, -59.1560, -69.2048, -53.9408, -29.6864, -55.9808, -46.6304,\n",
      "          -44.0240, -24.9920],\n",
      "         [-65.2808, -59.1560, -69.2048, -53.9408, -29.6864, -55.9808, -46.6304,\n",
      "          -44.0240, -24.9920],\n",
      "         [-65.2808, -59.1560, -69.2048, -53.9408, -29.6864, -55.9808, -46.6304,\n",
      "          -44.0240, -24.9920]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Update weights for Conv2d \n",
    "criteria2 = nn.MSELoss()\n",
    "optimizer2 = torch.optim.SGD(convnn.parameters(), lr=0.01)\n",
    "\n",
    "output = convnn(input) \n",
    "print(output)\n",
    "loss = criteria2(output, label)\n",
    "print(f\"Initial Loss: {loss.item()}\")\n",
    "loss.backward()\n",
    "optimizer2.step()\n",
    "\n",
    "print(\"Conv2d Weights Gradient:\", convnn.conv1d_layer.weight.grad)\n",
    "print(\"Conv2d Weights:\", convnn.conv1d_layer.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb2aa4",
   "metadata": {},
   "source": [
    "## III. 1x1 Convolution2d vs. K = 1 ConvNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f219a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ba17a2",
   "metadata": {},
   "source": [
    "# IV. Diagonal Values in similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9331e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75441c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1024, 1024])\n",
      "tensor([[ 0.9445,  0.5089,  0.7589, -2.4946, -0.7204],\n",
      "        [-1.6612,  0.8809,  0.9946,  0.9474,  1.1866],\n",
      "        [ 0.4319, -1.2607, -2.0790,  1.0553, -0.5461],\n",
      "        [ 1.1134, -1.2104, -0.4969,  1.1870, -0.3413],\n",
      "        [-0.5527, -0.2225,  0.0992,  0.4581,  0.4443]])\n"
     ]
    }
   ],
   "source": [
    "ex = torch.randn(3, 1024, 1024)\n",
    "print(ex.shape)\n",
    "\n",
    "print(ex[0, :5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9205878",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = torch.diagonal(ex, dim1=1, dim2=2).fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f7dcddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  0.5089,  0.7589, -2.4946, -0.7204],\n",
      "        [-1.6612,  1.0000,  0.9946,  0.9474,  1.1866],\n",
      "        [ 0.4319, -1.2607,  1.0000,  1.0553, -0.5461],\n",
      "        [ 1.1134, -1.2104, -0.4969,  1.0000, -0.3413],\n",
      "        [-0.5527, -0.2225,  0.0992,  0.4581,  1.0000]])\n",
      "tensor([[ 1.0000,  0.6430, -0.3798,  0.3133,  0.8300],\n",
      "        [ 0.5218,  1.0000,  1.4829, -1.0199,  0.0105],\n",
      "        [ 0.3890, -0.3033,  1.0000,  0.4287, -0.6139],\n",
      "        [ 1.3525, -0.2247,  0.7585,  1.0000,  0.9723],\n",
      "        [-1.0640,  0.8160,  0.6521,  0.4532,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(ex[0, :5, :5])\n",
    "print(ex[1, :5, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2260a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
