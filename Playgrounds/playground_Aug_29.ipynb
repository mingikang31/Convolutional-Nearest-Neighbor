{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a28c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc5e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_cache = {}\n",
    "def _add_coordinate_encoding(x):\n",
    "    b, _, h, w = x.shape\n",
    "    cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "    if cache_key in coordinate_cache:\n",
    "        expanded_grid = coordinate_cache[cache_key]\n",
    "    else:\n",
    "        y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "        x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "        y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "        grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "        expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "        coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "    x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "    return x_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fac47c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "ex = torch.rand(1, 3, 5, 5) \n",
    "ex_coord = _add_coordinate_encoding(ex)\n",
    "print(ex_coord.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fa5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "coord = ex_coord[:, -2:, :, :]\n",
    "print(coord.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94fa2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_similarity_matrix(matrix):\n",
    "    # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e368ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 25])\n",
      "tensor([[[ 1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "          -1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00],\n",
      "         [ 9.4868e-01,  1.0000e+00,  8.9443e-01,  6.0000e-01,  3.1623e-01,\n",
      "           8.0000e-01,  9.4868e-01,  8.9443e-01,  3.1623e-01,  1.4263e-08,\n",
      "           4.4721e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -4.4721e-01,\n",
      "          -1.4263e-08, -3.1623e-01, -8.9443e-01, -9.4868e-01, -8.0000e-01,\n",
      "          -3.1623e-01, -6.0000e-01, -8.9443e-01, -1.0000e+00, -9.4868e-01],\n",
      "         [ 7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "          -7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01],\n",
      "         [ 3.1623e-01,  6.0000e-01,  8.9443e-01,  1.0000e+00,  9.4868e-01,\n",
      "           1.4263e-08,  3.1623e-01,  8.9443e-01,  9.4868e-01,  8.0000e-01,\n",
      "          -4.4721e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  4.4721e-01,\n",
      "          -8.0000e-01, -9.4868e-01, -8.9443e-01, -3.1623e-01, -1.4263e-08,\n",
      "          -9.4868e-01, -1.0000e+00, -8.9443e-01, -6.0000e-01, -3.1623e-01],\n",
      "         [ 1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08],\n",
      "         [ 9.4868e-01,  8.0000e-01,  4.4721e-01,  1.4263e-08, -3.1623e-01,\n",
      "           1.0000e+00,  9.4868e-01,  4.4721e-01, -3.1623e-01, -6.0000e-01,\n",
      "           8.9443e-01,  8.9443e-01,  0.0000e+00, -8.9443e-01, -8.9443e-01,\n",
      "           6.0000e-01,  3.1623e-01, -4.4721e-01, -9.4868e-01, -1.0000e+00,\n",
      "           3.1623e-01, -1.4263e-08, -4.4721e-01, -8.0000e-01, -9.4868e-01],\n",
      "         [ 1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "          -1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00],\n",
      "         [ 7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "          -7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01],\n",
      "         [ 1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08],\n",
      "         [-3.1623e-01,  1.4263e-08,  4.4721e-01,  8.0000e-01,  9.4868e-01,\n",
      "          -6.0000e-01, -3.1623e-01,  4.4721e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -8.9443e-01, -8.9443e-01,  0.0000e+00,  8.9443e-01,  8.9443e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -4.4721e-01,  3.1623e-01,  6.0000e-01,\n",
      "          -9.4868e-01, -8.0000e-01, -4.4721e-01, -1.4263e-08,  3.1623e-01],\n",
      "         [ 7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           1.0000e+00,  1.0000e+00,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01],\n",
      "         [ 7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           1.0000e+00,  1.0000e+00,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
      "           8.9443e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -8.9443e-01,\n",
      "           7.0711e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -7.0711e-01],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [-7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -1.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01],\n",
      "         [-7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -1.0000e+00, -1.0000e+00,  0.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          -8.9443e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  8.9443e-01,\n",
      "          -7.0711e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  7.0711e-01],\n",
      "         [ 3.1623e-01, -1.4263e-08, -4.4721e-01, -8.0000e-01, -9.4868e-01,\n",
      "           6.0000e-01,  3.1623e-01, -4.4721e-01, -9.4868e-01, -1.0000e+00,\n",
      "           8.9443e-01,  8.9443e-01,  0.0000e+00, -8.9443e-01, -8.9443e-01,\n",
      "           1.0000e+00,  9.4868e-01,  4.4721e-01, -3.1623e-01, -6.0000e-01,\n",
      "           9.4868e-01,  8.0000e-01,  4.4721e-01,  1.4263e-08, -3.1623e-01],\n",
      "         [-1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08],\n",
      "         [-7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01],\n",
      "         [-1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "           1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00],\n",
      "         [-9.4868e-01, -8.0000e-01, -4.4721e-01, -1.4263e-08,  3.1623e-01,\n",
      "          -1.0000e+00, -9.4868e-01, -4.4721e-01,  3.1623e-01,  6.0000e-01,\n",
      "          -8.9443e-01, -8.9443e-01,  0.0000e+00,  8.9443e-01,  8.9443e-01,\n",
      "          -6.0000e-01, -3.1623e-01,  4.4721e-01,  9.4868e-01,  1.0000e+00,\n",
      "          -3.1623e-01,  1.4263e-08,  4.4721e-01,  8.0000e-01,  9.4868e-01],\n",
      "         [-1.2688e-08, -3.1623e-01, -7.0711e-01, -9.4868e-01, -1.0000e+00,\n",
      "           3.1623e-01, -1.2688e-08, -7.0711e-01, -1.0000e+00, -9.4868e-01,\n",
      "           7.0711e-01,  7.0711e-01,  0.0000e+00, -7.0711e-01, -7.0711e-01,\n",
      "           9.4868e-01,  1.0000e+00,  7.0711e-01,  1.2688e-08, -3.1623e-01,\n",
      "           1.0000e+00,  9.4868e-01,  7.0711e-01,  3.1623e-01,  1.2688e-08],\n",
      "         [-3.1623e-01, -6.0000e-01, -8.9443e-01, -1.0000e+00, -9.4868e-01,\n",
      "          -1.4263e-08, -3.1623e-01, -8.9443e-01, -9.4868e-01, -8.0000e-01,\n",
      "           4.4721e-01,  4.4721e-01,  0.0000e+00, -4.4721e-01, -4.4721e-01,\n",
      "           8.0000e-01,  9.4868e-01,  8.9443e-01,  3.1623e-01,  1.4263e-08,\n",
      "           9.4868e-01,  1.0000e+00,  8.9443e-01,  6.0000e-01,  3.1623e-01],\n",
      "         [-7.0711e-01, -8.9443e-01, -1.0000e+00, -8.9443e-01, -7.0711e-01,\n",
      "          -4.4721e-01, -7.0711e-01, -1.0000e+00, -7.0711e-01, -4.4721e-01,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           4.4721e-01,  7.0711e-01,  1.0000e+00,  7.0711e-01,  4.4721e-01,\n",
      "           7.0711e-01,  8.9443e-01,  1.0000e+00,  8.9443e-01,  7.0711e-01],\n",
      "         [-9.4868e-01, -1.0000e+00, -8.9443e-01, -6.0000e-01, -3.1623e-01,\n",
      "          -8.0000e-01, -9.4868e-01, -8.9443e-01, -3.1623e-01, -1.4263e-08,\n",
      "          -4.4721e-01, -4.4721e-01,  0.0000e+00,  4.4721e-01,  4.4721e-01,\n",
      "           1.4263e-08,  3.1623e-01,  8.9443e-01,  9.4868e-01,  8.0000e-01,\n",
      "           3.1623e-01,  6.0000e-01,  8.9443e-01,  1.0000e+00,  9.4868e-01],\n",
      "         [-1.0000e+00, -9.4868e-01, -7.0711e-01, -3.1623e-01, -1.2688e-08,\n",
      "          -9.4868e-01, -1.0000e+00, -7.0711e-01, -1.2688e-08,  3.1623e-01,\n",
      "          -7.0711e-01, -7.0711e-01,  0.0000e+00,  7.0711e-01,  7.0711e-01,\n",
      "          -3.1623e-01,  1.2688e-08,  7.0711e-01,  1.0000e+00,  9.4868e-01,\n",
      "           1.2688e-08,  3.1623e-01,  7.0711e-01,  9.4868e-01,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "coord = nn.Flatten(start_dim=2)(ex_coord[:, -2:, :, :])\n",
    "print(coord.shape)\n",
    "coord_mat = _calculate_similarity_matrix(coord)\n",
    "print(coord_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653c3e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9541, 0.0684, 0.7752, 0.0143, 0.3638],\n",
      "         [0.5275, 0.3893, 0.6406, 0.2623, 0.3145],\n",
      "         [0.3377, 0.2503, 0.4977, 0.4110, 0.5179]]])\n",
      "\n",
      "tensor([[[1.0000, 0.6650, 0.9727, 0.5224, 0.8528],\n",
      "         [0.6650, 1.0000, 0.8133, 0.9026, 0.8375],\n",
      "         [0.9727, 0.8133, 1.0000, 0.7010, 0.9347],\n",
      "         [0.5224, 0.9026, 0.7010, 1.0000, 0.8719],\n",
      "         [0.8528, 0.8375, 0.9347, 0.8719, 1.0000]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex = torch.rand(1, 3, 5)\n",
    "print(ex)\n",
    "print()\n",
    "sim_mat = _calculate_similarity_matrix(ex)\n",
    "print(sim_mat)\n",
    "print() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201eeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_similarity_matrix(matrix):\n",
    "    # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44884b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.3027, 0.3551, 1.2456, 0.2909, 0.6879],\n",
      "         [0.3551, 0.2189, 0.4269, 0.2060, 0.2769],\n",
      "         [1.2456, 0.4269, 1.2590, 0.3837, 0.7412],\n",
      "         [0.2909, 0.2060, 0.3837, 0.2380, 0.3006],\n",
      "         [0.6879, 0.2769, 0.7412, 0.3006, 0.4994]]])\n",
      "\n",
      "tensor([[[0.6569, 0.5150, 0.6216, 0.4482, 0.5734],\n",
      "         [0.1791, 0.3174, 0.2130, 0.3174, 0.2308],\n",
      "         [0.6281, 0.6192, 0.6283, 0.5912, 0.6179],\n",
      "         [0.1467, 0.2987, 0.1915, 0.3666, 0.2506],\n",
      "         [0.3469, 0.4016, 0.3699, 0.4631, 0.4163]]])\n",
      "\n",
      "tensor([[[1.0000, 0.6650, 0.9727, 0.5224, 0.8528],\n",
      "         [0.6650, 1.0000, 0.8133, 0.9026, 0.8375],\n",
      "         [0.9727, 0.8133, 1.0000, 0.7010, 0.9347],\n",
      "         [0.5224, 0.9026, 0.7010, 1.0000, 0.8719],\n",
      "         [0.8528, 0.8375, 0.9347, 0.8719, 1.0000]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sim_mat = ex.transpose(2, 1) @ ex\n",
    "print(sim_mat)\n",
    "print()\n",
    "sim_mat = nn.functional.normalize(sim_mat, p=2, dim=1)\n",
    "print(sim_mat)\n",
    "print()\n",
    "sim_mat = _calculate_similarity_matrix(ex)\n",
    "print(sim_mat)\n",
    "print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3f3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(sim_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5beae31",
   "metadata": {},
   "source": [
    "# SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb045c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_NN(nn.Module): \n",
    "    \"\"\"Convolution 2D Nearest Neighbor Layer\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                K,\n",
    "                stride, \n",
    "                sampling_type, \n",
    "                num_samples, \n",
    "                sample_padding,\n",
    "                shuffle_pattern, \n",
    "                shuffle_scale, \n",
    "                magnitude_type,\n",
    "                coordinate_encoding\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            K (int): Number of Nearest Neighbors for consideration.\n",
    "            stride (int): Stride size.\n",
    "            sampling_type (str): Sampling type: \"all\", \"random\", \"spatial\".\n",
    "            num_samples (int): Number of samples to consider. -1 for all samples.\n",
    "            shuffle_pattern (str): Shuffle pattern: \"B\", \"A\", \"BA\".\n",
    "            shuffle_scale (int): Shuffle scale factor.\n",
    "            magnitude_type (str): Distance or Similarity.\n",
    "        \"\"\"\n",
    "        super(Conv2d_NN, self).__init__()\n",
    "        \n",
    "        # Assertions \n",
    "        assert K == stride, \"Error: K must be same as stride. K == stride.\"\n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Error: shuffle_pattern must be one of ['B', 'A', 'BA', 'NA']\"\n",
    "        assert magnitude_type in [\"distance\", \"similarity\"], \"Error: magnitude_type must be one of ['distance', 'similarity']\"\n",
    "        assert sampling_type in [\"all\", \"random\", \"spatial\"], \"Error: sampling_type must be one of ['all', 'random', 'spatial']\"\n",
    "        assert int(num_samples) > 0 or int(num_samples) == -1, \"Error: num_samples must be greater than 0 or -1 for all samples\"\n",
    "        assert (sampling_type == \"all\" and int(num_samples) == -1) or (sampling_type != \"all\" and isinstance(num_samples, int)), \"Error: num_samples must be -1 for 'all' sampling or an integer for 'random' and 'spatial' sampling\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.stride = stride\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples if num_samples != -1 else 'all'  # -1 for all samples\n",
    "        self.sample_padding = sample_padding if sampling_type == \"spatial\" else 0\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'similarity' else False\n",
    "        self.INF_DISTANCE = 1e10\n",
    "        self.NEG_INF_DISTANCE = -1e10\n",
    "\n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        self.in_channels = in_channels + 2 if self.coordinate_encoding else in_channels\n",
    "        self.out_channels = out_channels # + 2 if self.coordinate_encoding else out_channels\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Adjust Channels for PixelShuffle\n",
    "        self.in_channels_1d = self.in_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_1d = self.out_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        # Conv1d Layer\n",
    "        self.in_channels_1d = 1\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=self.in_channels_1d, \n",
    "                                      out_channels=self.out_channels_1d, \n",
    "                                      kernel_size=self.K, \n",
    "                                      stride=self.stride, \n",
    "                                      padding=0)\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "\n",
    "        # # Pointwise Convolution Layer\n",
    "        # self.pointwise_conv = nn.Conv2d(in_channels=self.out_channels,\n",
    "        #                                  out_channels=self.out_channels - 2,\n",
    "        #                                  kernel_size=1,\n",
    "        #                                  stride=1,\n",
    "        #                                  padding=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        # Coordinate Channels (optional) + Unshuffle + Flatten \n",
    "        x = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "        print(\"x padded: \")\n",
    "        print(x)\n",
    "        print()\n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        print(\"x after coordinate encoding: \")\n",
    "        print(x)\n",
    "        x_2d = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "        x = self.flatten(x_2d)\n",
    "        print(\"x: \")\n",
    "        print(x)\n",
    "        print() \n",
    "        print(x.shape)\n",
    "        \n",
    "        if self.sampling_type == \"all\":    \n",
    "            # ConvNN Algorithm \n",
    "            x_dist = x[:, -2:, :]\n",
    "            print(\"x_dist: \")\n",
    "            print(x_dist)\n",
    "            print(x_dist.shape)\n",
    "\n",
    "            matrix_magnitude = self._calculate_distance_matrix(x_dist, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix(x_dist)\n",
    "            print(\"matrix_magnitude: \")\n",
    "            print(matrix_magnitude)\n",
    "            x = x[:, 0, :].unsqueeze(1)\n",
    "            print(x)\n",
    "            print(x.shape)\n",
    "            prime = self._prime(x, matrix_magnitude, self.K, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == \"random\":\n",
    "            # Select random samples\n",
    "            rand_idx = torch.randperm(x.shape[2], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, :, rand_idx]\n",
    "\n",
    "            # ConvNN Algorithm \n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(rand_idx), device=x.device)\n",
    "            matrix_magnitude[:, rand_idx, range_idx] = self.INF_DISTANCE if self.magnitude_type == 'distance' else self.NEG_INF_DISTANCE\n",
    "            prime = self._prime_N(x, matrix_magnitude, self.K, rand_idx, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type == \"spatial\":\n",
    "            # Get spatial sampled indices\n",
    "            x_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[2] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            y_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[3] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "            x_idx_flat, y_idx_flat = x_grid.flatten(), y_grid.flatten()\n",
    "            width = x_2d.shape[2] \n",
    "            flat_indices = y_idx_flat * width + x_idx_flat  \n",
    "            x_sample = x[:, :, flat_indices]\n",
    "\n",
    "            # ConvNN Algorithm\n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(flat_indices), device=x.device)\n",
    "            matrix_magnitude[:, flat_indices, range_idx] = self.INF_DISTANCE if self.magnitude_type == 'distance' else self.NEG_INF_DISTANCE\n",
    "            prime = self._prime_N(x, matrix_magnitude, self.K, flat_indices, self.maximum)\n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial'].\")\n",
    "        print(\"\\n prime: \")\n",
    "        print(prime.shape)\n",
    "        print(prime)\n",
    "        # Post-Processing \n",
    "        x_conv = self.conv1d_layer(prime) \n",
    "        \n",
    "        # Unflatten + Shuffle\n",
    "        unflatten = nn.Unflatten(dim=2, unflattened_size=x_2d.shape[2:])\n",
    "        x = unflatten(x_conv)  # [batch_size, out_channels\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x\n",
    "        # x = self.pointwise_conv(x) if self.coordinate_encoding else x\n",
    "        return x\n",
    "\n",
    "    def _calculate_distance_matrix(self, matrix, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared.transpose(2, 1) - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=-1.0, max=1.0) \n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix \n",
    "        \n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_distance_matrix_N(self, matrix, matrix_sample, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True).permute(0, 2, 1)\n",
    "        norm_squared_sample = torch.sum(matrix_sample ** 2, dim=1, keepdim=True).transpose(2, 1).permute(0, 2, 1)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix_sample)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared_sample - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=-1.0, max=1.0) \n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "\n",
    "        return dist_matrix\n",
    "    \n",
    "    # def _calculate_similarity_matrix(self, matrix):\n",
    "    #     # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "    #     norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "    #     similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "    #     similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "    #     return similarity_matrix\n",
    "\n",
    "    def _calculate_similarity_matrix(self, matrix, sigma=0.1):\n",
    "        \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "        b, c, t = matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "        \n",
    "        # Calculate pairwise Euclidean distances between coordinates\n",
    "        coord_expanded_1 = matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "        coord_expanded_2 = matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "        \n",
    "        # Euclidean distance between coordinates\n",
    "        coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "        coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1) + 1e-8)  # [B, T, T]\n",
    "        \n",
    "        # Convert distance to similarity using Gaussian kernel\n",
    "        similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix_N(self, matrix, matrix_sample):\n",
    "        # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_sample)\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        _, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "        \n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "        print(\"prime: \")\n",
    "        print(prime)\n",
    "        print()\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "        \n",
    "    def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        \n",
    "        _, topk_indices = torch.topk(magnitude_matrix, k=K - 1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indices back to original matrix positions\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords ### Last two channels are coordinate channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09e41aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x padded: \n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 1., 2., 3., 0.],\n",
      "          [0., 4., 5., 6., 0.],\n",
      "          [0., 7., 8., 9., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "\n",
      "x after coordinate encoding: \n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  2.0000,  3.0000,  0.0000],\n",
      "          [ 0.0000,  4.0000,  5.0000,  6.0000,  0.0000],\n",
      "          [ 0.0000,  7.0000,  8.0000,  9.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "          [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]]])\n",
      "x: \n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
      "           2.0000,  3.0000,  0.0000,  0.0000,  4.0000,  5.0000,  6.0000,\n",
      "           0.0000,  0.0000,  7.0000,  8.0000,  9.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000, -0.5000,\n",
      "           0.0000,  0.5000,  1.0000, -1.0000, -0.5000,  0.0000,  0.5000,\n",
      "           1.0000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000,\n",
      "          -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000]]])\n",
      "\n",
      "torch.Size([1, 3, 25])\n",
      "x_dist: \n",
      "tensor([[[-1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000, -0.5000,\n",
      "           0.0000,  0.5000,  1.0000, -1.0000, -0.5000,  0.0000,  0.5000,\n",
      "           1.0000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000, -1.0000,\n",
      "          -0.5000,  0.0000,  0.5000,  1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5000, -0.5000,\n",
      "          -0.5000, -0.5000, -0.5000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000]]])\n",
      "torch.Size([1, 2, 25])\n",
      "matrix_magnitude: \n",
      "tensor([[[1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00],\n",
      "         [7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00],\n",
      "         [3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44],\n",
      "         [0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00, 0.0000e+00,\n",
      "          3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00, 0.0000e+00,\n",
      "          1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44, 0.0000e+00,\n",
      "          1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28, 0.0000e+00,\n",
      "          3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28, 3.7835e-44,\n",
      "          7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11, 7.1878e-28,\n",
      "          1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06, 1.9287e-22],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22, 7.1878e-28,\n",
      "          0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06, 1.3888e-11,\n",
      "          0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00, 3.7267e-06],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 3.7835e-44, 7.1878e-28, 1.9287e-22,\n",
      "          0.0000e+00, 0.0000e+00, 7.1878e-28, 1.3888e-11, 3.7267e-06,\n",
      "          0.0000e+00, 0.0000e+00, 1.9287e-22, 3.7267e-06, 1.0000e+00]]])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1., 2., 3., 0., 0., 4., 5., 6., 0., 0., 7.,\n",
      "          8., 9., 0., 0., 0., 0., 0., 0.]]])\n",
      "torch.Size([1, 1, 25])\n",
      "prime: \n",
      "tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
      "          [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
      "          [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
      "          [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
      "          [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
      "          [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
      "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "          [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
      "          [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
      "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "          [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
      "          [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
      "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "          [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
      "          [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
      "          [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
      "          [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
      "          [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
      "          [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
      "          [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
      "\n",
      "\n",
      " prime: \n",
      "torch.Size([1, 1, 225])\n",
      "tensor([[[0., 0., 0., 1., 0., 0., 4., 2., 5., 0., 0., 0., 1., 0., 2., 0., 4.,\n",
      "          3., 0., 0., 0., 2., 3., 1., 5., 0., 0., 0., 0., 0., 3., 0., 2., 6.,\n",
      "          0., 1., 0., 0., 0., 3., 0., 0., 6., 2., 5., 0., 0., 0., 1., 0., 4.,\n",
      "          0., 2., 5., 1., 4., 0., 2., 0., 5., 0., 0., 0., 2., 1., 0., 3., 5.,\n",
      "          0., 0., 6., 4., 3., 6., 0., 0., 2., 5., 0., 0., 0., 0., 0., 0., 3.,\n",
      "          6., 0., 0., 2., 5., 0., 0., 4., 0., 7., 1., 5., 0., 0., 4., 5., 1.,\n",
      "          0., 7., 0., 2., 0., 8., 5., 6., 8., 4., 2., 7., 1., 9., 3., 6., 0.,\n",
      "          5., 3., 9., 8., 2., 0., 0., 0., 0., 6., 0., 9., 3., 0., 5., 0., 0.,\n",
      "          0., 0., 7., 4., 0., 8., 0., 5., 7., 0., 8., 0., 4., 5., 0., 0., 0.,\n",
      "          8., 5., 0., 7., 9., 4., 0., 0., 6., 9., 0., 0., 8., 6., 0., 0., 0.,\n",
      "          5., 0., 0., 9., 0., 0., 6., 8., 0., 3., 0., 0., 0., 7., 0., 0., 8.,\n",
      "          4., 5., 0., 0., 0., 7., 8., 0., 4., 0., 9., 0., 0., 0., 8., 9., 7.,\n",
      "          0., 5., 0., 0., 0., 0., 9., 0., 8., 0., 6., 5., 0., 0., 0., 9., 0.,\n",
      "          0., 8., 6., 5.]]])\n",
      "torch.Size([1, 6, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe results may be different because of the different positions of the kernel window from ConvNN. \\nExample: \\nConvolution = \\ntensor([[0., 0., 0.],\\n        [0., 1., 2.],\\n        [0., 4., 5.]])\\nFlattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\\n\\nConvNN = \\ntensor([[0., 0., 0.],\\n        [0., 1., 2.],\\n        [0., 4., 5.]])\\nTopk:      [1.0, 4.0, 0.0, 2.0, 0.0, 5.0, 0.0, 0.0, 0.0],\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = torch.Tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "convnn = Conv2d_NN(in_channels=1, out_channels=6, K=9, stride=9, sampling_type='all', num_samples=-1, sample_padding=0, shuffle_pattern='NA', shuffle_scale=0.0, magnitude_type='similarity', coordinate_encoding=True)\n",
    "out = convnn(ex)\n",
    "print(out.shape)\n",
    "\n",
    "\"\"\"\n",
    "tensor([[[0., 0., 0., 0., 0., \n",
    "          0., 1., 2., 3., 0., \n",
    "          0., 4., 5., 6., 0., \n",
    "          0., 7., 8., 9., 0., \n",
    "          0., 0., 0., 0., 0.]]])\n",
    "\n",
    "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],  \n",
    "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "          [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "          \n",
    "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "          [9., 0., 0., 8., 6., 0., 0., 0., 5.]]]])\n",
    "          \n",
    "Window 0 (position [0, 0]):\n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 1., 2.],\n",
    "        [0., 4., 5.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\n",
    "\n",
    "Window 1 (position [0, 1]):\n",
    "tensor([[0., 0., 0.],\n",
    "        [1., 2., 3.],\n",
    "        [4., 5., 6.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
    "\n",
    "Window 2 (position [0, 2]):\n",
    "tensor([[0., 0., 0.],\n",
    "        [2., 3., 0.],\n",
    "        [5., 6., 0.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 2.0, 3.0, 0.0, 5.0, 6.0, 0.0]\n",
    "\n",
    "Window 3 (position [1, 0]):\n",
    "tensor([[0., 1., 2.],\n",
    "        [0., 4., 5.],\n",
    "        [0., 7., 8.]])\n",
    "Flattened: [0.0, 1.0, 2.0, 0.0, 4.0, 5.0, 0.0, 7.0, 8.0]\n",
    "\n",
    "Window 4 (position [1, 1]):\n",
    "tensor([[1., 2., 3.],\n",
    "        [4., 5., 6.],\n",
    "        [7., 8., 9.]])\n",
    "Flattened: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "\n",
    "Window 5 (position [1, 2]):\n",
    "tensor([[2., 3., 0.],\n",
    "        [5., 6., 0.],\n",
    "        [8., 9., 0.]])\n",
    "Flattened: [2.0, 3.0, 0.0, 5.0, 6.0, 0.0, 8.0, 9.0, 0.0]\n",
    "\n",
    "Window 6 (position [2, 0]):\n",
    "tensor([[0., 4., 5.],\n",
    "        [0., 7., 8.],\n",
    "        [0., 0., 0.]])\n",
    "Flattened: [0.0, 4.0, 5.0, 0.0, 7.0, 8.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "Window 7 (position [2, 1]):\n",
    "tensor([[4., 5., 6.],\n",
    "        [7., 8., 9.],\n",
    "        [0., 0., 0.]])\n",
    "Flattened: [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "Window 8 (position [2, 2]):\n",
    "tensor([[5., 6., 0.],\n",
    "        [8., 9., 0.],\n",
    "        [0., 0., 0.]])\n",
    "Flattened: [5.0, 6.0, 0.0, 8.0, 9.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "          \n",
    "torch.Size([1, 1, 25])\n",
    "prime: \n",
    "tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
    "          [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
    "          [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
    "          [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
    "          [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
    "          [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
    "          [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "          [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
    "          [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
    "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "          [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
    "          [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
    "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "          [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "          [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
    "          [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
    "          [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
    "          [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
    "          [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
    "          [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
    "\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The results may be different because of the different positions of the kernel window from ConvNN. \n",
    "Example: \n",
    "Convolution = \n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 1., 2.],\n",
    "        [0., 4., 5.]])\n",
    "Flattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\n",
    "\n",
    "ConvNN = \n",
    "tensor([[0., 0., 0.],\n",
    "        [0., 1., 2.],\n",
    "        [0., 4., 5.]])\n",
    "Topk:      [1.0, 4.0, 0.0, 2.0, 0.0, 5.0, 0.0, 0.0, 0.0],\n",
    "\"\"\"\n",
    "\n",
    "# SOLVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66289130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-channel tensor shape: torch.Size([1, 3, 25, 9])\n",
      "Original shape: torch.Size([1, 3, 25, 9])\n",
      "Filtered shape: torch.Size([1, 3, 9, 9])\n",
      "Kept row indices: tensor([ 6,  7,  8, 11, 12, 13, 16, 17, 18])\n",
      "\n",
      "Filtered first channel:\n",
      "tensor([[1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "        [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "        [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "        [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "        [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "        [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "        [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "        [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "        [9., 0., 0., 8., 6., 0., 0., 0., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def filter_non_zero_starting_rows_multichannel(tensor):\n",
    "    \"\"\"\n",
    "    Filter rows based on the first element of the first channel being non-zero\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor of shape [B, C, num_rows, row_length]\n",
    "    \n",
    "    Returns:\n",
    "        Filtered tensor with only rows where first channel's first element != 0\n",
    "    \"\"\"\n",
    "    # Get the shape\n",
    "    b, c, num_rows, row_length = tensor.shape\n",
    "    \n",
    "    # Create mask based on first channel only\n",
    "    # tensor[:, 0, :, 0] gets first element of each row in first channel\n",
    "    mask = tensor[:, 0, :, 0] != 0  # Shape: [b, num_rows]\n",
    "    \n",
    "    # Get indices of non-zero starting rows\n",
    "    non_zero_indices = torch.where(mask[0])[0]  # [0] because batch dimension\n",
    "    \n",
    "    # Select rows from ALL channels\n",
    "    filtered_tensor = tensor[:, :, non_zero_indices, :]\n",
    "    \n",
    "    return filtered_tensor, non_zero_indices\n",
    "\n",
    "# Test with your example (assuming you have multiple channels)\n",
    "# For demonstration, let's create a multi-channel version\n",
    "prime_single_channel = torch.tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
    "                                      [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
    "                                      [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
    "                                      [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
    "                                      [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
    "                                      [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
    "                                      [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "                                      [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "                                      [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "                                      [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
    "                                      [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
    "                                      [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "                                      [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "                                      [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "                                      [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
    "                                      [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
    "                                      [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "                                      [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "                                      [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "                                      [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
    "                                      [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
    "                                      [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
    "                                      [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
    "                                      [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
    "                                      [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
    "\n",
    "# Create a multi-channel version for demonstration\n",
    "# Let's say we have 3 channels\n",
    "prime_multi_channel = torch.cat([\n",
    "    prime_single_channel,  # Channel 0 (your original data)\n",
    "    torch.rand_like(prime_single_channel),  # Channel 1 (random data)\n",
    "    torch.rand_like(prime_single_channel)   # Channel 2 (random data)\n",
    "], dim=1)\n",
    "\n",
    "print(f\"Multi-channel tensor shape: {prime_multi_channel.shape}\")\n",
    "\n",
    "# Filter based on first channel only\n",
    "filtered_tensor, kept_indices = filter_non_zero_starting_rows_multichannel(prime_multi_channel)\n",
    "\n",
    "print(f\"Original shape: {prime_multi_channel.shape}\")\n",
    "print(f\"Filtered shape: {filtered_tensor.shape}\")\n",
    "print(f\"Kept row indices: {kept_indices}\")\n",
    "\n",
    "# Show the filtered first channel\n",
    "print(\"\\nFiltered first channel:\")\n",
    "print(filtered_tensor[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379f3a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 1, 25, 9])\n",
      "Filtered shape: torch.Size([1, 1, 9, 9])\n",
      "\n",
      "Filtered tensor:\n",
      "tensor([[[[1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "          [9., 0., 0., 8., 6., 0., 0., 0., 5.]]]])\n",
      "\n",
      "Method 2 - Filtered shape: torch.Size([1, 1, 9, 9])\n",
      "Filtered tensor (Method 2):\n",
      "tensor([[[[1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
      "          [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
      "          [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
      "          [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
      "          [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
      "          [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
      "          [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
      "          [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
      "          [9., 0., 0., 8., 6., 0., 0., 0., 5.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your original tensor\n",
    "prime = torch.tensor([[[[0., 0., 0., 1., 0., 0., 4., 2., 5.],\n",
    "                        [0., 0., 0., 1., 0., 2., 0., 4., 3.],\n",
    "                        [0., 0., 0., 2., 3., 1., 5., 0., 0.],\n",
    "                        [0., 0., 0., 3., 0., 2., 6., 0., 1.],\n",
    "                        [0., 0., 0., 3., 0., 0., 6., 2., 5.],\n",
    "                        [0., 0., 0., 1., 0., 4., 0., 2., 5.],\n",
    "                        [1., 4., 0., 2., 0., 5., 0., 0., 0.],\n",
    "                        [2., 1., 0., 3., 5., 0., 0., 6., 4.],\n",
    "                        [3., 6., 0., 0., 2., 5., 0., 0., 0.],\n",
    "                        [0., 0., 0., 3., 6., 0., 0., 2., 5.],\n",
    "                        [0., 0., 4., 0., 7., 1., 5., 0., 0.],\n",
    "                        [4., 5., 1., 0., 7., 0., 2., 0., 8.],\n",
    "                        [5., 6., 8., 4., 2., 7., 1., 9., 3.],\n",
    "                        [6., 0., 5., 3., 9., 8., 2., 0., 0.],\n",
    "                        [0., 0., 6., 0., 9., 3., 0., 5., 0.],\n",
    "                        [0., 0., 0., 7., 4., 0., 8., 0., 5.],\n",
    "                        [7., 0., 8., 0., 4., 5., 0., 0., 0.],\n",
    "                        [8., 5., 0., 7., 9., 4., 0., 0., 6.],\n",
    "                        [9., 0., 0., 8., 6., 0., 0., 0., 5.],\n",
    "                        [0., 0., 9., 0., 0., 6., 8., 0., 3.],\n",
    "                        [0., 0., 0., 7., 0., 0., 8., 4., 5.],\n",
    "                        [0., 0., 0., 7., 8., 0., 4., 0., 9.],\n",
    "                        [0., 0., 0., 8., 9., 7., 0., 5., 0.],\n",
    "                        [0., 0., 0., 9., 0., 8., 0., 6., 5.],\n",
    "                        [0., 0., 0., 9., 0., 0., 8., 6., 5.]]]])\n",
    "\n",
    "# Method 1: Create a mask for rows that don't start with 0\n",
    "def filter_non_zero_starting_rows(tensor):\n",
    "    # Get the shape\n",
    "    b, c, num_rows, row_length = tensor.shape\n",
    "    \n",
    "    # Create mask for rows where first element is not 0\n",
    "    mask = tensor[:, :, :, 0] != 0  # Shape: [b, c, num_rows]\n",
    "    \n",
    "    # Get indices of non-zero starting rows\n",
    "    non_zero_indices = torch.where(mask[0, 0])[0]\n",
    "    \n",
    "    # Select only the rows that don't start with 0\n",
    "    filtered_tensor = tensor[:, :, non_zero_indices, :]\n",
    "    \n",
    "    return filtered_tensor\n",
    "\n",
    "# Apply the filter\n",
    "filtered_prime = filter_non_zero_starting_rows(prime)\n",
    "\n",
    "print(f\"Original shape: {prime.shape}\")\n",
    "print(f\"Filtered shape: {filtered_prime.shape}\")\n",
    "print(\"\\nFiltered tensor:\")\n",
    "print(filtered_prime)\n",
    "\n",
    "# Method 2: More concise using boolean indexing\n",
    "def filter_non_zero_starting_rows_concise(tensor):\n",
    "    # Reshape to 2D for easier indexing\n",
    "    reshaped = tensor.squeeze()  # Remove batch and channel dimensions\n",
    "    \n",
    "    # Create mask for rows that don't start with 0\n",
    "    mask = reshaped[:, 0] != 0\n",
    "    \n",
    "    # Filter rows\n",
    "    filtered_rows = reshaped[mask]\n",
    "    \n",
    "    # Restore original dimensions\n",
    "    return filtered_rows.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Alternative method\n",
    "filtered_prime_v2 = filter_non_zero_starting_rows_concise(prime)\n",
    "print(f\"\\nMethod 2 - Filtered shape: {filtered_prime_v2.shape}\")\n",
    "print(\"Filtered tensor (Method 2):\")\n",
    "print(filtered_prime_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce720f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bd542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c56fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baad55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a0df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 1., 1., 1., 0.],\n",
      "          [0., 1., 1., 1., 0.],\n",
      "          [0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(1, 1, 3, 3) \n",
    "print(ones)\n",
    "ones = F.pad(ones, (1, 1, 1, 1), mode='constant', value=0)\n",
    "print(ones)\n",
    "print(ones.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a6c9f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 3x3 windows: 9 (3x3)\n",
      "Each window has 9 elements (3x3 = 9)\n",
      "\n",
      "Window 0 (position [0, 0]):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 1., 2.],\n",
      "        [0., 4., 5.]])\n",
      "Flattened: [0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 4.0, 5.0]\n",
      "\n",
      "Window 1 (position [0, 1]):\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Flattened: [0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "\n",
      "Window 2 (position [0, 2]):\n",
      "tensor([[0., 0., 0.],\n",
      "        [2., 3., 0.],\n",
      "        [5., 6., 0.]])\n",
      "Flattened: [0.0, 0.0, 0.0, 2.0, 3.0, 0.0, 5.0, 6.0, 0.0]\n",
      "\n",
      "Window 3 (position [1, 0]):\n",
      "tensor([[0., 1., 2.],\n",
      "        [0., 4., 5.],\n",
      "        [0., 7., 8.]])\n",
      "Flattened: [0.0, 1.0, 2.0, 0.0, 4.0, 5.0, 0.0, 7.0, 8.0]\n",
      "\n",
      "Window 4 (position [1, 1]):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "Flattened: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "\n",
      "Window 5 (position [1, 2]):\n",
      "tensor([[2., 3., 0.],\n",
      "        [5., 6., 0.],\n",
      "        [8., 9., 0.]])\n",
      "Flattened: [2.0, 3.0, 0.0, 5.0, 6.0, 0.0, 8.0, 9.0, 0.0]\n",
      "\n",
      "Window 6 (position [2, 0]):\n",
      "tensor([[0., 4., 5.],\n",
      "        [0., 7., 8.],\n",
      "        [0., 0., 0.]])\n",
      "Flattened: [0.0, 4.0, 5.0, 0.0, 7.0, 8.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Window 7 (position [2, 1]):\n",
      "tensor([[4., 5., 6.],\n",
      "        [7., 8., 9.],\n",
      "        [0., 0., 0.]])\n",
      "Flattened: [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Window 8 (position [2, 2]):\n",
      "tensor([[5., 6., 0.],\n",
      "        [8., 9., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Flattened: [5.0, 6.0, 0.0, 8.0, 9.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_3x3_windows_from_flattened(x_flat, original_shape=(5, 5)):\n",
    "    \"\"\"\n",
    "    Extract all 3x3 convolutional windows from a flattened tensor\n",
    "    \n",
    "    Args:\n",
    "        x_flat: Flattened tensor [1, 1, H*W]\n",
    "        original_shape: Original 2D shape (H, W)\n",
    "    \n",
    "    Returns:\n",
    "        windows: All 3x3 windows in 1D format [num_windows, 9]\n",
    "    \"\"\"\n",
    "    h, w = original_shape\n",
    "    \n",
    "    # Reshape back to 2D\n",
    "    x_2d = x_flat.reshape(1, 1, h, w)\n",
    "    \n",
    "    # Use unfold to get 3x3 patches\n",
    "    # unfold(dim, size, step) extracts sliding windows\n",
    "    patches = x_2d.unfold(2, 3, 1).unfold(3, 3, 1)  # [1, 1, H-2, W-2, 3, 3]\n",
    "    \n",
    "    # Reshape to get each window as a 1D vector\n",
    "    num_windows_h, num_windows_w = patches.shape[2], patches.shape[3]\n",
    "    windows = patches.reshape(num_windows_h * num_windows_w, 9)\n",
    "    \n",
    "    return windows, num_windows_h, num_windows_w\n",
    "\n",
    "# Your input tensor\n",
    "x_flat = torch.tensor([[[0., 0., 0., 0., 0., \n",
    "                        0., 1., 2., 3., 0., \n",
    "                        0., 4., 5., 6., 0., \n",
    "                        0., 7., 8., 9., 0., \n",
    "                        0., 0., 0., 0., 0.]]])\n",
    "\n",
    "# Extract all 3x3 windows\n",
    "windows, num_h, num_w = get_3x3_windows_from_flattened(x_flat)\n",
    "\n",
    "print(f\"Number of 3x3 windows: {windows.shape[0]} ({num_h}x{num_w})\")\n",
    "print(f\"Each window has {windows.shape[1]} elements (3x3 = 9)\")\n",
    "print()\n",
    "\n",
    "# Display all windows\n",
    "for i, window in enumerate(windows):\n",
    "    row = i // num_w\n",
    "    col = i % num_w\n",
    "    print(f\"Window {i} (position [{row}, {col}]):\")\n",
    "    print(window.reshape(3, 3))\n",
    "    print(\"Flattened:\", window.tolist())\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9350537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef682c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b86a180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coordinate-based similarity approach ===\n",
      "Input tensor:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 3., 0.],\n",
      "        [0., 4., 5., 6., 0.],\n",
      "        [0., 7., 8., 9., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Original shape: torch.Size([1, 1, 5, 5])\n",
      "Padded shape: torch.Size([1, 1, 7, 7])\n",
      "Flattened coordinate shape: torch.Size([1, 2, 49])\n",
      "Similarity matrix shape: torch.Size([1, 49, 49])\n",
      "Neighbors for center position (value 5):\n",
      "tensor([0., 0., 0., 0., 0., 3., 0., 0., 0.])\n",
      "\n",
      "Neighbors for position with value 1:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 3.])\n",
      "\n",
      "=== Direct 3x3 patch approach ===\n",
      "3x3 patch around value 5:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [3., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _add_coordinate_encoding(x):\n",
    "    \"\"\"Add coordinate channels to input tensor\"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords = torch.linspace(-1, 1, h, device=x.device)\n",
    "    x_coords = torch.linspace(-1, 1, w, device=x.device)\n",
    "    \n",
    "    y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack coordinates and add batch dimension\n",
    "    coord_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0)  # [1, 2, H, W]\n",
    "    coord_grid = coord_grid.expand(b, -1, -1, -1)  # [B, 2, H, W]\n",
    "    \n",
    "    return coord_grid\n",
    "\n",
    "def _calculate_coordinate_similarity_matrix(coord_matrix, sigma=0.5):\n",
    "    \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "    b, c, t = coord_matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "    \n",
    "    # Calculate pairwise Euclidean distances between coordinates\n",
    "    coord_expanded_1 = coord_matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "    coord_expanded_2 = coord_matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "    \n",
    "    # Euclidean distance between coordinates\n",
    "    coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "    coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1))  # [B, T, T]\n",
    "    \n",
    "    # Convert distance to similarity using Gaussian kernel\n",
    "    similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def get_spatial_neighbors(x, K=9, sigma=0.5):\n",
    "    \"\"\"\n",
    "    Get K nearest spatial neighbors using coordinate-based similarity\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor [B, C, H, W]\n",
    "        K: Number of neighbors to select\n",
    "        sigma: Standard deviation for Gaussian similarity kernel\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Add padding\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    \n",
    "    # Get coordinate encoding for the padded tensor\n",
    "    coord_grid = _add_coordinate_encoding(x_padded)  # [B, 2, H+2, W+2]\n",
    "    \n",
    "    # Flatten spatial dimensions\n",
    "    x_flat = x_padded.flatten(2)  # [B, C, (H+2)*(W+2)]\n",
    "    coord_flat = coord_grid.flatten(2)  # [B, 2, (H+2)*(W+2)]\n",
    "    \n",
    "    print(f\"Original shape: {x.shape}\")\n",
    "    print(f\"Padded shape: {x_padded.shape}\")\n",
    "    print(f\"Flattened coordinate shape: {coord_flat.shape}\")\n",
    "    \n",
    "    # Calculate similarity matrix based on coordinates\n",
    "    similarity_matrix = _calculate_coordinate_similarity_matrix(coord_flat, sigma=sigma)\n",
    "    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "    \n",
    "    # Get top K neighbors for each position\n",
    "    topk_values, topk_indices = torch.topk(similarity_matrix, k=K, dim=2, largest=True)\n",
    "    \n",
    "    # Expand indices to match all channels\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, -1, K)  # [B, C, T, K]\n",
    "    \n",
    "    # Gather the neighboring values\n",
    "    x_expanded = x_flat.unsqueeze(-1).expand(b, c, -1, K)\n",
    "    neighbors = torch.gather(x_expanded, dim=2, index=topk_indices_exp)\n",
    "    \n",
    "    return neighbors, topk_indices, similarity_matrix\n",
    "\n",
    "# Test with your example\n",
    "def test_spatial_neighbors():\n",
    "    # Create test tensor - your example reshaped to 2D\n",
    "    x = torch.tensor([[[\n",
    "        [0., 0., 0., 0., 0.],\n",
    "        [0., 1., 2., 3., 0.],\n",
    "        [0., 4., 5., 6., 0.],\n",
    "        [0., 7., 8., 9., 0.],\n",
    "        [0., 0., 0., 0., 0.]\n",
    "    ]]])\n",
    "    \n",
    "    print(\"Input tensor:\")\n",
    "    print(x.squeeze())\n",
    "    print()\n",
    "    \n",
    "    # Get 9 nearest neighbors\n",
    "    neighbors, indices, sim_matrix = get_spatial_neighbors(x, K=9, sigma=0.3)\n",
    "    \n",
    "    # Look at neighbors for the position containing value '5' (center of 3x3 grid)\n",
    "    # In the padded 5x5 grid, value '5' should be at position (2,2)\n",
    "    # In flattened coordinates: 2*5 + 2 = 12\n",
    "    center_pos = 12  # This should correspond to value '5'\n",
    "    \n",
    "    print(f\"Neighbors for center position (value 5):\")\n",
    "    center_neighbors = neighbors[0, 0, center_pos, :]  # [K]\n",
    "    print(center_neighbors)\n",
    "    \n",
    "    # Let's also check a few other positions\n",
    "    print(f\"\\nNeighbors for position with value 1:\")\n",
    "    pos_1 = 6  # Position of value '1'\n",
    "    neighbors_1 = neighbors[0, 0, pos_1, :]\n",
    "    print(neighbors_1)\n",
    "    \n",
    "    return neighbors, indices, sim_matrix\n",
    "\n",
    "# Alternative approach: Direct coordinate-based neighbor selection\n",
    "def get_3x3_neighbors_direct(x):\n",
    "    \"\"\"\n",
    "    Directly get 3x3 neighbors without similarity matrix calculation\n",
    "    This mimics standard convolution receptive field\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Pad the input\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    \n",
    "    # Use unfold to get 3x3 patches\n",
    "    patches = x_padded.unfold(2, 3, 1).unfold(3, 3, 1)  # [B, C, H, W, 3, 3]\n",
    "    patches = patches.reshape(b, c, h*w, 9)  # [B, C, H*W, 9]\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Test both approaches\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Coordinate-based similarity approach ===\")\n",
    "    neighbors, indices, sim_matrix = test_spatial_neighbors()\n",
    "    \n",
    "    print(\"\\n=== Direct 3x3 patch approach ===\")\n",
    "    x = torch.tensor([[[\n",
    "        [0., 0., 0., 0., 0.],\n",
    "        [0., 1., 2., 3., 0.],\n",
    "        [0., 4., 5., 6., 0.],\n",
    "        [0., 7., 8., 9., 0.],\n",
    "        [0., 0., 0., 0., 0.]\n",
    "    ]]])\n",
    "    \n",
    "    patches = get_3x3_neighbors_direct(x)\n",
    "    \n",
    "    # Get patch for center position (1, 1) in original coordinates\n",
    "    # This corresponds to value '5'\n",
    "    center_patch = patches[0, 0, 1*3 + 1, :]  # 3x3 grid, position (1,1)\n",
    "    print(\"3x3 patch around value 5:\")\n",
    "    print(center_patch.reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56ea00d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Coordinate-based similarity approach ===\n",
      "Input tensor:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 1., 2., 3., 0.],\n",
      "        [0., 4., 5., 6., 0.],\n",
      "        [0., 7., 8., 9., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Original shape: torch.Size([1, 1, 5, 5])\n",
      "Padded shape: torch.Size([1, 1, 7, 7])\n",
      "Flattened coordinate shape: torch.Size([1, 2, 49])\n",
      "Similarity matrix shape: torch.Size([1, 49, 49])\n",
      "Padded and flattened tensor:\n",
      "Shape: torch.Size([1, 1, 49])\n",
      "Values: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2.,\n",
      "        3., 0., 0., 0., 0., 4., 5., 6., 0., 0., 0., 0., 7., 8., 9., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Value 5.0 found at flattened position 24 (2D: row=3, col=3)\n",
      "Value 1.0 found at flattened position 16 (2D: row=2, col=2)\n",
      "\n",
      "Neighbors for center position (value 5) at position 24:\n",
      "Values: tensor([5., 2., 4., 6., 8., 1., 3., 7., 9.])\n",
      "Indices: tensor([24, 17, 23, 25, 31, 16, 18, 30, 32])\n",
      "Neighbor positions in 2D grid:\n",
      "  Index 24: (row=3, col=3), value=5.0\n",
      "  Index 17: (row=2, col=3), value=2.0\n",
      "  Index 23: (row=3, col=2), value=4.0\n",
      "  Index 25: (row=3, col=4), value=6.0\n",
      "  Index 31: (row=4, col=3), value=8.0\n",
      "  Index 16: (row=2, col=2), value=1.0\n",
      "  Index 18: (row=2, col=4), value=3.0\n",
      "  Index 30: (row=4, col=2), value=7.0\n",
      "  Index 32: (row=4, col=4), value=9.0\n",
      "\n",
      "Neighbors for position with value 1 at position 16:\n",
      "Values: tensor([1., 4., 2., 0., 0., 5., 0., 0., 0.])\n",
      "Indices: tensor([16, 23, 17, 15,  9, 24, 22,  8, 10])\n",
      "Neighbor positions in 2D grid:\n",
      "  Index 16: (row=2, col=2), value=1.0\n",
      "  Index 23: (row=3, col=2), value=4.0\n",
      "  Index 17: (row=2, col=3), value=2.0\n",
      "  Index 15: (row=2, col=1), value=0.0\n",
      "  Index 9: (row=1, col=2), value=0.0\n",
      "  Index 24: (row=3, col=3), value=5.0\n",
      "  Index 22: (row=3, col=1), value=0.0\n",
      "  Index 8: (row=1, col=1), value=0.0\n",
      "  Index 10: (row=1, col=3), value=0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _add_coordinate_encoding(x):\n",
    "    \"\"\"Add coordinate channels to input tensor\"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_coords = torch.linspace(-1, 1, h, device=x.device)\n",
    "    x_coords = torch.linspace(-1, 1, w, device=x.device)\n",
    "    \n",
    "    y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack coordinates and add batch dimension\n",
    "    coord_grid = torch.stack([x_grid, y_grid], dim=0).unsqueeze(0)  # [1, 2, H, W]\n",
    "    coord_grid = coord_grid.expand(b, -1, -1, -1)  # [B, 2, H, W]\n",
    "    \n",
    "    return coord_grid\n",
    "\n",
    "def _calculate_coordinate_similarity_matrix(coord_matrix, sigma=0.3):\n",
    "    \"\"\"Calculate similarity matrix based on coordinate distance\"\"\"\n",
    "    b, c, t = coord_matrix.shape  # c should be 2 for (x, y) coordinates\n",
    "    \n",
    "    # Calculate pairwise Euclidean distances between coordinates\n",
    "    coord_expanded_1 = coord_matrix.unsqueeze(3)  # [B, 2, T, 1]\n",
    "    coord_expanded_2 = coord_matrix.unsqueeze(2)  # [B, 2, 1, T]\n",
    "    \n",
    "    # Euclidean distance between coordinates\n",
    "    coord_diff = coord_expanded_1 - coord_expanded_2  # [B, 2, T, T]\n",
    "    coord_dist = torch.sqrt(torch.sum(coord_diff ** 2, dim=1) + 1e-8)  # [B, T, T]\n",
    "    \n",
    "    # Convert distance to similarity using Gaussian kernel\n",
    "    similarity_matrix = torch.exp(-coord_dist ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def get_spatial_neighbors(x, K=9, sigma=0.3):\n",
    "    \"\"\"\n",
    "    Get K nearest spatial neighbors using coordinate-based similarity\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Add padding\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    padded_h, padded_w = x_padded.shape[2], x_padded.shape[3]\n",
    "    \n",
    "    # Get coordinate encoding for the padded tensor\n",
    "    coord_grid = _add_coordinate_encoding(x_padded)  # [B, 2, H+2, W+2]\n",
    "    \n",
    "    # Flatten spatial dimensions\n",
    "    x_flat = x_padded.flatten(2)  # [B, C, (H+2)*(W+2)]\n",
    "    coord_flat = coord_grid.flatten(2)  # [B, 2, (H+2)*(W+2)]\n",
    "    \n",
    "    print(f\"Original shape: {x.shape}\")\n",
    "    print(f\"Padded shape: {x_padded.shape}\")\n",
    "    print(f\"Flattened coordinate shape: {coord_flat.shape}\")\n",
    "    \n",
    "    # Calculate similarity matrix based on coordinates\n",
    "    similarity_matrix = _calculate_coordinate_similarity_matrix(coord_flat, sigma=sigma)\n",
    "    print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "    \n",
    "    # Get top K neighbors for each position\n",
    "    topk_values, topk_indices = torch.topk(similarity_matrix, k=K, dim=2, largest=True)\n",
    "    \n",
    "    # Expand indices to match all channels\n",
    "    topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, -1, K)  # [B, C, T, K]\n",
    "    \n",
    "    # Gather the neighboring values\n",
    "    x_expanded = x_flat.unsqueeze(-1).expand(b, c, -1, K)\n",
    "    neighbors = torch.gather(x_expanded, dim=2, index=topk_indices_exp)\n",
    "    \n",
    "    return neighbors, topk_indices, similarity_matrix, x_flat, padded_h, padded_w\n",
    "\n",
    "def find_value_position(x_flat, value, padded_h, padded_w):\n",
    "    \"\"\"Find the flattened position of a specific value\"\"\"\n",
    "    # Find where the value occurs\n",
    "    positions = (x_flat[0, 0, :] == value).nonzero(as_tuple=False).flatten()\n",
    "    if len(positions) > 0:\n",
    "        pos = positions[0].item()\n",
    "        # Convert back to 2D coordinates for verification\n",
    "        row = pos // padded_w\n",
    "        col = pos % padded_w\n",
    "        print(f\"Value {value} found at flattened position {pos} (2D: row={row}, col={col})\")\n",
    "        return pos\n",
    "    else:\n",
    "        print(f\"Value {value} not found\")\n",
    "        return None\n",
    "\n",
    "def test_spatial_neighbors():\n",
    "    # Create test tensor - your example\n",
    "    x = torch.tensor([[[\n",
    "        [0., 0., 0., 0., 0.],\n",
    "        [0., 1., 2., 3., 0.],\n",
    "        [0., 4., 5., 6., 0.],\n",
    "        [0., 7., 8., 9., 0.],\n",
    "        [0., 0., 0., 0., 0.]\n",
    "    ]]])\n",
    "    \n",
    "    print(\"Input tensor:\")\n",
    "    print(x.squeeze())\n",
    "    print()\n",
    "    \n",
    "    # Get 9 nearest neighbors\n",
    "    neighbors, indices, sim_matrix, x_flat, padded_h, padded_w = get_spatial_neighbors(x, K=9, sigma=0.3)\n",
    "    \n",
    "    print(\"Padded and flattened tensor:\")\n",
    "    print(\"Shape:\", x_flat.shape)\n",
    "    print(\"Values:\", x_flat[0, 0, :])\n",
    "    print()\n",
    "    \n",
    "    # Find positions of values 5 and 1\n",
    "    pos_5 = find_value_position(x_flat, 5.0, padded_h, padded_w)\n",
    "    pos_1 = find_value_position(x_flat, 1.0, padded_h, padded_w)\n",
    "    \n",
    "    if pos_5 is not None:\n",
    "        print(f\"\\nNeighbors for center position (value 5) at position {pos_5}:\")\n",
    "        center_neighbors = neighbors[0, 0, pos_5, :]\n",
    "        print(\"Values:\", center_neighbors)\n",
    "        print(\"Indices:\", indices[0, pos_5, :])\n",
    "        \n",
    "        # Show the neighbor positions in 2D grid for verification\n",
    "        print(\"Neighbor positions in 2D grid:\")\n",
    "        for i, idx in enumerate(indices[0, pos_5, :]):\n",
    "            row = idx.item() // padded_w\n",
    "            col = idx.item() % padded_w\n",
    "            val = center_neighbors[i].item()\n",
    "            print(f\"  Index {idx.item()}: (row={row}, col={col}), value={val}\")\n",
    "    \n",
    "    if pos_1 is not None:\n",
    "        print(f\"\\nNeighbors for position with value 1 at position {pos_1}:\")\n",
    "        neighbors_1 = neighbors[0, 0, pos_1, :]\n",
    "        print(\"Values:\", neighbors_1)\n",
    "        print(\"Indices:\", indices[0, pos_1, :])\n",
    "        \n",
    "        # Show the neighbor positions in 2D grid for verification\n",
    "        print(\"Neighbor positions in 2D grid:\")\n",
    "        for i, idx in enumerate(indices[0, pos_1, :]):\n",
    "            row = idx.item() // padded_w\n",
    "            col = idx.item() % padded_w\n",
    "            val = neighbors_1[i].item()\n",
    "            print(f\"  Index {idx.item()}: (row={row}, col={col}), value={val}\")\n",
    "    \n",
    "    return neighbors, indices, sim_matrix\n",
    "\n",
    "def get_3x3_neighbors_direct(x):\n",
    "    \"\"\"\n",
    "    Directly get 3x3 neighbors without similarity matrix calculation\n",
    "    \"\"\"\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Pad the input\n",
    "    x_padded = F.pad(x, (1, 1, 1, 1), mode='constant', value=0)\n",
    "    \n",
    "    # Use unfold to get 3x3 patches\n",
    "    patches = x_padded.unfold(2, 3, 1).unfold(3, 3, 1)  # [B, C, H, W, 3, 3]\n",
    "    patches = patches.reshape(b, c, h*w, 9)  # [B, C, H*W, 9]\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Test both approaches\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Coordinate-based similarity approach ===\")\n",
    "    neighbors, indices, sim_matrix = test_spatial_neighbors()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b85b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a5fe483",
   "metadata": {},
   "source": [
    "# MyTopK function\n",
    "- need to make selecting Ks more stable (same pattern for when comparing same similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0822b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k values: tensor([0.5000, 0.5000])\n",
      "Original indices of top-k values: tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor\n",
    "data = torch.tensor([0.1, 0.5, 0.2, 0.5, 0.3])\n",
    "k = 2\n",
    "\n",
    "# Get sorted values and indices with stable tie-breaking\n",
    "sorted_values, sorted_indices = torch.sort(data, descending=True, stable=True)\n",
    "\n",
    "# Select the top-k elements and their original indices\n",
    "topk_values = sorted_values[:k]\n",
    "topk_original_indices = sorted_indices[:k]\n",
    "\n",
    "print(f\"Top-k values: {topk_values}\")\n",
    "print(f\"Original indices of top-k values: {topk_original_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7eefe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k values: tensor([0.5000, 0.5000])\n",
      "Original indices of top-k values: tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor\n",
    "data = torch.tensor([0.1, 0.5, 0.2, 0.5, 0.3])\n",
    "k = 2\n",
    "\n",
    "topk_values, topk_indices = torch.topk(data, k, largest=True)\n",
    "print(f\"Top-k values: {topk_values}\")\n",
    "print(f\"Original indices of top-k values: {topk_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a4fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
