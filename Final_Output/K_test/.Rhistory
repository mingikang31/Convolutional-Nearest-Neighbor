source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_long
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_baseline
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_test
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_baseline
df_k_test
df_baseline
df_baseline
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_convnn_k_test
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_long
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
spec()
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
df_k_test
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
source("~/Developer/Convolutional-Nearest-Neighbor/Final_Output/K_test/plots2.r")
install.packages("svglite") # Good for saving high-quality vector graphics
################################################################################
# Publication-Grade Plots with ggplot2: Complete Tutorial
# Author: Tutorial for Publication-Quality Visualizations
################################################################################
# Install required packages (run once)
# install.packages(c("tidyverse", "ggplot2", "patchwork", "scales",
#                    "RColorBrewer", "viridis", "cowplot", "ggpubr",
#                    "extrafont", "showtext"))
library(tidyverse)
library(ggplot2)
library(patchwork)  # For combining plots
library(scales)     # For scale functions
library(viridis)    # For color palettes
################################################################################
# SECTION 1: FUNDAMENTAL PRINCIPLES
################################################################################
# Key principles for publication-quality plots:
# 1. High resolution (300+ DPI for print)
# 2. Clear, readable fonts (10-12pt minimum)
# 3. Appropriate colors (colorblind-friendly)
# 4. Clean, minimal design
# 5. Proper aspect ratio
# 6. Clear labels and legends
# 7. Consistent styling across figures
################################################################################
# SECTION 2: BASIC SETUP - Creating Sample Data
################################################################################
# Generate sample data similar to ML training curves
set.seed(123)
epochs <- 1:100
# Simulate training curves
training_data <- tibble(
epoch = rep(epochs, 3),
model = rep(c("Model_A", "Model_B", "Model_C"), each = 100),
train_loss = c(
2 * exp(-epochs/20) + rnorm(100, 0, 0.05),
2.5 * exp(-epochs/25) + rnorm(100, 0, 0.06),
1.8 * exp(-epochs/15) + rnorm(100, 0, 0.04)
),
val_loss = c(
2 * exp(-epochs/20) + 0.3 + rnorm(100, 0, 0.08),
2.5 * exp(-epochs/25) + 0.25 + rnorm(100, 0, 0.09),
1.8 * exp(-epochs/15) + 0.35 + rnorm(100, 0, 0.07)
),
train_acc = c(
100 * (1 - exp(-epochs/15)) + rnorm(100, 0, 2),
100 * (1 - exp(-epochs/20)) + rnorm(100, 0, 2.5),
100 * (1 - exp(-epochs/12)) + rnorm(100, 0, 1.5)
),
val_acc = c(
95 * (1 - exp(-epochs/15)) + rnorm(100, 0, 3),
95 * (1 - exp(-epochs/20)) + rnorm(100, 0, 3.5),
95 * (1 - exp(-epochs/12)) + rnorm(100, 0, 2.5)
)
)
# --- BASIC PLOT (What NOT to do) ---
basic_plot <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line()
print("Basic Plot (needs improvement):")
print(basic_plot)
# --- PUBLICATION PLOT (What TO do) ---
publication_plot <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_brewer(
palette = "Set1",
name = "Model",
labels = c("Model A", "Model B", "Model C")
) +
labs(
title = "Training Loss Comparison",
x = "Epoch",
y = "Training Loss"
) +
theme_bw(base_size = 12) +
theme(
plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
axis.title = element_text(face = "bold", size = 12),
legend.position = "top",
legend.title = element_text(face = "bold"),
panel.grid.minor = element_blank(),
panel.border = element_rect(color = "black", linewidth = 1)
)
print("Publication Plot (much better!):")
print(publication_plot)
################################################################################
# SECTION 4: CUSTOM THEMES - Build Your Own
################################################################################
# Create a custom theme for consistency across all plots
theme_publication <- function(base_size = 12, base_family = "") {
theme_bw(base_size = base_size, base_family = base_family) +
theme(
# Text elements
plot.title = element_text(face = "bold", size = rel(1.2), hjust = 0.5,
margin = margin(0, 0, 10, 0)),
plot.subtitle = element_text(size = rel(1.0), hjust = 0.5,
margin = margin(0, 0, 10, 0)),
axis.title = element_text(face = "bold", size = rel(1.0)),
axis.text = element_text(size = rel(0.9), color = "black"),
# Legend
legend.title = element_text(face = "bold", size = rel(1.0)),
legend.text = element_text(size = rel(0.9)),
legend.position = "top",
legend.key.size = unit(1.5, "lines"),
legend.background = element_blank(),
# Panel
panel.grid.major = element_line(color = "grey90", linewidth = 0.3),
panel.grid.minor = element_blank(),
panel.border = element_rect(color = "black", fill = NA, linewidth = 0.8),
panel.background = element_rect(fill = "white"),
# Plot margins
plot.margin = margin(10, 10, 10, 10),
# Strip (for facets)
strip.background = element_rect(fill = "grey95", color = "black", linewidth = 0.8),
strip.text = element_text(face = "bold", size = rel(1.0))
)
}
# Test the custom theme
plot_with_custom_theme <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Custom Theme Example", x = "Epoch", y = "Training Loss") +
theme_publication()
print(plot_with_custom_theme)
# --- Option 1: Manual color specification ---
# Good for small number of categories (2-4)
colors_manual <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
"#0072B2", "#D55E00", "#CC79A7")
plot_manual_colors <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_manual(
values = colors_manual[1:3],
name = "Model",
labels = c("Model A", "Model B", "Model C")
) +
labs(title = "Manual Colors (Colorblind-Safe)", x = "Epoch", y = "Training Loss") +
theme_publication()
print(plot_manual_colors)
# --- Option 2: Viridis scales (excellent for continuous and categorical) ---
plot_viridis <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_viridis_d(
option = "D",  # Options: A, B, C, D, E
name = "Model",
begin = 0.2,   # Start at 20% of color range
end = 0.9      # End at 90% of color range
) +
labs(title = "Viridis Colors", x = "Epoch", y = "Training Loss") +
theme_publication()
print(plot_viridis)
# --- Option 1: Manual color specification ---
# Good for small number of categories (2-4)
colors_manual <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442",
"#0072B2", "#D55E00", "#CC79A7")
plot_manual_colors <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_manual(
values = colors_manual[1:3],
name = "Model",
labels = c("Model A", "Model B", "Model C")
) +
labs(title = "Manual Colors (Colorblind-Safe)", x = "Epoch", y = "Training Loss") +
theme_publication()
print(plot_manual_colors)
# --- Option 2: Viridis scales (excellent for continuous and categorical) ---
plot_viridis <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_viridis_d(
option = "D",  # Options: A, B, C, D, E
name = "Model",
begin = 0.2,   # Start at 20% of color range
end = 0.9      # End at 90% of color range
) +
labs(title = "Viridis Colors", x = "Epoch", y = "Training Loss") +
theme_publication()
print(plot_viridis)
# --- Option 3: ColorBrewer palettes ---
# Sequential: Blues, Greens, Reds, etc.
# Diverging: RdYlBu, RdYlGn, Spectral, etc.
# Qualitative: Set1, Set2, Set3, Paired, Dark2, etc.
plot_brewer <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.2) +
scale_color_brewer(
palette = "Dark2",  # Try: Set1, Set2, Dark2, Paired
name = "Model"
) +
labs(title = "ColorBrewer Palette", x = "Epoch", y = "Training Loss") +
theme_publication()
print(plot_brewer)
# Reshape data for train/val comparison
data_long <- training_data %>%
pivot_longer(
cols = c(train_loss, val_loss),
names_to = "type",
values_to = "loss"
) %>%
mutate(
type = factor(type, levels = c("train_loss", "val_loss"),
labels = c("Training", "Validation"))
)
# Create plot with both training and validation
dual_plot <- ggplot(data_long, aes(x = epoch, y = loss, color = model, linetype = type)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
scale_linetype_manual(
values = c("Training" = "solid", "Validation" = "dashed"),
name = "Dataset"
) +
labs(
title = "Training and Validation Loss",
x = "Epoch",
y = "Loss"
) +
theme_publication() +
theme(
legend.box = "vertical"
)
print(dual_plot)
################################################################################
# SECTION 7: MULTI-PANEL FIGURES with Patchwork
################################################################################
# Create individual plots
p1 <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Training Loss", x = "Epoch", y = "Loss") +
theme_publication() +
theme(legend.position = "none")
p2 <- ggplot(training_data, aes(x = epoch, y = val_loss, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Validation Loss", x = "Epoch", y = "Loss") +
theme_publication() +
theme(legend.position = "none")
p3 <- ggplot(training_data, aes(x = epoch, y = train_acc, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Training Accuracy", x = "Epoch", y = "Accuracy (%)") +
theme_publication() +
theme(legend.position = "none")
p4 <- ggplot(training_data, aes(x = epoch, y = val_acc, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Validation Accuracy", x = "Epoch", y = "Accuracy (%)") +
theme_publication()
# Combine plots using patchwork
# Syntax: + places plots side by side, / stacks them vertically
combined_plot <- (p1 + p2) / (p3 + p4) +
plot_layout(guides = "collect") +  # Collect legends
plot_annotation(
title = "Model Performance Comparison",
theme = theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5))
)
print(combined_plot)
# Create individual plots
p1 <- ggplot(training_data, aes(x = epoch, y = train_loss, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Training Loss", x = "Epoch", y = "Loss") +
theme_publication() +
theme(legend.position = "right")
p2 <- ggplot(training_data, aes(x = epoch, y = val_loss, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Validation Loss", x = "Epoch", y = "Loss") +
theme_publication() +
theme(legend.position = "right")
p3 <- ggplot(training_data, aes(x = epoch, y = train_acc, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Training Accuracy", x = "Epoch", y = "Accuracy (%)") +
theme_publication() +
theme(legend.position = "right")
p4 <- ggplot(training_data, aes(x = epoch, y = val_acc, color = model)) +
geom_line(linewidth = 1.0) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(title = "Validation Accuracy", x = "Epoch", y = "Accuracy (%)") +
theme_publication()
# Combine plots using patchwork
# Syntax: + places plots side by side, / stacks them vertically
combined_plot <- (p1 + p2) / (p3 + p4) +
plot_layout(guides = "collect") +  # Collect legends
plot_annotation(
title = "Model Performance Comparison",
theme = theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5))
)
print(combined_plot)
# Prepare data for faceting
facet_data <- training_data %>%
pivot_longer(
cols = c(train_loss, val_loss, train_acc, val_acc),
names_to = "metric",
values_to = "value"
) %>%
mutate(
metric = factor(metric,
levels = c("train_loss", "val_loss", "train_acc", "val_acc"),
labels = c("Training Loss", "Validation Loss",
"Training Accuracy", "Validation Accuracy"))
)
facet_plot <- ggplot(facet_data, aes(x = epoch, y = value, color = model)) +
geom_line(linewidth = 0.8) +
facet_wrap(~metric, scales = "free_y", ncol = 2) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(
title = "Comprehensive Model Comparison",
x = "Epoch",
y = NULL
) +
theme_publication()
print(facet_plot)
# Find best epoch for Model A
best_epoch_data <- training_data %>%
filter(model == "Model_A") %>%
filter(val_loss == min(val_loss))
annotated_plot <- ggplot(training_data, aes(x = epoch, y = val_loss, color = model)) +
geom_line(linewidth = 1.2) +
# Add vertical line at best epoch
geom_vline(
xintercept = best_epoch_data$epoch,
linetype = "dashed",
color = "gray40",
linewidth = 0.8
) +
# Add point at minimum
geom_point(
data = best_epoch_data,
size = 4,
shape = 21,
fill = "red",
color = "black",
stroke = 1.5
) +
# Add text annotation
annotate(
"text",
x = best_epoch_data$epoch + 10,
y = best_epoch_data$val_loss,
label = sprintf("Best: Epoch %d\nLoss = %.3f",
best_epoch_data$epoch,
best_epoch_data$val_loss),
hjust = 0,
size = 3.5,
fontface = "bold"
) +
# Add shaded region for "early training"
annotate(
"rect",
xmin = 0, xmax = 20,
ymin = -Inf, ymax = Inf,
alpha = 0.1,
fill = "blue"
) +
annotate(
"text",
x = 10, y = max(training_data$val_loss) * 0.95,
label = "Early Training",
size = 3,
fontface = "italic",
color = "blue"
) +
scale_color_brewer(palette = "Set1", name = "Model") +
labs(
title = "Validation Loss with Annotations",
x = "Epoch",
y = "Validation Loss"
) +
theme_publication()
print(annotated_plot)
