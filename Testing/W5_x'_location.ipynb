{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X' (X Prime) Experiment \n",
    "- Adding the location channels in the beginning of the image, and not at each layer step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingikang/miniforge3/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Torch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim \n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# Train + Data \n",
    "import sys \n",
    "sys.path.append('../Layers')\n",
    "from Conv1d_NN import *\n",
    "from Conv2d_NN import *\n",
    "from Conv1d_NN_spatial import * \n",
    "from Conv2d_NN_spatial import * \n",
    "from ConvNN_CNN_Branching import *\n",
    "\n",
    "sys.path.append('../Data')\n",
    "from CIFAR10 import * \n",
    "\n",
    "\n",
    "sys.path.append('../Models')\n",
    "from models_2d import *\n",
    "\n",
    "sys.path.append('../Train')\n",
    "from train2d import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10 = CIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN\n",
    "class CNN_Location_Before(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=10, kernel_size=3):\n",
    "        super(CNN_Location_Before, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch+2, 16, kernel_size=kernel_size, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=kernel_size, stride=1, padding=1)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(32768, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.to(\"mps\")\n",
    "        self.name = \"CNN\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_coordinates = self.coordinate_channels(x.shape, x.device)\n",
    "        x = torch.cat((x, x_coordinates), dim=1)\n",
    "        \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def summary(self, input_size = (3, 32, 32)): \n",
    "        self.to(\"cpu\")\n",
    "        print(summary(self, input_size))\n",
    "        self.to(\"mps\")\n",
    "            \n",
    "    def coordinate_channels(self, tensor_shape, device):\n",
    "        x_ind = torch.arange(0, tensor_shape[2])\n",
    "        y_ind = torch.arange(0, tensor_shape[3])\n",
    "        \n",
    "        x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "        \n",
    "        x_grid = x_grid.float().unsqueeze(0).expand(tensor_shape[0], -1, -1).unsqueeze(1)\n",
    "        y_grid = y_grid.float().unsqueeze(0).expand(tensor_shape[0], -1, -1).unsqueeze(1)\n",
    "        \n",
    "        xy_grid = torch.cat((x_grid, y_grid), dim=1)\n",
    "        xy_grid_normalized = F.normalize(xy_grid, p=2, dim=1)\n",
    "        return xy_grid_normalized.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN_2D_K_All_Location_Before(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=10, K=9):\n",
    "        super(ConvNN_2D_K_All_Location_Before, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d_NN(in_ch+2, 16, K=K, stride=K, shuffle_pattern=\"BA\", shuffle_scale=2, samples=\"all\")\n",
    "        self.conv2 = Conv2d_NN(16, 32, K=K, stride=K, shuffle_pattern=\"BA\", shuffle_scale=2, samples=\"all\")\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(32768, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.to(\"mps\")\n",
    "        self.name = \"ConvNN_2D_K_All\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_coordinates = self.coordinate_channels(x.shape, x.device)\n",
    "        x = torch.cat((x, x_coordinates), dim=1)\n",
    "        \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def summary(self, input_size = (3, 32, 32)): \n",
    "        self.to(\"cpu\")\n",
    "        print(summary(self, input_size))\n",
    "        self.to(\"mps\")\n",
    "        \n",
    "    def coordinate_channels(self, tensor_shape, device):\n",
    "        x_ind = torch.arange(0, tensor_shape[2])\n",
    "        y_ind = torch.arange(0, tensor_shape[3])\n",
    "        \n",
    "        x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "        \n",
    "        x_grid = x_grid.float().unsqueeze(0).expand(tensor_shape[0], -1, -1).unsqueeze(1)\n",
    "        y_grid = y_grid.float().unsqueeze(0).expand(tensor_shape[0], -1, -1).unsqueeze(1)\n",
    "        \n",
    "        xy_grid = torch.cat((x_grid, y_grid), dim=1)\n",
    "        xy_grid_normalized = F.normalize(xy_grid, p=2, dim=1)\n",
    "        return xy_grid_normalized.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branching_ConvNN_2D_Spatial_K_N_Location_Before(nn.Module):\n",
    "    def __init__(self, in_ch=3, channel_ratio=(16, 16), num_classes=10, kernel_size=3, K=9, N = 8, location_channels = False):\n",
    "        \n",
    "        super(Branching_ConvNN_2D_Spatial_K_N_Location_Before, self).__init__()\n",
    "        self.conv1 = ConvNN_CNN_Spatial_BranchingLayer(in_ch+2, 16, \n",
    "            channel_ratio=channel_ratio,kernel_size=kernel_size, K=K, samples=N, location_channels=location_channels)\n",
    "        self.conv2 = ConvNN_CNN_Spatial_BranchingLayer(16, 32, channel_ratio=(channel_ratio[0] *2, channel_ratio[1]*2),kernel_size=kernel_size, K=K, samples=N, location_channels=location_channels)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(32768, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.to(\"mps\")\n",
    "        self.name = \"Branching_ConvNN_2D_Spatial_K_N\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_coordinates = self.coordinate_channels(x.shape, x.device)\n",
    "        x = torch.cat((x, x_coordinates), dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def summary(self, input_size = (3, 32, 32)): \n",
    "        self.to(\"cpu\")\n",
    "        print(summary(self, input_size))\n",
    "        self.to(\"mps\")\n",
    "        \n",
    "    def coordinate_channels(self, tensor_shape, device):\n",
    "        x_ind = torch.arange(0, tensor_shape[2])\n",
    "        y_ind = torch.arange(0, tensor_shape[3])\n",
    "        \n",
    "        x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "        \n",
    "        x_grid = x_grid.float().unsqueeze(0).expand(tensor_shape[0], -1, -1).unsqueeze(1)\n",
    "        y_grid = y_grid.float().unsqueeze(0).expand(tensor_shape[0], -1, -1).unsqueeze(1)\n",
    "        \n",
    "        xy_grid = torch.cat((x_grid, y_grid), dim=1)\n",
    "        xy_grid_normalized = F.normalize(xy_grid, p=2, dim=1)\n",
    "        return xy_grid_normalized.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 30.6076557636261, Loss: 1.320821599277389\n",
      "Epoch 2, Time: 34.71222472190857, Loss: 0.804229320124592\n",
      "Epoch 3, Time: 34.860244035720825, Loss: 0.42631748023316685\n",
      "Epoch 4, Time: 34.507848024368286, Loss: 0.14106527700915436\n",
      "Epoch 5, Time: 34.50970196723938, Loss: 0.06549997788513331\n",
      "Epoch 6, Time: 34.5172758102417, Loss: 0.049463629004452614\n",
      "Epoch 7, Time: 34.6102077960968, Loss: 0.05258563129365911\n",
      "Epoch 8, Time: 36.005460023880005, Loss: 0.0432165561500988\n",
      "Epoch 9, Time: 42.281323194503784, Loss: 0.04389226100365262\n",
      "Epoch 10, Time: 51.81594204902649, Loss: 0.03636969356222109\n",
      "\n",
      " Average epoch time: 36.842788338661194\n",
      "Accuracy on test set: 64.05%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN with Location Channels added before \n",
    "cnn_location_before = CNN_Location_Before()\n",
    "\n",
    "cnn_location_before.to('mps')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_location_before.parameters(), lr=0.001)\n",
    "num_epochs = 10 \n",
    "train_model(cnn_location_before, cifar10.train_loader, criterion, optimizer, num_epochs)\n",
    "evaluate_accuracy(cnn_location_before, cifar10.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 88.54998207092285, Loss: 1.4418080857647655\n",
      "Epoch 2, Time: 102.55355286598206, Loss: 1.0979220754350238\n",
      "Epoch 3, Time: 105.41861128807068, Loss: 0.8576833204463925\n",
      "Epoch 4, Time: 107.13352918624878, Loss: 0.5801344805056482\n",
      "Epoch 5, Time: 104.16446471214294, Loss: 0.33415084219802066\n",
      "Epoch 6, Time: 99.26240801811218, Loss: 0.20348460427330584\n",
      "Epoch 7, Time: 101.254225730896, Loss: 0.15475174778467402\n",
      "Epoch 8, Time: 97.87570118904114, Loss: 0.12964877465625516\n",
      "Epoch 9, Time: 93.44150996208191, Loss: 0.1318924545585547\n",
      "Epoch 10, Time: 92.4614040851593, Loss: 0.09777499881991045\n",
      "\n",
      " Average epoch time: 99.21153891086578\n",
      "Accuracy on test set: 57.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.81"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ConNN 2D K = 9, All Samples with Location Channels added before\n",
    "convNN_k_all_location_before = ConvNN_2D_K_All_Location_Before()\n",
    "\n",
    "convNN_k_all_location_before.to('mps')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(convNN_k_all_location_before.parameters(), lr=0.001)\n",
    "num_epochs = 10 \n",
    "train_model(convNN_k_all_location_before, cifar10.train_loader, criterion, optimizer, num_epochs)\n",
    "evaluate_accuracy(convNN_k_all_location_before, cifar10.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Time: 94.06807804107666, Loss: 1.4150934026521795\n",
      "Epoch 2, Time: 103.46013283729553, Loss: 0.9707661204020995\n",
      "Epoch 3, Time: 103.36579298973083, Loss: 0.6381737245699329\n",
      "Epoch 4, Time: 95.47790884971619, Loss: 0.27994981874018676\n",
      "Epoch 5, Time: 96.22189211845398, Loss: 0.1296414005298577\n",
      "Epoch 6, Time: 109.62095499038696, Loss: 0.11480781364330875\n",
      "Epoch 7, Time: 93.30559086799622, Loss: 0.08681446669028972\n",
      "Epoch 8, Time: 89.27326512336731, Loss: 0.07692564570683214\n",
      "Epoch 9, Time: 100.74316811561584, Loss: 0.07736872752864614\n",
      "Epoch 10, Time: 136.39051604270935, Loss: 0.06751669987137286\n",
      "\n",
      " Average epoch time: 102.19272999763488\n",
      "Accuracy on test set: 65.72%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.72"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Branching ConvNN 2D Spatial Sampling with Location Channels added before\n",
    "branching_convNN_spatial_k_n_location_before = Branching_ConvNN_2D_Spatial_K_N_Location_Before()\n",
    "\n",
    "branching_convNN_spatial_k_n_location_before.to('mps')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(branching_convNN_spatial_k_n_location_before.parameters(), lr=0.001)\n",
    "num_epochs = 10 \n",
    "train_model(branching_convNN_spatial_k_n_location_before, cifar10.train_loader, criterion, optimizer, num_epochs)\n",
    "evaluate_accuracy(branching_convNN_spatial_k_n_location_before, cifar10.test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
