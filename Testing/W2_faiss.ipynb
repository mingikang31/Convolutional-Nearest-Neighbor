{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "\n",
    "from faiss.contrib import datasets\n",
    "from faiss.contrib.inspect_tools import get_additive_quantizer_codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch  # usort: skip\n",
    "from torch import nn  # usort: skip\n",
    "import unittest  # usort: skip\n",
    "import numpy as np  # usort: skip\n",
    "\n",
    "import faiss  # usort: skip\n",
    "\n",
    "from faiss.contrib import datasets  # usort: skip\n",
    "from faiss.contrib.inspect_tools import get_additive_quantizer_codebooks  # usort: skip\n",
    "\n",
    "\n",
    "class TestLayer(unittest.TestCase):\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_Embedding(self):\n",
    "        \"\"\" verify that the Faiss Embedding works the same as in Pytorch \"\"\"\n",
    "        torch.manual_seed(123)\n",
    "\n",
    "        emb = nn.Embedding(40, 50)\n",
    "        idx = torch.randint(40, (25, ))\n",
    "        ref_batch = emb(idx)\n",
    "\n",
    "        emb2 = faiss.Embedding(emb)\n",
    "        idx2 = faiss.Int32Tensor2D(idx[:, None].to(dtype=torch.int32))\n",
    "        new_batch = emb2(idx2)\n",
    "\n",
    "        new_batch = new_batch.numpy()\n",
    "        np.testing.assert_allclose(ref_batch.numpy(), new_batch, atol=2e-6)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def do_test_Linear(self, bias):\n",
    "        \"\"\" verify that the Faiss Linear works the same as in Pytorch \"\"\"\n",
    "        torch.manual_seed(123)\n",
    "        linear = nn.Linear(50, 40, bias=bias)\n",
    "        x = torch.randn(25, 50)\n",
    "        ref_y = linear(x)\n",
    "\n",
    "        linear2 = faiss.Linear(linear)\n",
    "        x2 = faiss.Tensor2D(x)\n",
    "        y = linear2(x2)\n",
    "        np.testing.assert_allclose(ref_y.numpy(), y.numpy(), atol=2e-6)\n",
    "\n",
    "    def test_Linear(self):\n",
    "        self.do_test_Linear(True)\n",
    "\n",
    "    def test_Linear_nobias(self):\n",
    "        self.do_test_Linear(False)\n",
    "\n",
    "######################################################\n",
    "# QINCo Pytorch implementation copied from\n",
    "# https://github.com/facebookresearch/Qinco/blob/main/model_qinco.py\n",
    "#\n",
    "# The implementation is copied here to avoid introducting an additional\n",
    "# dependency.\n",
    "######################################################\n",
    "\n",
    "\n",
    "def pairwise_distances(a, b):\n",
    "    anorms = (a**2).sum(-1)\n",
    "    bnorms = (b**2).sum(-1)\n",
    "    return anorms[:, None] + bnorms - 2 * a @ b.T\n",
    "\n",
    "\n",
    "def compute_batch_distances(a, b):\n",
    "    anorms = (a**2).sum(-1)\n",
    "    bnorms = (b**2).sum(-1)\n",
    "    return (\n",
    "        anorms.unsqueeze(-1) + bnorms.unsqueeze(1) - 2 * torch.bmm(a, b.transpose(2, 1))\n",
    "    )\n",
    "\n",
    "\n",
    "def assign_batch_multiple(x, zqs):\n",
    "    bs, d = x.shape\n",
    "    bs, K, d = zqs.shape\n",
    "\n",
    "    L2distances = compute_batch_distances(x.unsqueeze(1), zqs).squeeze(1)  # [bs x ksq]\n",
    "    idx = torch.argmin(L2distances, dim=1).unsqueeze(1)  # [bsx1]\n",
    "    quantized = torch.gather(zqs, dim=1, index=idx.unsqueeze(-1).repeat(1, 1, d))\n",
    "    return idx.squeeze(1), quantized.squeeze(1)\n",
    "\n",
    "\n",
    "def assign_to_codebook(x, c, bs=16384):\n",
    "    nq, d = x.shape\n",
    "    nb, d2 = c.shape\n",
    "    assert d == d2\n",
    "    if nq * nb < bs * bs:\n",
    "        # small enough to represent the whole distance table\n",
    "        dis = pairwise_distances(x, c)\n",
    "        return dis.argmin(1)\n",
    "\n",
    "    # otherwise tile computation to avoid OOM\n",
    "    res = torch.empty((nq,), dtype=torch.int64, device=x.device)\n",
    "    cnorms = (c**2).sum(1)\n",
    "    for i in range(0, nq, bs):\n",
    "        xnorms = (x[i : i + bs] ** 2).sum(1, keepdim=True)\n",
    "        for j in range(0, nb, bs):\n",
    "            dis = xnorms + cnorms[j : j + bs] - 2 * x[i : i + bs] @ c[j : j + bs].T\n",
    "            dmini, imini = dis.min(1)\n",
    "            if j == 0:\n",
    "                dmin = dmini\n",
    "                imin = imini\n",
    "            else:\n",
    "                (mask,) = torch.where(dmini < dmin)\n",
    "                dmin[mask] = dmini[mask]\n",
    "                imin[mask] = imini[mask] + j\n",
    "        res[i : i + bs] = imin\n",
    "    return res\n",
    "\n",
    "\n",
    "class QINCoStep(nn.Module):\n",
    "    \"\"\"\n",
    "    One quantization step for QINCo.\n",
    "    Contains the codebook, concatenation block, and residual blocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, K, L, h):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.d, self.K, self.L, self.h = d, K, L, h\n",
    "\n",
    "        self.codebook = nn.Embedding(K, d)\n",
    "        self.MLPconcat = nn.Linear(2 * d, d)\n",
    "\n",
    "        self.residual_blocks = []\n",
    "        for l in range(L):\n",
    "            residual_block = nn.Sequential(\n",
    "                nn.Linear(d, h, bias=False), nn.ReLU(), nn.Linear(h, d, bias=False)\n",
    "            )\n",
    "            self.add_module(f\"residual_block{l}\", residual_block)\n",
    "            self.residual_blocks.append(residual_block)\n",
    "\n",
    "    def decode(self, xhat, codes):\n",
    "        zqs = self.codebook(codes)\n",
    "        cc = torch.concatenate((zqs, xhat), 1)\n",
    "        zqs = zqs + self.MLPconcat(cc)\n",
    "\n",
    "        for residual_block in self.residual_blocks:\n",
    "            zqs = zqs + residual_block(zqs)\n",
    "\n",
    "        return zqs\n",
    "\n",
    "    def encode(self, xhat, x):\n",
    "        # we are trying out the whole codebook\n",
    "        zqs = self.codebook.weight\n",
    "        K, d = zqs.shape\n",
    "        bs, d = xhat.shape\n",
    "\n",
    "        # repeat so that they are of size bs * K\n",
    "        zqs_r = zqs.repeat(bs, 1, 1).reshape(bs * K, d)\n",
    "        xhat_r = xhat.reshape(bs, 1, d).repeat(1, K, 1).reshape(bs * K, d)\n",
    "\n",
    "        # pass on batch of size bs * K\n",
    "        cc = torch.concatenate((zqs_r, xhat_r), 1)\n",
    "        zqs_r = zqs_r + self.MLPconcat(cc)\n",
    "\n",
    "        for residual_block in self.residual_blocks:\n",
    "            zqs_r = zqs_r + residual_block(zqs_r)\n",
    "\n",
    "        # possible next steps\n",
    "        zqs_r = zqs_r.reshape(bs, K, d) + xhat.reshape(bs, 1, d)\n",
    "        codes, xhat_next = assign_batch_multiple(x, zqs_r)\n",
    "\n",
    "        return codes, xhat_next - xhat\n",
    "\n",
    "\n",
    "class QINCo(nn.Module):\n",
    "    \"\"\"\n",
    "    QINCo quantizer, built from a chain of residual quantization steps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d, K, L, M, h):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.d, self.K, self.L, self.M, self.h = d, K, L, M, h\n",
    "\n",
    "        self.codebook0 = nn.Embedding(K, d)\n",
    "\n",
    "        self.steps = []\n",
    "        for m in range(1, M):\n",
    "            step = QINCoStep(d, K, L, h)\n",
    "            self.add_module(f\"step{m}\", step)\n",
    "            self.steps.append(step)\n",
    "\n",
    "    def decode(self, codes):\n",
    "        xhat = self.codebook0(codes[:, 0])\n",
    "        for i, step in enumerate(self.steps):\n",
    "            xhat = xhat + step.decode(xhat, codes[:, i + 1])\n",
    "        return xhat\n",
    "\n",
    "    def encode(self, x, code0=None):\n",
    "        \"\"\"\n",
    "        Encode a batch of vectors x to codes of length M.\n",
    "        If this function is called from IVF-QINCo, codes are 1 index longer,\n",
    "        due to the first index being the IVF index, and codebook0 is the IVF codebook.\n",
    "        \"\"\"\n",
    "        M = len(self.steps) + 1\n",
    "        bs, d = x.shape\n",
    "        codes = torch.zeros(bs, M, dtype=int, device=x.device)\n",
    "\n",
    "        if code0 is None:\n",
    "            # at IVF training time, the code0 is fixed (and precomputed)\n",
    "            code0 = assign_to_codebook(x, self.codebook0.weight)\n",
    "\n",
    "        codes[:, 0] = code0\n",
    "        xhat = self.codebook0.weight[code0]\n",
    "\n",
    "        for i, step in enumerate(self.steps):\n",
    "            codes[:, i + 1], toadd = step.encode(xhat, x)\n",
    "            xhat = xhat + toadd\n",
    "\n",
    "        return codes, xhat\n",
    "\n",
    "\n",
    "######################################################\n",
    "# QINCo tests\n",
    "######################################################\n",
    "\n",
    "def copy_QINCoStep(step):\n",
    "    step2 = faiss.QINCoStep(step.d, step.K, step.L, step.h)\n",
    "    step2.codebook.from_torch(step.codebook)\n",
    "    step2.MLPconcat.from_torch(step.MLPconcat)\n",
    "\n",
    "    for l in range(step.L):\n",
    "        src = step.residual_blocks[l]\n",
    "        dest = step2.get_residual_block(l)\n",
    "        dest.linear1.from_torch(src[0])\n",
    "        dest.linear2.from_torch(src[2])\n",
    "    return step2\n",
    "\n",
    "\n",
    "class TestQINCoStep(unittest.TestCase):\n",
    "    @torch.no_grad()\n",
    "    def test_decode(self):\n",
    "        torch.manual_seed(123)\n",
    "        step = QINCoStep(d=16, K=20, L=2, h=8)\n",
    "\n",
    "        codes = torch.randint(0, 20, (10, ))\n",
    "        xhat = torch.randn(10, 16)\n",
    "        ref_decode = step.decode(xhat, codes)\n",
    "\n",
    "        # step2 = copy_QINCoStep(step)\n",
    "        step2 = faiss.QINCoStep(step)\n",
    "        codes2 = faiss.Int32Tensor2D(codes[:, None].to(dtype=torch.int32))\n",
    "\n",
    "        np.testing.assert_array_equal(\n",
    "            step.codebook(codes).numpy(),\n",
    "            step2.codebook(codes2).numpy()\n",
    "        )\n",
    "\n",
    "        xhat2 = faiss.Tensor2D(xhat)\n",
    "        # xhat2 = faiss.Tensor2D(len(codes), step2.d)\n",
    "\n",
    "        new_decode = step2.decode(xhat2, codes2)\n",
    "\n",
    "        np.testing.assert_allclose(\n",
    "            ref_decode.numpy(),\n",
    "            new_decode.numpy(),\n",
    "            atol=2e-6\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_encode(self):\n",
    "        torch.manual_seed(123)\n",
    "        step = QINCoStep(d=16, K=20, L=2, h=8)\n",
    "\n",
    "        # create plausible x for testing starting from actual codes\n",
    "        codes = torch.randint(0, 20, (10, ))\n",
    "        xhat = torch.zeros(10, 16)\n",
    "        x = step.decode(xhat, codes)\n",
    "        del codes\n",
    "        ref_codes, toadd = step.encode(xhat, x)\n",
    "\n",
    "        step2 = copy_QINCoStep(step)\n",
    "        xhat2 = faiss.Tensor2D(xhat)\n",
    "        x2 = faiss.Tensor2D(x)\n",
    "        toadd2 = faiss.Tensor2D(10, 16)\n",
    "\n",
    "        new_codes = step2.encode(xhat2, x2, toadd2)\n",
    "\n",
    "        np.testing.assert_allclose(\n",
    "            ref_codes.numpy(),\n",
    "            new_codes.numpy().ravel(),\n",
    "            atol=2e-6\n",
    "        )\n",
    "        np.testing.assert_allclose(toadd.numpy(), toadd2.numpy(), atol=2e-6)\n",
    "\n",
    "\n",
    "\n",
    "class TestQINCo(unittest.TestCase):\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_decode(self):\n",
    "        torch.manual_seed(123)\n",
    "        qinco = QINCo(d=16, K=20, L=2, M=3, h=8)\n",
    "        codes = torch.randint(0, 20, (10, 3))\n",
    "        x_ref = qinco.decode(codes)\n",
    "\n",
    "        qinco2 = faiss.QINCo(qinco)\n",
    "        codes2 = faiss.Int32Tensor2D(codes.to(dtype=torch.int32))\n",
    "        x_new = qinco2.decode(codes2)\n",
    "\n",
    "        np.testing.assert_allclose(x_ref.numpy(), x_new.numpy(), atol=2e-6)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_encode(self):\n",
    "        torch.manual_seed(123)\n",
    "        qinco = QINCo(d=16, K=20, L=2, M=3, h=8)\n",
    "        codes = torch.randint(0, 20, (10, 3))\n",
    "        x = qinco.decode(codes)\n",
    "        del codes\n",
    "\n",
    "        ref_codes, _ = qinco.encode(x)\n",
    "\n",
    "        qinco2 = faiss.QINCo(qinco)\n",
    "        x2 = faiss.Tensor2D(x)\n",
    "\n",
    "        new_codes = qinco2.encode(x2)\n",
    "\n",
    "        np.testing.assert_allclose(ref_codes.numpy(), new_codes.numpy(), atol=2e-6)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Test index\n",
    "######################################################\n",
    "\n",
    "class TestIndexQINCo(unittest.TestCase):\n",
    "\n",
    "    def test_search(self):\n",
    "        \"\"\"\n",
    "        We can't train qinco with just Faiss so we just train a RQ and use the \n",
    "        codebooks in QINCo with L = 0 residual blocks\n",
    "        \"\"\"\n",
    "        ds = datasets.SyntheticDataset(32, 1000, 100, 0)\n",
    "\n",
    "        # prepare reference quantizer\n",
    "        M = 5\n",
    "        index_ref = faiss.index_factory(ds.d, \"RQ5x4\")\n",
    "        rq = index_ref.rq\n",
    "        # rq = faiss.ResidualQuantizer(ds.d, M, 4)\n",
    "        rq.train_type = faiss.ResidualQuantizer.Train_default\n",
    "        rq.max_beam_size = 1    # beam search not implemented for QINCo (yet)\n",
    "        index_ref.train(ds.get_train())\n",
    "        codebooks = get_additive_quantizer_codebooks(rq)\n",
    "\n",
    "        # convert to QINCo index\n",
    "        qinco_index = faiss.IndexQINCo(ds.d, M, 4, 0, ds.d)\n",
    "        qinco = qinco_index.qinco\n",
    "        qinco.codebook0.from_array(codebooks[0])\n",
    "        for i in range(1, qinco.M):\n",
    "            step = qinco.get_step(i - 1)\n",
    "            step.codebook.from_array(codebooks[i])\n",
    "            # MLPConcat left at zero -- it's added to the backbone\n",
    "        qinco_index.is_trained = True\n",
    "\n",
    "        # verify that the encoding gives the same results\n",
    "        ref_codes = rq.compute_codes(ds.get_database())\n",
    "        ref_decoded = rq.decode(ref_codes)\n",
    "        new_decoded = qinco_index.sa_decode(ref_codes)\n",
    "        np.testing.assert_allclose(ref_decoded, new_decoded, atol=2e-6)\n",
    "\n",
    "        new_codes = qinco_index.sa_encode(ds.get_database())\n",
    "        np.testing.assert_array_equal(ref_codes, new_codes)\n",
    "\n",
    "        # verify that search gives the same results\n",
    "        Dref, Iref = index_ref.search(ds.get_queries(), 5)\n",
    "        Dnew, Inew = qinco_index.search(ds.get_queries(), 5)\n",
    "\n",
    "        np.testing.assert_array_equal(Iref, Inew)\n",
    "        np.testing.assert_allclose(Dref, Dnew, atol=2e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Embeddings Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 3\u001b[0m quantizer \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[43md\u001b[49m)  \u001b[38;5;66;03m# the other index\u001b[39;00m\n\u001b[1;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexIVFFlat(quantizer, d, nlist)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index\u001b[38;5;241m.\u001b[39mis_trained\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "nlist = 100\n",
    "k = 4\n",
    "quantizer = faiss.IndexFlatL2(d)  # the other index\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "assert not index.is_trained\n",
    "index.train(xb)\n",
    "assert index.is_trained\n",
    "\n",
    "index.add(xb)                  # add may be a bit slower as well\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print(I[-5:])                  # neighbors of the 5 last queries\n",
    "index.nprobe = 10              # default nprobe is 1, try a few more\n",
    "D, I = index.search(xq, k)\n",
    "print(I[-5:])                  # neighbors of the 5 last queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random training data \n",
    "mt = np.random.rand(1000, 40).astype('float32')\n",
    "mat = faiss.PCAMatrix (40, 10)\n",
    "mat.train(mt)\n",
    "assert mat.is_trained\n",
    "tr = mat.apply(mt)\n",
    "# print this to show that the magnitude of tr's columns is decreasing\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 64)\n",
      "(10000, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = 64                           # dimension\n",
    "nb = 100000                      # database size\n",
    "nq = 10000                       # nb of queries\n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "print(xb.shape)\n",
    "\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "print(xq.shape)\n",
    "xq[:, 0] += np.arange(nq) / 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "\n",
    "d = 64 \n",
    "nb = 100000\n",
    "nq = 10000\n",
    "torch.random.manual_seed(1234)\n",
    "xb = torch.randn(nb, d)\n",
    "print(\"xb shape\", xb.shape)\n",
    "# xb[:, 0] += torch.arange(nb) / 1000.\n",
    "\n",
    "xq = torch.randn(nq, d)\n",
    "print(\"xq shape\", xq.shape)\n",
    "# xq[:, 0] += torch.arange(nq) / 1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import faiss                   # make faiss available\n",
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "print(index.is_trained)\n",
    "\n",
    "index.add(xb)                  # add vectors to the index\n",
    "print(index.ntotal)\n",
    "\n",
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:5], k) # sanity  ->>>>>>>>> THIS IS THE ERROR BRRRURORRURHUHURHHRUHRRH\n",
    "print(I)\n",
    "print(D)\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print(I[-5:])                  # neighbors of the 5 last queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
