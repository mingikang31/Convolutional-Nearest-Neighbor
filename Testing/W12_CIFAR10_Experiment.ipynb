{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNN Attention Test \n",
    "## I. 2D Training for testing with CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim \n",
    "\n",
    "\n",
    "# Train + Data \n",
    "import sys \n",
    "sys.path.append('../Layers')\n",
    "from Conv1d_NN_spatial import * \n",
    "from Conv2d_NN_spatial import * \n",
    "\n",
    "sys.path.append('../Data')\n",
    "from CIFAR10 import * \n",
    "\n",
    "\n",
    "sys.path.append('../Models')\n",
    "from CIFAR_experiment_models.Attention import Attention\n",
    "from CIFAR_experiment_models.BranchingConvNN import Branching_ConvNN_K_All, Branching_ConvNN_K_N, Branching_ConvNN_Spatial_K_N, Branching_ConvNN_Attention_K_N\n",
    "from CIFAR_experiment_models.ConvNN import ConvNN_K_All,ConvNN_K_N, ConvNN_Spatial_K_N, ConvNN_Attn_K_N\n",
    "from CIFAR_experiment_models.CNN_Control import CNN\n",
    "\n",
    "\n",
    "sys.path.append('../Train')\n",
    "from train2d import train_eval, evaluate_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = CIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Layer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CNN\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m CNN_2 \u001b[38;5;241m=\u001b[39m \u001b[43mCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m CNN_2\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Test + Eval\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/Convolutional-Nearest-Neighbor/Testing/../Models/CIFAR_experiment_models/CNN_Control.py:47\u001b[0m, in \u001b[0;36mCNN.__init__\u001b[0;34m(self, in_ch, mid_ch, num_layers, num_classes, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     41\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# nn.Dropout(0.5),\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(flattened_size, num_classes), \u001b[38;5;66;03m# Increased intermediate size\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.9/site-packages/torch/cuda/__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "# CNN\n",
    "CNN_2 = CNN(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + CNN_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(CNN_2)))\n",
    "print(\"Num layers: \" + str(CNN_2.num_layers))\n",
    "print()\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(CNN_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(CNN_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(CNN_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "Attention_2 = Attention(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Attention_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(Attention_2)))\n",
    "print(\"Num layers: \" + str(Attention_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Attention_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Attention_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Attention_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN All \n",
    "ConvNN_All_2 = ConvNN_K_All(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_All_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_All_2)))\n",
    "print(\"Num layers: \" + str(ConvNN_All_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_All_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_All_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_All_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN N \n",
    "ConvNN_N_2 = ConvNN_K_N(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_N_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_N_2)))\n",
    "print(\"Num layers: \" + str(ConvNN_N_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_N_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_N_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_N_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN Spatial N\n",
    "ConvNN_Spatial_N_2 = ConvNN_Spatial_K_N(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_Spatial_N_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_Spatial_N_2)))\n",
    "print(\"Num layers: \" + str(ConvNN_Spatial_N_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_Spatial_N_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_Spatial_N_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_Spatial_N_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN Attention N\n",
    "ConvNN_Attn_N_2 = ConvNN_Attn_K_N(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_Attn_N_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_Attn_N_2)))\n",
    "print(\"Num layers: \" + str(ConvNN_Attn_N_2.num_layers))\n",
    "print()\n",
    "\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_Attn_N_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_Attn_N_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_Attn_N_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Branching ConvNN All\n",
    "Branching_ConvNN_All_2 = Branching_ConvNN_K_All(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_All_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_All_2)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_All_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_All_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_All_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_All_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Branching ConvNN N\n",
    "Branching_ConvNN_N_2 = Branching_ConvNN_K_N(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_N_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_N_2)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_N_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_N_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_N_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_N_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Branching ConvNN Spatial N\n",
    "Branching_ConvNN_Spatial_N_2 = Branching_ConvNN_Spatial_K_N(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_Spatial_N_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_Spatial_N_2)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_Spatial_N_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_Spatial_N_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_Spatial_N_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_Spatial_N_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Branching ConvNN Attention N \n",
    "Branching_ConvNN_Attn_N_2 = Branching_ConvNN_Attention_K_N(num_layers=2, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_Attn_N_2.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_Attn_N_2)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_Attn_N_2.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_Attn_N_2.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_Attn_N_2, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_Attn_N_2, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Layer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN\n",
    "CNN_4 = CNN(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + CNN_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(CNN_4)))\n",
    "print(\"Num layers: \" + str(CNN_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(CNN_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(CNN_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(CNN_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attention\n",
    "Attention_4 = Attention(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Attention_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(Attention_4)))\n",
    "print(\"Num layers: \" + str(Attention_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Attention_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Attention_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Attention_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN All \n",
    "ConvNN_All_4 = ConvNN_K_All(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_All_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_All_4)))\n",
    "print(\"Num layers: \" + str(ConvNN_All_4.num_layers))\n",
    "print()\n",
    "\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_All_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_All_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_All_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN N \n",
    "ConvNN_N_4 = ConvNN_K_N(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_N_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_N_4)))\n",
    "print(\"Num layers: \" + str(ConvNN_N_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_N_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_N_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_N_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN Spatial N\n",
    "ConvNN_Spatial_N_4 = ConvNN_Spatial_K_N(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_Spatial_N_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_Spatial_N_4)))\n",
    "print(\"Num layers: \" + str(ConvNN_Spatial_N_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_Spatial_N_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_Spatial_N_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_Spatial_N_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN Attention N\n",
    "ConvNN_Attn_N_4 = ConvNN_Attn_K_N(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_Attn_N_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_Attn_N_4)))\n",
    "print(\"Num layers: \" + str(ConvNN_Attn_N_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_Attn_N_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_Attn_N_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_Attn_N_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN All\n",
    "Branching_ConvNN_All_4 = Branching_ConvNN_K_All(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_All_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_All_4)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_All_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_All_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_All_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_All_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN N\n",
    "Branching_ConvNN_N_4 = Branching_ConvNN_K_N(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_N_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_N_4)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_N_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_N_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_N_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_N_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN Spatial N\n",
    "Branching_ConvNN_Spatial_N_4 = Branching_ConvNN_Spatial_K_N(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_Spatial_N_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_Spatial_N_4)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_Spatial_N_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_Spatial_N_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_Spatial_N_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_Spatial_N_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN Attention N \n",
    "Branching_ConvNN_Attn_N_4 = Branching_ConvNN_Attention_K_N(num_layers=4, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_Attn_N_4.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_Attn_N_4)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_Attn_N_4.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_Attn_N_4.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_Attn_N_4, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_Attn_N_4, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Layer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN\n",
    "CNN_8 = CNN(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + CNN_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(CNN_8)))\n",
    "print(\"Num layers: \" + str(CNN_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(CNN_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(CNN_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(CNN_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attention\n",
    "Attention_8 = Attention(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Attention_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(Attention_8)))\n",
    "print(\"Num layers: \" + str(Attention_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Attention_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Attention_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Attention_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN All \n",
    "ConvNN_All_8 = ConvNN_K_All(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_All_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_All_8)))\n",
    "print(\"Num layers: \" + str(ConvNN_All_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_All_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_All_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_All_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN N \n",
    "ConvNN_N_8 = ConvNN_K_N(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_N_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_N_8)))\n",
    "print(\"Num layers: \" + str(ConvNN_N_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_N_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_N_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_N_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN Spatial N\n",
    "ConvNN_Spatial_N_8 = ConvNN_Spatial_K_N(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_Spatial_N_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_Spatial_N_8)))\n",
    "print(\"Num layers: \" + str(ConvNN_Spatial_N_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_Spatial_N_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_Spatial_N_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_Spatial_N_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNN Attention N\n",
    "ConvNN_Attn_N_8 = ConvNN_Attn_K_N(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + ConvNN_Attn_N_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(ConvNN_Attn_N_8)))  \n",
    "print(\"Num layers: \" + str(ConvNN_Attn_N_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ConvNN_Attn_N_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(ConvNN_Attn_N_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(ConvNN_Attn_N_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN All\n",
    "Branching_ConvNN_All_8 = Branching_ConvNN_K_All(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_All_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_All_8)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_All_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_All_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_All_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_All_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN N\n",
    "Branching_ConvNN_N_8 = Branching_ConvNN_K_N(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_N_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_N_8)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_N_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_N_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_N_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_N_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN Spatial N\n",
    "Branching_ConvNN_Spatial_N_8 = Branching_ConvNN_Spatial_K_N(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_Spatial_N_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_Spatial_N_8)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_Spatial_N_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_Spatial_N_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_Spatial_N_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_Spatial_N_8, cifar10.test_loader, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching ConvNN Attention N \n",
    "Branching_ConvNN_Attn_N_8 = Branching_ConvNN_Attention_K_N(num_layers=8, num_classes=10, device='cuda')\n",
    "\n",
    "print(\"Model: \" + Branching_ConvNN_Attn_N_8.name)\n",
    "print(\"Num params: \" + str(count_parameters(Branching_ConvNN_Attn_N_8)))\n",
    "print(\"Num layers: \" + str(Branching_ConvNN_Attn_N_8.num_layers))\n",
    "print()\n",
    "\n",
    "# Test + Eval\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Branching_ConvNN_Attn_N_8.parameters(), lr=0.0001)\n",
    "num_epochs = 100 \n",
    "train_eval(Branching_ConvNN_Attn_N_8, cifar10.train_loader, cifar10.test_loader, criterion, optimizer, num_epochs, device='cuda')\n",
    "evaluate_accuracy(Branching_ConvNN_Attn_N_8, cifar10.test_loader, device='cuda')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
