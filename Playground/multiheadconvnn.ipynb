{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary  \n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c046714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadConvNN(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, conv=nn.Conv2d):\n",
    "        super(MultiHeadConvNN, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.conv = nn.Conv1d(self.d_k, self.d_k, kernel_size=9, padding=4)        \n",
    "        \n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        print(\"\\nIN MULTIHEADCONVNN\")\n",
    "        print(f\"Input shape: {x.shape}\")  # Debugging line\n",
    "        \n",
    "        x = self.split_head(x)  # (B, num_heads, seq_length, d_k)\n",
    "        print(f\"After split_head shape: {x.shape}\")  # Debugging line\n",
    "        \n",
    "        # Reshape for conv1d: combine batch and heads, transpose to get channels last\n",
    "        x_reshaped = x.permute(0, 1, 3, 2).contiguous()  # [B, num_heads, d_k, seq_length]\n",
    "        print(f\"After permute shape: {x_reshaped.shape}\")\n",
    "        x_reshaped = x_reshaped.view(-1, self.d_k, seq_length)  # [B*num_heads, d_k, seq_length]\n",
    "        print(f\"After view shape: {x_reshaped.shape}\")\n",
    "        \n",
    "        # Apply convolution\n",
    "        output = self.conv(x_reshaped)  # [B*num_heads, d_k, seq_length]\n",
    "        print(f\"After conv shape: {output.shape}\")\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.view(batch_size, self.num_heads, self.d_k, seq_length)  # [B, num_heads, d_k, seq_length]\n",
    "        print(f\"After view back shape: {output.shape}\")\n",
    "        output = output.permute(0, 1, 3, 2).contiguous()  # [B, num_heads, seq_length, d_k]\n",
    "        print(f\"After permute back shape: {output.shape}\")\n",
    "        \n",
    "        # Combine heads\n",
    "        output = self.combine_heads(output)  # [B, seq_length, d_model]\n",
    "        print(f\"After combine_heads shape: {output.shape} \\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps \n",
      "d_model:  6\n",
      "Input shape:  torch.Size([128, 3, 224, 224])\n",
      "after patch embedding:  torch.Size([128, 196, 6])\n",
      "after positional encoding:  torch.Size([128, 197, 6])\n",
      "In transformerencoder: before attention:  torch.Size([128, 197, 6])\n",
      "\n",
      "multattn: q:  torch.Size([128, 3, 197, 2])\n",
      "multattn: attn_output:  torch.Size([128, 3, 197, 2])\n",
      "multattn: output:  torch.Size([128, 197, 6])\n",
      "after transformer encoder:  torch.Size([128, 197, 6])\n",
      "after classifier:  torch.Size([128, 10])\n",
      "Output shape:  torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision.transforms as T \n",
    "from torch.optim import AdamW\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "\n",
    "class PatchEmbedding(nn.Module): \n",
    "    def __init__(self, d_model, img_size, patch_size, n_channels=3): \n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model # Dimensionality of Model \n",
    "        self.img_size = img_size # Size of Image\n",
    "        self.patch_size = patch_size # Patch Size \n",
    "        self.n_channels = n_channels # Number of Channels in Image\n",
    "        \n",
    "        self.linear_projection = nn.Conv2d(in_channels=n_channels, out_channels=d_model, kernel_size=patch_size, stride=patch_size) # Linear Projection Layer\n",
    "        \n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.linear_projection(x) # (B, C, H, W) -> (B, d_model, H', W')\n",
    "        x = self.flatten(x) # (B, d_model, H', W') -> (B, d_model, n_patches)\n",
    "        x = x.transpose(1, 2) # (B, d_model, n_patches) -> (B, n_patches, d_model)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length): \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.cls_tokens = nn.Parameter(torch.randn(1, 1, d_model)) # Classification Token\n",
    "        \n",
    "        # Creating Positional Encoding \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x): \n",
    "        # Expand to have class token for each image in batch \n",
    "        tokens_batch = self.cls_tokens.expand(x.shape[0], -1, -1) # (B, 1, d_model)\n",
    "        \n",
    "        # Concatenate class token with positional encoding\n",
    "        x = torch.cat((tokens_batch, x), dim=1)\n",
    "        \n",
    "        # Add positional encoding to the input \n",
    "        x = x + self.pe\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # dimension of each head\n",
    "        \n",
    "        print(\"d_model: \", d_model)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)        \n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output, attn_probs\n",
    "    \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model) \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        q = self.split_head(self.W_q(x)) # (B, num_heads, seq_length, d_k)\n",
    "        print(\"\\nmultattn: q: \", q.shape)\n",
    "        k = self.split_head(self.W_k(x))\n",
    "        v = self.split_head(self.W_v(x))\n",
    "        \n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask) # (B, num_heads, seq_length, d_k)\n",
    "        print(\"multattn: attn_output: \", attn_output.shape)\n",
    "        output = self.W_o(self.combine_heads(attn_output)) # (B, seq_length, d_model)\n",
    "        print(\"multattn: output: \", output.shape)\n",
    "        return output\n",
    "    \n",
    "class TransformerEncoder(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, r_mlp=4):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.r_mlp = r_mlp        \n",
    "        \n",
    "        # self.attention = MultiHeadConvNN(d_model, num_heads)\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Multilayer Perceptron \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * r_mlp, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Multi-Head Attention\n",
    "        print(\"In transformerencoder: before attention: \", x.shape)\n",
    "        \n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed Forward Network \n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class VisionTransformer(nn.Module): \n",
    "    def __init__(self, d_model, img_size, n_classes, n_heads, patch_size, n_channels, n_layers): \n",
    "        super(VisionTransformer, self).__init__()\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model # Dimensionality of Model \n",
    "        self.img_size = img_size # Image size \n",
    "        self.n_classes = n_classes # Number of Classes \n",
    "        self.n_heads = n_heads # Number of Heads\n",
    "        self.patch_size = patch_size # Patch Size\n",
    "        self.n_channels = n_channels # Number of Channels\n",
    "        self.n_layers = n_layers # Number of Layers\n",
    "        \n",
    "        self.option = 0 # Option for Transformer Encoder\n",
    "                             # Attention,\n",
    "                             # Conv2d, \n",
    "                             # ConvNN\n",
    "                             # ConvNN_Attn\n",
    "                             # Conv2d + ConvNN \n",
    "                             # Attention + ConvNN\n",
    "        \n",
    "        self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Patch Embedding Layer \n",
    "        self.patch_embedding = PatchEmbedding(d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, self.max_seq_length)\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoder(d_model, n_heads) for _ in range(n_layers)]) # Stacking Transformer Encoder Layers\n",
    "        \n",
    "        # Classification MLP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.n_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        print(\"after patch embedding: \", x.shape)\n",
    "        x = self.positional_encoding(x)\n",
    "        print(\"after positional encoding: \", x.shape)\n",
    "        x = self.transformer_encoder(x)\n",
    "        print(\"after transformer encoder: \", x.shape)\n",
    "        x = self.classifier(x[:, 0]) # Taking the CLS token for classification\n",
    "        print(\"after classifier: \", x.shape)\n",
    "        return x       \n",
    "    \n",
    "    # after patch embedding:  torch.Size([128, 4, 9])\n",
    "    # after positional encoding:  torch.Size([128, 5, 9])\n",
    "    # after transformer encoder:  torch.Size([128, 5, 9])\n",
    "    # after classifier:  torch.Size([128, 10])\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 6\n",
    "    n_classes = 10\n",
    "    img_size = (224,224)\n",
    "    patch_size = (16,16)\n",
    "    n_channels = 3\n",
    "    n_heads = 3\n",
    "    n_layers = 1\n",
    "    batch_size = 128\n",
    "    epochs = 5\n",
    "    alpha = 0.005\n",
    "    \n",
    "    \n",
    "    # Training the Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "    transformer = VisionTransformer( \n",
    "                    d_model, \n",
    "                    img_size, \n",
    "                    n_classes, \n",
    "                    n_heads, \n",
    "                    patch_size, \n",
    "                    n_channels, \n",
    "                    n_layers\n",
    "                ).to(device)\n",
    "\n",
    "    ex = torch.randn(batch_size, n_channels, img_size[0], img_size[1]).to(device)\n",
    "    print(\"Input shape: \", ex.shape)\n",
    "    print(\"Output shape: \", transformer(ex).shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a54955",
   "metadata": {},
   "source": [
    "### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4766e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([128, 3, 197])\n",
      "Output shape:  torch.Size([128, 3, 197])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 6\n",
    "    n_classes = 10\n",
    "    img_size = (224,224)\n",
    "    patch_size = (16,16)\n",
    "    n_channels = 3\n",
    "    n_heads = 3\n",
    "    n_layers = 1\n",
    "    batch_size = 128\n",
    "    epochs = 5\n",
    "    alpha = 0.005\n",
    "    \n",
    "    \n",
    "    # Training the Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "    transformer = VisionTransformer( \n",
    "                    d_model, \n",
    "                    img_size, \n",
    "                    n_classes, \n",
    "                    n_heads, \n",
    "                    patch_size, \n",
    "                    n_channels, \n",
    "                    n_layers\n",
    "                ).to(device)\n",
    "\n",
    "    ex = torch.randn(batch_size, n_channels, img_size[0], img_size[1]).to(device)\n",
    "    print(\"Input shape: \", ex.shape)\n",
    "    print(\"Output shape: \", transformer(ex).shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff4bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
