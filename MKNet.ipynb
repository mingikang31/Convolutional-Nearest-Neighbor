{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f769eab5-1ff0-45d7-9379-628bfcfefc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ImageNet, CIFAR10, CIFAR100\n",
    "from train_eval import Train_Eval\n",
    "from types import SimpleNamespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b07a11-c8f5-41a4-a50c-e0d845ed5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2d_NN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 K, \n",
    "                 stride, \n",
    "                 padding, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding,\n",
    "                 shuffle_pattern, \n",
    "                 shuffle_scale, \n",
    "                 magnitude_type, \n",
    "                 similarity_type, \n",
    "                 aggregation_type, \n",
    "                 lambda_param\n",
    "                ):\n",
    "\n",
    "        super(Conv2d_NN, self).__init__()\n",
    "\n",
    "        assert K == stride, \"K must be equal to stride for ConvNN\"\n",
    "        assert padding > 0 or padding == 0, \"Cannot have Negative Padding\"\n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Shuffle pattern must be: Before, After, Before After, Not Applicable\"\n",
    "        assert magnitude_type in [\"cosine\", \"euclidean\"], \"Similarity Matrix must be either cosine similarity or euclidean distance\"\n",
    "        assert sampling_type in [\"all\", \"random\", \"spatial\"], \"Consider all neighbors, random neighbors, or spatial neighbors\"\n",
    "        assert int(num_samples) > 0 or int(num_samples) == -1, \"Number of samples to consider must be greater than 0 or -1 for all samples\"\n",
    "        assert (sampling_type == \"all\" and int(num_samples) == -1) or (sampling_type != \"all\" and isinstance(num_samples, int)), \"Number of samples must be -1 for all samples or integer for random and spatial sampling\"\n",
    "\n",
    "        assert similarity_type in [\"Loc\", \"Col\", \"Loc_Col\"], \"Similarity Matrix based on Location, Color, or both\"\n",
    "        assert aggregation_type in [\"Col\", \"Loc_Col\"], \"Aggregation based on Color or Location and Color\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.in_channels = in_channels \n",
    "        self.out_channels = out_channels \n",
    "        self.K = K\n",
    "        self.stride = stride \n",
    "        self.padding = padding \n",
    "\n",
    "        # 3 Sampling Types: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples)\n",
    "        self.sample_padding = int(sample_padding) if sampling_type == \"spatial\" else 0\n",
    "\n",
    "        # Pixel Shuffling (optional) \n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "\n",
    "        # Similarity Metric\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if magnitude_type == \"cosine\" else False\n",
    "\n",
    "        # Similarity and Aggregation Types\n",
    "        self.similarity_type = similarity_type\n",
    "        self.aggregation_type = aggregation_type\n",
    "        \n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = True if (similarity_type in [\"Loc\", \"Loc_Col\"] or aggregation_type == \"Loc_Col\") else False\n",
    "        self.coordinate_cache = {}\n",
    "\n",
    "        # Pixel Shuffle Adjustments\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale) \n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "\n",
    "        self.in_channels_1d = self.in_channels * (self.shuffle_scale ** 2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_1d = self.out_channels * (self.shuffle_scale ** 2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        self.in_channels_1d = self.in_channels_1d + 2 if self.aggregation_type == \"Loc_Col\" else self.in_channels_1d\n",
    "\n",
    "        # Conv1d Layer\n",
    "        self.conv1d_layer = nn.Conv1d(\n",
    "            in_channels = self.in_channels_1d,\n",
    "            out_channels = self.out_channels_1d,\n",
    "            kernel_size = self.K, \n",
    "            stride = self.stride, \n",
    "            padding = 0, \n",
    "            # bias = False # Only if similarity_type is \"Loc\" (make ConvNN exactly same as Conv2d)\n",
    "        )\n",
    "\n",
    "        # Flatten * Unflatten layers \n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        self.unflatten = None\n",
    "\n",
    "        # Shapes\n",
    "        self.og_shape = None \n",
    "        self.padded_shape = None\n",
    "\n",
    "        # Utility Variables\n",
    "        self.INF = 1e5\n",
    "        self.NEG_INF = -1e5\n",
    "\n",
    "        self.lambda_param = lambda_param\n",
    "        # self.lambda_param = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):  \n",
    "        # 1. Pixel Unshuffle Layer\n",
    "        x = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "        self.og_shape = x.shape\n",
    "\n",
    "        # 2. Add Padding \n",
    "        if self.padding > 0:\n",
    "            x = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
    "            self.padded_shape = x.shape\n",
    "\n",
    "        # 3. Add Coordinate Encoding\n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "\n",
    "        # 4. Flatten Layer\n",
    "        x = self.flatten(x) \n",
    "\n",
    "        # 5. Similarity and Aggregation Type \n",
    "        if self.similarity_type == \"Loc\":\n",
    "            x_sim = x[:, -2:, :]\n",
    "        elif self.similarity_type == \"Loc_Col\":\n",
    "            x_sim = x\n",
    "        elif self.similarity_type == \"Col\" and self.aggregation_type == \"Col\":\n",
    "            x_sim = x\n",
    "        elif self.similarity_type == \"Col\" and self.aggregation_type == \"Loc_Col\":\n",
    "            x_sim = x[:, :-2, :]\n",
    "\n",
    "        if self.similarity_type in [\"Loc\", \"Loc_Col\"] and self.aggregation_type == \"Col\":\n",
    "            x = x[:, :-2, :]\n",
    "        else: \n",
    "            x = x\n",
    "\n",
    "        if self.similarity_type == \"Loc_Col\":      \n",
    "            # Normalize each modality to unit variance before combining\n",
    "            color_feats = x_sim[:, :-2, :]\n",
    "            color_std = torch.std(color_feats, dim=[1,2], keepdim=True) + 1e-6\n",
    "            color_norm = color_feats / color_std\n",
    "\n",
    "            coord_feats = x_sim[:, -2:, :]  # Already in [-1,1]\n",
    "            x_sim = torch.cat([self.lambda_param * color_norm, \n",
    "                            (1-self.lambda_param) * coord_feats], dim=1)\n",
    "            \n",
    "        # 6. Sampling + Similarity Calculation + Aggregation\n",
    "        if self.sampling_type == \"all\":\n",
    "            similarity_matrix = self._calculate_euclidean_matrix(x_sim) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix(x_sim)\n",
    "            prime = self._prime(x, similarity_matrix, self.K, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type == \"random\":\n",
    "            if self.num_samples > x.shape[-1]:\n",
    "                x_sample = x_sim\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "                torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(-0.1 if self.magnitude_type == \"euclidean\" else 1.1)\n",
    "                prime = self._prime(x, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "            else:\n",
    "                rand_idx = torch.randperm(x.shape[-1], device=x.device)[:self.num_samples]\n",
    "                x_sample = x_sim[:, :, rand_idx]\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "                range_idx = torch.arange(len(rand_idx), device=x.device)\n",
    "                similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == \"euclidean\" else self.NEG_INF\n",
    "                prime = self._prime_N(x, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "            \n",
    "\n",
    "        elif self.sampling_type == \"spatial\":\n",
    "            if self.num_samples > self.og_shape[-2]:\n",
    "                x_sample = x_sim\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "                torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(-0.1 if self.magnitude_type == \"euclidean\" else 1.1)\n",
    "                prime = self._prime(x, similarity_matrix, self.K, self.maximum)\n",
    "            else:\n",
    "                x_ind = torch.linspace(0 + self.sample_padding, self.og_shape[-2] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "                y_ind = torch.linspace(0 + self.sample_padding, self.og_shape[-1] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "                x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "                x_idx_flat, y_idx_flat = x_grid.flatten(), y_grid.flatten()\n",
    "                width = self.og_shape[-2]\n",
    "                flat_indices = y_idx_flat * width + x_idx_flat\n",
    "                x_sample = x_sim[:, :, flat_indices]\n",
    "\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "\n",
    "                range_idx = torch.arange(len(flat_indices), device=x.device)    \n",
    "                similarity_matrix[:, flat_indices, range_idx] = self.INF if self.magnitude_type == \"euclidean\" else self.NEG_INF\n",
    "                \n",
    "                prime = self._prime_N(x, similarity_matrix, self.K, flat_indices, self.maximum)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Sampling Type not Implemented\")\n",
    "        \n",
    "        # 7. Conv1d Layer\n",
    "        x = self.conv1d_layer(prime)\n",
    "\n",
    "        # 8. Unflatten Layer\n",
    "        if not self.unflatten: \n",
    "            self.unflatten = nn.Unflatten(dim=2, unflattened_size=self.og_shape[2:])\n",
    "        x = self.unflatten(x)\n",
    "\n",
    "        # 9. Pixel Shuffle Layer\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x \n",
    "        return x \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, matrix, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.matmul(matrix.transpose(1, 2), matrix)\n",
    "\n",
    "        dist_matrix = norm_squared.transpose(1, 2) + norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1)\n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_euclidean_matrix_N(self, matrix, matrix_sample, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        norm_squared_sample = torch.sum(matrix_sample ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.matmul(matrix.transpose(1, 2), matrix_sample)\n",
    "        \n",
    "        dist_matrix = norm_squared.transpose(1, 2) + norm_squared_sample - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0) \n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_cosine_matrix(self, matrix):\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_matrix.transpose(1, 2), norm_matrix)\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_cosine_matrix_N(self, matrix, matrix_sample):\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_matrix.transpose(1, 2), norm_sample)\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "\n",
    "        if self.similarity_type == \"Loc\":\n",
    "            topk_values, topk_indices = torch.sort(magnitude_matrix, dim=2, descending=maximum, stable=True)\n",
    "            topk_indices = topk_indices[:, :, :K]\n",
    "            topk_indices, _ = torch.sort(topk_indices, dim=-1)\n",
    "        else:\n",
    "            topk_values, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "\n",
    "        if self.padding > 0: \n",
    "            prime = prime.view(b, c, self.padded_shape[-2], self.padded_shape[-1], K)\n",
    "            prime = prime[:, :, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "            prime = prime.reshape(b, c, K * self.og_shape[-2] * self.og_shape[-1])\n",
    "        else: \n",
    "            prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "        \n",
    "    def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        \n",
    "        topk_values, topk_indices = torch.topk(magnitude_matrix, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indices back to original matrix positions\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        if self.similarity_type == \"Loc\":\n",
    "            final_indices, _ = torch.sort(final_indices, dim=-1)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "\n",
    "        if self.padding > 0:\n",
    "            prime = prime.view(b, c, self.padded_shape[-2], self.padded_shape[-1], K)\n",
    "            prime = prime[:, :, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "            prime = prime.reshape(b, c, K * self.og_shape[-2] * self.og_shape[-1])\n",
    "        else:\n",
    "            prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "\n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords ### Last two channels are coordinate channels \n",
    "\n",
    "\n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else (d_hidden // num_heads)\n",
    "        self.out_channels = (d_hidden // num_heads) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1.1\n",
    "        self.NEG_INF = -0.1 \n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.batch_combine(self.split_head(self.W_k(x)))\n",
    "        v = self.batch_combine(self.split_head(self.W_v(x)))\n",
    "        \n",
    "\n",
    "        # 2. Add Coordinate Encoding \n",
    "        k = self._add_coordinate_encoding(k) if self.coordinate_encoding else k\n",
    "        v = self._add_coordinate_encoding(v) if self.coordinate_encoding else v\n",
    "\n",
    "\n",
    "        # 3. Sampling & Similarity Calculation\n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "            \n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix(k, q, sqrt=True)\n",
    "\n",
    "            \n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'random': # Random Samples\n",
    "            rand_idx = torch.randperm(x.shape[1], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, rand_idx, :]            \n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(rand_idx), device=q.device)\n",
    "            similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'spatial': # Spatial Samples\n",
    "            spat_idx = torch.linspace(0 + self.sample_padding, x.shape[1] - self.sample_padding - 1, self.num_samples, device=x.device).long()\n",
    "            x_sample = x[:, spat_idx, :]\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(spat_idx), device=q.device)\n",
    "            similarity_matrix[:, spat_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, spat_idx, self.maximum)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial']\")\n",
    "\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        # 6. Final Linear Projection\n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x)))\n",
    "        return x       \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1) \n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        topk_values = torch.softmax(topk_values, dim=-1)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "    \n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime \n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indicies back to original matrix positions \n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=-1)\n",
    "        topk_indices_exp = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Expand topk values to match the shape of indices\n",
    "        topk_values = torch.softmax(topk_values, dim=-1)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        zeros = torch.zeros((b, c, t, 1), device=v.device)\n",
    "        topk_values_exp = torch.cat((zeros, topk_values_exp), dim=-1)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting \n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()    \n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, c, t = x.shape \n",
    "        cache_key = f\"{b}_{t}_{x.device}\"\n",
    "        if cache_key in self.coordinate_cache: \n",
    "            expanded_coords = self.coordinate_cache[cache_key]\n",
    "        else: \n",
    "            coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1) \n",
    "            expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1) \n",
    "            self.coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "        x_with_coords = torch.cat([x, expanded_coords], dim=1) \n",
    "        return x_with_coords \n",
    "\n",
    "class MKNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_hidden = 128, \n",
    "                d_ffn = 512,\n",
    "                num_layers = 8, \n",
    "                dropout = 0.1, \n",
    "                convnn_args = {\n",
    "                    'K': 9,\n",
    "                    'stride': 9,\n",
    "                    'padding': 1,\n",
    "                    'sampling_type': 'all',\n",
    "                    'num_samples': -1,\n",
    "                    'sample_padding': 0,\n",
    "                    'shuffle_pattern': \"NA\", \n",
    "                    'shuffle_scale': 0,\n",
    "                    'magnitude_type': \"cosine\",\n",
    "                    'similarity_type': \"Col\",\n",
    "                    'aggregation_type': \"Col\",\n",
    "                    'lambda_param': 0.5\n",
    "                }, \n",
    "                convnn_attn_args = {\n",
    "                    \"num_heads\": 1,\n",
    "                    \"attention_dropout\": 0.1, \n",
    "                    \"K\": 9,\n",
    "                    \"sampling_type\": \"all\",\n",
    "                    \"num_samples\": -1,\n",
    "                    \"sample_padding\": 0,\n",
    "                    \"magnitude_type\": \"cosine\",\n",
    "                    \"seq_length\": 196, \n",
    "                    \"coordinate_encoding\": False\n",
    "                }\n",
    "                 ):\n",
    "\n",
    "        super(MKNet, self).__init__()\n",
    "        # Implementation of MKNet architecture goes here\n",
    "        \"\"\"\n",
    "        MKNet architecture implementation\n",
    "        \"\"\"\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.convnn_args = convnn_args\n",
    "        self.convnn_attn_args = convnn_attn_args\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            d_hidden=d_hidden, \n",
    "            img_size=224, \n",
    "            patch_size=16, \n",
    "            n_channels=3\n",
    "            )\n",
    "\n",
    "        self.cross_layers = nn.Sequential(*[MKCrossBlock(\n",
    "            d_hidden=self.d_hidden, \n",
    "            dropout = self.dropout, \n",
    "            convnn_args=self.convnn_args,\n",
    "            convnn_attn_args=self.convnn_attn_args\n",
    "            ) for _ in range(self.num_layers)])\n",
    "\n",
    "        self.ffn_layers = MKFFN(\n",
    "            d_hidden=self.d_hidden, \n",
    "            d_ffn=d_ffn, \n",
    "            dropout=self.dropout\n",
    "            )\n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]      \n",
    "        x = self.patch_embedding(x)  # [B, d_hidden, H/patch_size, W/patch_size]\n",
    "        x = self.cross_layers(x)     # [B, d_hidden, H/patch_size, W/patch_size]\n",
    "        x = self.ffn_layers(x)       # [B, d_hidden, H/patch_size, W/patch_size]\n",
    "        return x\n",
    "\n",
    "    def parameter_count(self): \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params\n",
    "\n",
    "\n",
    "class MKLocalBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_hidden, \n",
    "                convnn_args):\n",
    "        super(MKLocalBlock, self).__init__()\n",
    "        # Implementation of MKNet block goes here\n",
    "        \"\"\"\n",
    "        MKNet block implementation\n",
    "        \"\"\"\n",
    "        self.d_hidden = d_hidden\n",
    "        self.convnn = Conv2d_NN(\n",
    "            in_channels=d_hidden,\n",
    "            out_channels=d_hidden,\n",
    "            K=convnn_args['K'],\n",
    "            stride=convnn_args['stride'],\n",
    "            padding=convnn_args['padding'],\n",
    "            sampling_type=convnn_args['sampling_type'],\n",
    "            num_samples=convnn_args['num_samples'],\n",
    "            sample_padding=convnn_args['sample_padding'],\n",
    "            shuffle_pattern=convnn_args['shuffle_pattern'],\n",
    "            shuffle_scale=convnn_args['shuffle_scale'],\n",
    "            magnitude_type=convnn_args['magnitude_type'],\n",
    "            similarity_type=convnn_args['similarity_type'],\n",
    "            aggregation_type=convnn_args['aggregation_type'],\n",
    "            lambda_param=convnn_args['lambda_param']\n",
    "        )\n",
    "        self.conv = nn.Conv2d(d_hidden, d_hidden, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]\n",
    "        x = self.convnn(x)\n",
    "        # x = self.conv(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "class MKGlobalBlock(nn.Module):\n",
    "    def __init__(self, d_hidden, convnn_attn_args):\n",
    "        super(MKGlobalBlock, self).__init__()\n",
    "        # Implementation of MKNet global block goes here\n",
    "        \"\"\"\n",
    "        MKNet global block implementation\n",
    "        \"\"\"\n",
    "        self.convnn_attn = MultiHeadConvNNAttention(\n",
    "            d_hidden=d_hidden,\n",
    "            num_heads=convnn_attn_args[\"num_heads\"],\n",
    "            attention_dropout=convnn_attn_args[\"attention_dropout\"],\n",
    "            K=convnn_attn_args[\"K\"],\n",
    "            sampling_type=convnn_attn_args[\"sampling_type\"],\n",
    "            num_samples=convnn_attn_args[\"num_samples\"],\n",
    "            sample_padding=convnn_attn_args[\"sample_padding\"],\n",
    "            magnitude_type=convnn_attn_args[\"magnitude_type\"],\n",
    "            seq_length=convnn_attn_args[\"seq_length\"],\n",
    "            coordinate_encoding=convnn_attn_args[\"coordinate_encoding\"]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_hidden)\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]\n",
    "        B, C, H, W = x.size()\n",
    "        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)  # [B, H*W, C]\n",
    "        x_attn = self.convnn_attn(x_reshaped)  # [B, H*W, C]\n",
    "        x_attn = self.norm(x_attn)\n",
    "        x_attn = self.dropout(x_attn)\n",
    "        x = x + x_attn.view(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "        return x\n",
    "\n",
    "class MKCrossBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 dropout, \n",
    "                 convnn_args, \n",
    "                 convnn_attn_args\n",
    "                 ):\n",
    "        super(MKCrossBlock, self).__init__()\n",
    "        # Implementation of MKNet cross block goes here\n",
    "        \"\"\"\n",
    "        MKNet cross block implementation\n",
    "        \"\"\"\n",
    "\n",
    "        self.local_block = MKLocalBlock(d_hidden=d_hidden, convnn_args=convnn_args)\n",
    "        self.global_block = MKGlobalBlock(d_hidden=d_hidden, convnn_attn_args=convnn_attn_args)\n",
    "\n",
    "        self.local_conv = nn.Conv2d(in_channels=d_hidden, \n",
    "                                    out_channels=d_hidden, \n",
    "                                    kernel_size=1,\n",
    "                                    stride=1,\n",
    "                                    padding=0, \n",
    "                                    bias=False)\n",
    "\n",
    "        self.global_conv = nn.Conv2d(in_channels=d_hidden, \n",
    "                                     out_channels=d_hidden, \n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding=0, \n",
    "                                     bias=False)\n",
    "\n",
    "        self.combine_conv = nn.Conv2d(in_channels=d_hidden, \n",
    "                                      out_channels=d_hidden, \n",
    "                                      kernel_size=1,\n",
    "                                      stride=1,\n",
    "                                      padding=0, \n",
    "                                      bias=False)\n",
    "\n",
    "        self.dropout_local = nn.Dropout(dropout)\n",
    "        self.dropout_global = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm_local = nn.LayerNorm(d_hidden)\n",
    "        self.norm_global = nn.LayerNorm(d_hidden)\n",
    "        self.norm_combine = nn.LayerNorm(d_hidden)\n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]\n",
    "        identity = x \n",
    "        \n",
    "        # Local Branch\n",
    "        x_local = self.norm_local(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) \n",
    "        x_local = self.local_block(x)\n",
    "        x_local = x + self.dropout_local(x_local)\n",
    "        x_local = self.local_conv(x_local)\n",
    "\n",
    "        # Global Branch\n",
    "        x_global = self.norm_global(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) \n",
    "        x_global = self.global_block(x)\n",
    "        x_global = x + self.dropout_global(x_global)\n",
    "        x_global = self.global_conv(x_global)\n",
    "\n",
    "        # Combine Local and Global\n",
    "        x_combine = x_local + x_global + identity\n",
    "        x_combine = self.norm_combine(x_combine.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        x_combine = self.combine_conv(x_combine)\n",
    "\n",
    "        x = x_combine\n",
    "        return x\n",
    "\n",
    "class MKFFN(nn.Module):\n",
    "    def __init__(self, d_hidden, d_ffn, num_classes=100, dropout=0.1):\n",
    "        super(MKFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_hidden, d_ffn)\n",
    "        self.fc2 = nn.Linear(d_ffn, d_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        # Use adaptive pooling instead of flattening all spatial dimensions\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(d_hidden, num_classes) \n",
    "\n",
    "    def forward(self, x):  # [B, C, H, W]\n",
    "        # Apply FFN to spatial features\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).contiguous().view(B, H*W, C)  # [B, H*W, C]\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.pool(x)  # [B, C, 1, 1]\n",
    "        x = x.flatten(1)  # [B, C]\n",
    "        x = self.output_layer(x)  # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_hidden, img_size, patch_size, n_channels=3):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(n_channels, d_hidden, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(d_hidden)\n",
    "        \n",
    "        # Add learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, d_hidden, \n",
    "                                                   img_size // patch_size, \n",
    "                                                   img_size // patch_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, d_hidden, H/P, W/P]\n",
    "        x = x + self.pos_embed  # Add positional encoding\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H/P, W/P, d_hidden]\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, d_hidden, H/P, W/P]\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066a7965-762e-467b-899d-2380e6677770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "STAGE:2025-10-25 21:14:17 670132:670132 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-10-25 21:14:18 670132:670132 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-10-25 21:14:18 670132:670132 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Trainable Parameters: 4.74019600 M\n",
      "Model Complexity (Profiler):\n",
      "   - GFLOPs: 0.66533120\n",
      "   - Trainable Parameters: 4.74019600 M\n",
      "[Epoch 001] Time: 55.5390s | [Train] Loss: 4.28792878 Accuracy: Top1: 4.0585%, Top5: 16.0179% | [Test] Loss: 4.10508718 Accuracy: Top1: 6.9267%, Top5: 23.0480%\n",
      "[Epoch 002] Time: 57.3784s | [Train] Loss: 3.96277302 Accuracy: Top1: 8.3790%, Top5: 27.1630% | [Test] Loss: 3.87655861 Accuracy: Top1: 10.0236%, Top5: 30.0408%\n",
      "[Epoch 003] Time: 55.6566s | [Train] Loss: 3.74826213 Accuracy: Top1: 11.7288%, Top5: 34.2549% | [Test] Loss: 3.66287577 Accuracy: Top1: 13.6259%, Top5: 37.7625%\n",
      "[Epoch 004] Time: 57.6250s | [Train] Loss: 3.57093498 Accuracy: Top1: 14.4844%, Top5: 39.4177% | [Test] Loss: 3.50172971 Accuracy: Top1: 16.0811%, Top5: 41.9784%\n",
      "[Epoch 005] Time: 57.7203s | [Train] Loss: 3.39235998 Accuracy: Top1: 17.8017%, Top5: 44.5895% | [Test] Loss: 3.34094841 Accuracy: Top1: 19.2900%, Top5: 47.1783%\n",
      "[Epoch 006] Time: 57.4128s | [Train] Loss: 3.27581040 Accuracy: Top1: 19.9122%, Top5: 48.0121% | [Test] Loss: 3.33597742 Accuracy: Top1: 19.6306%, Top5: 47.1392%\n",
      "[Epoch 007] Time: 57.6696s | [Train] Loss: 3.18429915 Accuracy: Top1: 21.7766%, Top5: 50.2390% | [Test] Loss: 3.16390285 Accuracy: Top1: 23.1761%, Top5: 50.8824%\n",
      "[Epoch 008] Time: 57.0585s | [Train] Loss: 3.10155040 Accuracy: Top1: 23.5235%, Top5: 52.5171% | [Test] Loss: 3.11550610 Accuracy: Top1: 23.7058%, Top5: 52.4121%\n",
      "[Epoch 009] Time: 56.4261s | [Train] Loss: 3.01976527 Accuracy: Top1: 24.6530%, Top5: 54.8960% | [Test] Loss: 3.07690848 Accuracy: Top1: 24.6433%, Top5: 53.8408%\n",
      "[Epoch 010] Time: 58.7132s | [Train] Loss: 2.95963067 Accuracy: Top1: 25.9938%, Top5: 56.3768% | [Test] Loss: 2.95975114 Accuracy: Top1: 26.1489%, Top5: 56.7406%\n",
      "[Epoch 011] Time: 59.8374s | [Train] Loss: 2.88536229 Accuracy: Top1: 27.3346%, Top5: 58.0797% | [Test] Loss: 2.94259586 Accuracy: Top1: 27.1082%, Top5: 57.4207%\n",
      "[Epoch 012] Time: 57.7577s | [Train] Loss: 2.82511613 Accuracy: Top1: 28.8535%, Top5: 59.6999% | [Test] Loss: 2.90211657 Accuracy: Top1: 27.5540%, Top5: 58.4496%\n",
      "[Epoch 013] Time: 57.1629s | [Train] Loss: 2.76252384 Accuracy: Top1: 29.9188%, Top5: 61.1361% | [Test] Loss: 2.80199393 Accuracy: Top1: 29.6881%, Top5: 60.6020%\n",
      "[Epoch 014] Time: 58.4389s | [Train] Loss: 2.70501624 Accuracy: Top1: 31.0845%, Top5: 62.3307% | [Test] Loss: 2.83490698 Accuracy: Top1: 29.0407%, Top5: 59.3555%\n",
      "[Epoch 015] Time: 57.2527s | [Train] Loss: 2.65885917 Accuracy: Top1: 32.1669%, Top5: 63.3921% | [Test] Loss: 2.73759054 Accuracy: Top1: 31.2931%, Top5: 61.5108%\n",
      "[Epoch 016] Time: 57.8038s | [Train] Loss: 2.61806466 Accuracy: Top1: 32.8656%, Top5: 64.4616% | [Test] Loss: 2.68821526 Accuracy: Top1: 31.7894%, Top5: 62.9423%\n",
      "[Epoch 017] Time: 56.7298s | [Train] Loss: 2.56542341 Accuracy: Top1: 33.7508%, Top5: 65.5870% | [Test] Loss: 2.67637169 Accuracy: Top1: 32.5379%, Top5: 63.2945%\n",
      "[Epoch 018] Time: 57.9779s | [Train] Loss: 2.52738652 Accuracy: Top1: 35.0489%, Top5: 66.2772% | [Test] Loss: 2.62935780 Accuracy: Top1: 33.2313%, Top5: 64.2917%\n",
      "[Epoch 019] Time: 57.4775s | [Train] Loss: 2.48857185 Accuracy: Top1: 35.7779%, Top5: 67.1754% | [Test] Loss: 2.72470396 Accuracy: Top1: 32.4247%, Top5: 61.6113%\n",
      "[Epoch 020] Time: 56.7469s | [Train] Loss: 2.45271900 Accuracy: Top1: 36.4450%, Top5: 68.1156% | [Test] Loss: 2.66001781 Accuracy: Top1: 33.1951%, Top5: 63.5903%\n",
      "[Epoch 021] Time: 58.2605s | [Train] Loss: 2.42758824 Accuracy: Top1: 36.8212%, Top5: 68.6310% | [Test] Loss: 2.56947232 Accuracy: Top1: 34.5841%, Top5: 65.9094%\n",
      "[Epoch 022] Time: 56.9992s | [Train] Loss: 2.39621287 Accuracy: Top1: 37.6227%, Top5: 69.1492% | [Test] Loss: 2.56869555 Accuracy: Top1: 34.7978%, Top5: 65.7376%\n",
      "[Epoch 023] Time: 56.2466s | [Train] Loss: 2.34726571 Accuracy: Top1: 38.4079%, Top5: 70.1887% | [Test] Loss: 2.52308130 Accuracy: Top1: 36.4258%, Top5: 66.7733%\n",
      "[Epoch 024] Time: 58.0742s | [Train] Loss: 2.31909179 Accuracy: Top1: 39.1787%, Top5: 70.8878% | [Test] Loss: 2.51215093 Accuracy: Top1: 35.9013%, Top5: 67.0881%\n",
      "[Epoch 025] Time: 55.9295s | [Train] Loss: 2.28915425 Accuracy: Top1: 39.9270%, Top5: 71.5180% | [Test] Loss: 2.50117522 Accuracy: Top1: 36.2264%, Top5: 67.1140%\n",
      "[Epoch 026] Time: 57.9166s | [Train] Loss: 2.24661817 Accuracy: Top1: 40.7111%, Top5: 72.2763% | [Test] Loss: 2.43465848 Accuracy: Top1: 37.8223%, Top5: 68.5731%\n",
      "[Epoch 027] Time: 58.0434s | [Train] Loss: 2.22378163 Accuracy: Top1: 41.3951%, Top5: 72.6730% | [Test] Loss: 2.46121552 Accuracy: Top1: 37.3047%, Top5: 68.3364%\n",
      "[Epoch 028] Time: 56.9227s | [Train] Loss: 2.20471538 Accuracy: Top1: 41.4838%, Top5: 73.2681% | [Test] Loss: 2.44786193 Accuracy: Top1: 38.2008%, Top5: 68.2497%\n",
      "[Epoch 029] Time: 55.6815s | [Train] Loss: 2.17708461 Accuracy: Top1: 42.1663%, Top5: 73.9268% | [Test] Loss: 2.46465839 Accuracy: Top1: 37.0979%, Top5: 68.6523%\n",
      "[Epoch 030] Time: 57.7699s | [Train] Loss: 2.15499335 Accuracy: Top1: 42.6405%, Top5: 74.3920% | [Test] Loss: 2.44695673 Accuracy: Top1: 37.6867%, Top5: 68.4829%\n",
      "[Epoch 031] Time: 58.1350s | [Train] Loss: 2.12152176 Accuracy: Top1: 43.4378%, Top5: 74.9223% | [Test] Loss: 2.41736335 Accuracy: Top1: 38.1129%, Top5: 69.2423%\n",
      "[Epoch 032] Time: 60.0172s | [Train] Loss: 2.09844252 Accuracy: Top1: 43.8903%, Top5: 75.4425% | [Test] Loss: 2.41309785 Accuracy: Top1: 38.4082%, Top5: 69.1360%\n",
      "[Epoch 033] Time: 58.2914s | [Train] Loss: 2.08402382 Accuracy: Top1: 44.3627%, Top5: 75.8836% | [Test] Loss: 2.42722747 Accuracy: Top1: 38.5524%, Top5: 68.8161%\n",
      "[Epoch 034] Time: 56.0179s | [Train] Loss: 2.04910201 Accuracy: Top1: 45.1201%, Top5: 76.2415% | [Test] Loss: 2.39432747 Accuracy: Top1: 38.5558%, Top5: 69.8024%\n",
      "[Epoch 035] Time: 56.5430s | [Train] Loss: 2.04147566 Accuracy: Top1: 44.9551%, Top5: 76.4781% | [Test] Loss: 2.36642522 Accuracy: Top1: 39.2641%, Top5: 70.3188%\n",
      "[Epoch 036] Time: 56.8151s | [Train] Loss: 1.99572980 Accuracy: Top1: 46.1936%, Top5: 77.2903% | [Test] Loss: 2.37033600 Accuracy: Top1: 39.1309%, Top5: 70.4234%\n",
      "[Epoch 037] Time: 56.8310s | [Train] Loss: 1.97536015 Accuracy: Top1: 46.9243%, Top5: 77.7069% | [Test] Loss: 2.38551043 Accuracy: Top1: 39.5215%, Top5: 70.1562%\n",
      "[Epoch 038] Time: 55.4684s | [Train] Loss: 1.95284065 Accuracy: Top1: 47.1062%, Top5: 78.0920% | [Test] Loss: 2.41529138 Accuracy: Top1: 38.7523%, Top5: 69.5249%\n",
      "[Epoch 039] Time: 57.7033s | [Train] Loss: 1.94288072 Accuracy: Top1: 47.3272%, Top5: 78.2640% | [Test] Loss: 2.34787563 Accuracy: Top1: 40.0281%, Top5: 71.1793%\n",
      "[Epoch 040] Time: 58.7699s | [Train] Loss: 1.90746447 Accuracy: Top1: 48.2156%, Top5: 78.8783% | [Test] Loss: 2.35638978 Accuracy: Top1: 39.9156%, Top5: 70.6239%\n",
      "[Epoch 041] Time: 57.4298s | [Train] Loss: 1.90467467 Accuracy: Top1: 48.1002%, Top5: 79.0993% | [Test] Loss: 2.35071599 Accuracy: Top1: 40.3171%, Top5: 71.5556%\n",
      "[Epoch 042] Time: 57.7215s | [Train] Loss: 1.88038397 Accuracy: Top1: 48.7567%, Top5: 79.3894% | [Test] Loss: 2.32752144 Accuracy: Top1: 40.6704%, Top5: 71.2270%\n",
      "[Epoch 043] Time: 56.4299s | [Train] Loss: 1.86142043 Accuracy: Top1: 49.0934%, Top5: 79.6512% | [Test] Loss: 2.36797072 Accuracy: Top1: 40.6796%, Top5: 70.6405%\n",
      "[Epoch 044] Time: 58.6100s | [Train] Loss: 1.84335701 Accuracy: Top1: 49.7473%, Top5: 80.1014% | [Test] Loss: 2.33981050 Accuracy: Top1: 40.6600%, Top5: 71.0696%\n",
      "[Epoch 045] Time: 57.7594s | [Train] Loss: 1.82100059 Accuracy: Top1: 50.2347%, Top5: 80.3806% | [Test] Loss: 2.34906573 Accuracy: Top1: 40.7991%, Top5: 71.3637%\n",
      "[Epoch 046] Time: 56.2431s | [Train] Loss: 1.79808955 Accuracy: Top1: 50.6243%, Top5: 80.7764% | [Test] Loss: 2.34938368 Accuracy: Top1: 40.9634%, Top5: 71.4062%\n",
      "[Epoch 047] Time: 59.0765s | [Train] Loss: 1.77956070 Accuracy: Top1: 50.9885%, Top5: 81.2374% | [Test] Loss: 2.35606592 Accuracy: Top1: 41.2345%, Top5: 71.7119%\n",
      "[Epoch 048] Time: 58.0752s | [Train] Loss: 1.76259957 Accuracy: Top1: 51.6249%, Top5: 81.4252% | [Test] Loss: 2.33084453 Accuracy: Top1: 41.1104%, Top5: 71.9830%\n",
      "[Epoch 049] Time: 57.1390s | [Train] Loss: 1.74287273 Accuracy: Top1: 51.8054%, Top5: 82.0361% | [Test] Loss: 2.33208910 Accuracy: Top1: 41.2144%, Top5: 71.7814%\n",
      "[Epoch 050] Time: 58.0505s | [Train] Loss: 1.72534614 Accuracy: Top1: 52.1443%, Top5: 82.1452% | [Test] Loss: 2.32368103 Accuracy: Top1: 41.8589%, Top5: 72.0209%\n",
      "[Epoch 051] Time: 58.2279s | [Train] Loss: 1.70005840 Accuracy: Top1: 52.8605%, Top5: 82.6714% | [Test] Loss: 2.33951664 Accuracy: Top1: 41.1977%, Top5: 72.0376%\n",
      "[Epoch 052] Time: 57.1054s | [Train] Loss: 1.69137125 Accuracy: Top1: 53.1445%, Top5: 82.7452% | [Test] Loss: 2.35406526 Accuracy: Top1: 41.1139%, Top5: 72.0043%\n",
      "[Epoch 053] Time: 55.6883s | [Train] Loss: 1.68343120 Accuracy: Top1: 53.3609%, Top5: 82.7775% | [Test] Loss: 2.34415504 Accuracy: Top1: 41.6814%, Top5: 71.5493%\n",
      "[Epoch 054] Time: 58.5044s | [Train] Loss: 1.65805192 Accuracy: Top1: 53.8578%, Top5: 83.2812% | [Test] Loss: 2.31276862 Accuracy: Top1: 41.9031%, Top5: 72.3162%\n",
      "[Epoch 055] Time: 58.0976s | [Train] Loss: 1.64383927 Accuracy: Top1: 54.3369%, Top5: 83.4507% | [Test] Loss: 2.33034416 Accuracy: Top1: 42.1892%, Top5: 72.1978%\n",
      "[Epoch 056] Time: 56.2786s | [Train] Loss: 1.62065384 Accuracy: Top1: 55.0158%, Top5: 83.8243% | [Test] Loss: 2.33660969 Accuracy: Top1: 41.9979%, Top5: 72.3053%\n",
      "[Epoch 057] Time: 56.1938s | [Train] Loss: 1.60772471 Accuracy: Top1: 55.3263%, Top5: 84.1369% | [Test] Loss: 2.34256420 Accuracy: Top1: 42.0795%, Top5: 71.9411%\n",
      "[Epoch 058] Time: 58.2537s | [Train] Loss: 1.58890591 Accuracy: Top1: 55.6711%, Top5: 84.3294% | [Test] Loss: 2.32294210 Accuracy: Top1: 43.0273%, Top5: 72.6430%\n",
      "[Epoch 059] Time: 57.6945s | [Train] Loss: 1.57711198 Accuracy: Top1: 55.9173%, Top5: 84.7931% | [Test] Loss: 2.32448308 Accuracy: Top1: 42.3156%, Top5: 72.4586%\n",
      "[Epoch 060] Time: 56.2453s | [Train] Loss: 1.56479321 Accuracy: Top1: 55.9088%, Top5: 85.0282% | [Test] Loss: 2.33373252 Accuracy: Top1: 42.2777%, Top5: 72.5517%\n",
      "[Epoch 061] Time: 56.4937s | [Train] Loss: 1.53841580 Accuracy: Top1: 56.7630%, Top5: 85.3128% | [Test] Loss: 2.34826204 Accuracy: Top1: 42.1398%, Top5: 72.2369%\n",
      "[Epoch 062] Time: 57.5405s | [Train] Loss: 1.53532514 Accuracy: Top1: 56.5326%, Top5: 85.3702% | [Test] Loss: 2.34562860 Accuracy: Top1: 42.5758%, Top5: 72.5138%\n",
      "[Epoch 063] Time: 57.5386s | [Train] Loss: 1.52030608 Accuracy: Top1: 57.0909%, Top5: 85.5558% | [Test] Loss: 2.34347897 Accuracy: Top1: 42.4546%, Top5: 72.4575%\n",
      "[Epoch 064] Time: 57.8332s | [Train] Loss: 1.49983391 Accuracy: Top1: 57.5426%, Top5: 85.8372% | [Test] Loss: 2.34438771 Accuracy: Top1: 42.0560%, Top5: 72.5867%\n",
      "[Epoch 065] Time: 57.0277s | [Train] Loss: 1.49172213 Accuracy: Top1: 57.9594%, Top5: 86.0250% | [Test] Loss: 2.36025128 Accuracy: Top1: 42.7022%, Top5: 72.8349%\n",
      "[Epoch 066] Time: 58.3220s | [Train] Loss: 1.47706562 Accuracy: Top1: 58.1988%, Top5: 86.2713% | [Test] Loss: 2.36854470 Accuracy: Top1: 42.5523%, Top5: 72.7459%\n",
      "[Epoch 067] Time: 56.8544s | [Train] Loss: 1.46603599 Accuracy: Top1: 58.6516%, Top5: 86.3709% | [Test] Loss: 2.37077547 Accuracy: Top1: 42.7269%, Top5: 72.9193%\n",
      "[Epoch 068] Time: 56.3885s | [Train] Loss: 1.45329146 Accuracy: Top1: 58.9018%, Top5: 86.5395% | [Test] Loss: 2.35426813 Accuracy: Top1: 42.4506%, Top5: 72.7568%\n",
      "[Epoch 069] Time: 58.6000s | [Train] Loss: 1.44715599 Accuracy: Top1: 58.8844%, Top5: 86.7282% | [Test] Loss: 2.34359636 Accuracy: Top1: 43.0635%, Top5: 72.7177%\n",
      "[Epoch 070] Time: 56.5831s | [Train] Loss: 1.42772666 Accuracy: Top1: 59.5135%, Top5: 87.0816% | [Test] Loss: 2.34854891 Accuracy: Top1: 42.7522%, Top5: 72.7418%\n",
      "[Epoch 071] Time: 58.2772s | [Train] Loss: 1.41614305 Accuracy: Top1: 59.7548%, Top5: 87.0575% | [Test] Loss: 2.36335002 Accuracy: Top1: 42.8217%, Top5: 73.0940%\n",
      "[Epoch 072] Time: 59.5493s | [Train] Loss: 1.40584826 Accuracy: Top1: 59.9831%, Top5: 87.2795% | [Test] Loss: 2.37841846 Accuracy: Top1: 42.7551%, Top5: 72.6764%\n",
      "[Epoch 073] Time: 57.8417s | [Train] Loss: 1.40228331 Accuracy: Top1: 60.2294%, Top5: 87.2246% | [Test] Loss: 2.37789025 Accuracy: Top1: 42.5276%, Top5: 72.8826%\n",
      "[Epoch 074] Time: 58.0458s | [Train] Loss: 1.38404650 Accuracy: Top1: 60.5976%, Top5: 87.6215% | [Test] Loss: 2.37542064 Accuracy: Top1: 42.6275%, Top5: 73.1330%\n",
      "[Epoch 075] Time: 54.2871s | [Train] Loss: 1.37347106 Accuracy: Top1: 61.0125%, Top5: 87.8864% | [Test] Loss: 2.38633573 Accuracy: Top1: 43.0808%, Top5: 73.2623%\n",
      "[Epoch 076] Time: 56.1442s | [Train] Loss: 1.36370605 Accuracy: Top1: 61.0006%, Top5: 87.8738% | [Test] Loss: 2.39345672 Accuracy: Top1: 42.7551%, Top5: 72.8912%\n",
      "[Epoch 077] Time: 57.9899s | [Train] Loss: 1.35251209 Accuracy: Top1: 61.3909%, Top5: 88.0173% | [Test] Loss: 2.40233563 Accuracy: Top1: 43.0090%, Top5: 72.8608%\n",
      "[Epoch 078] Time: 56.2568s | [Train] Loss: 1.34999568 Accuracy: Top1: 61.5619%, Top5: 88.0598% | [Test] Loss: 2.37185041 Accuracy: Top1: 42.9061%, Top5: 72.8251%\n",
      "[Epoch 079] Time: 58.9725s | [Train] Loss: 1.34344984 Accuracy: Top1: 61.7896%, Top5: 88.2416% | [Test] Loss: 2.38859951 Accuracy: Top1: 43.4191%, Top5: 72.8533%\n",
      "[Epoch 080] Time: 56.1414s | [Train] Loss: 1.33090919 Accuracy: Top1: 62.1781%, Top5: 88.4507% | [Test] Loss: 2.40325583 Accuracy: Top1: 42.9561%, Top5: 72.7028%\n",
      "[Epoch 081] Time: 56.1653s | [Train] Loss: 1.32094198 Accuracy: Top1: 62.1946%, Top5: 88.4653% | [Test] Loss: 2.39060228 Accuracy: Top1: 43.0641%, Top5: 73.0182%\n",
      "[Epoch 082] Time: 57.5881s | [Train] Loss: 1.31456990 Accuracy: Top1: 62.4526%, Top5: 88.6956% | [Test] Loss: 2.39595228 Accuracy: Top1: 42.9601%, Top5: 72.9423%\n",
      "[Epoch 083] Time: 56.4265s | [Train] Loss: 1.30877418 Accuracy: Top1: 62.6337%, Top5: 88.7420% | [Test] Loss: 2.38935628 Accuracy: Top1: 43.2772%, Top5: 73.1279%\n",
      "[Epoch 084] Time: 55.4396s | [Train] Loss: 1.30196920 Accuracy: Top1: 62.7478%, Top5: 88.8795% | [Test] Loss: 2.40531240 Accuracy: Top1: 43.3289%, Top5: 72.7039%\n",
      "[Epoch 085] Time: 57.9243s | [Train] Loss: 1.29311941 Accuracy: Top1: 63.2013%, Top5: 88.9406% | [Test] Loss: 2.40647148 Accuracy: Top1: 43.3519%, Top5: 72.9366%\n",
      "[Epoch 086] Time: 58.0608s | [Train] Loss: 1.29237921 Accuracy: Top1: 63.1911%, Top5: 88.8944% | [Test] Loss: 2.40629122 Accuracy: Top1: 43.3755%, Top5: 72.7763%\n",
      "[Epoch 087] Time: 57.0340s | [Train] Loss: 1.28266762 Accuracy: Top1: 63.1252%, Top5: 89.1198% | [Test] Loss: 2.40662377 Accuracy: Top1: 43.2617%, Top5: 72.9033%\n",
      "[Epoch 088] Time: 55.8935s | [Train] Loss: 1.28146911 Accuracy: Top1: 63.2723%, Top5: 89.1147% | [Test] Loss: 2.40589458 Accuracy: Top1: 42.9957%, Top5: 72.9216%\n",
      "[Epoch 089] Time: 58.0484s | [Train] Loss: 1.27598122 Accuracy: Top1: 63.5590%, Top5: 89.1049% | [Test] Loss: 2.41138592 Accuracy: Top1: 43.1554%, Top5: 72.9377%\n",
      "[Epoch 090] Time: 57.2284s | [Train] Loss: 1.27046098 Accuracy: Top1: 63.6013%, Top5: 89.2388% | [Test] Loss: 2.40833564 Accuracy: Top1: 43.4254%, Top5: 73.1279%\n",
      "[Epoch 091] Time: 55.9257s | [Train] Loss: 1.26962393 Accuracy: Top1: 63.7062%, Top5: 89.1722% | [Test] Loss: 2.41613103 Accuracy: Top1: 43.1934%, Top5: 73.1037%\n",
      "[Epoch 092] Time: 56.8611s | [Train] Loss: 1.26241428 Accuracy: Top1: 63.7811%, Top5: 89.3632% | [Test] Loss: 2.41154503 Accuracy: Top1: 43.3324%, Top5: 73.1635%\n",
      "[Epoch 093] Time: 58.1395s | [Train] Loss: 1.26114867 Accuracy: Top1: 64.0183%, Top5: 89.3944% | [Test] Loss: 2.41362291 Accuracy: Top1: 43.3352%, Top5: 73.0767%\n",
      "[Epoch 094] Time: 57.3361s | [Train] Loss: 1.26212013 Accuracy: Top1: 63.8795%, Top5: 89.4125% | [Test] Loss: 2.41673820 Accuracy: Top1: 43.2497%, Top5: 73.2307%\n",
      "[Epoch 095] Time: 56.4502s | [Train] Loss: 1.25761027 Accuracy: Top1: 63.9800%, Top5: 89.3434% | [Test] Loss: 2.41342926 Accuracy: Top1: 43.4157%, Top5: 73.0647%\n",
      "[Epoch 096] Time: 59.3175s | [Train] Loss: 1.25269964 Accuracy: Top1: 64.2652%, Top5: 89.5453% | [Test] Loss: 2.41499225 Accuracy: Top1: 43.5036%, Top5: 73.1830%\n",
      "[Epoch 097] Time: 56.8869s | [Train] Loss: 1.25076573 Accuracy: Top1: 64.1282%, Top5: 89.6538% | [Test] Loss: 2.41548404 Accuracy: Top1: 43.4254%, Top5: 73.2112%\n",
      "[Epoch 098] Time: 58.0786s | [Train] Loss: 1.24739391 Accuracy: Top1: 64.1966%, Top5: 89.5503% | [Test] Loss: 2.41735130 Accuracy: Top1: 43.2950%, Top5: 72.9975%\n",
      "[Epoch 099] Time: 57.7870s | [Train] Loss: 1.25471659 Accuracy: Top1: 63.9901%, Top5: 89.4063% | [Test] Loss: 2.41618354 Accuracy: Top1: 43.3278%, Top5: 73.0756%\n",
      "[Epoch 100] Time: 58.9555s | [Train] Loss: 1.25438434 Accuracy: Top1: 64.0697%, Top5: 89.5091% | [Test] Loss: 2.41607004 Accuracy: Top1: 43.3766%, Top5: 73.1710%\n"
     ]
    }
   ],
   "source": [
    "mknet = MKNet()\n",
    "\n",
    "args = SimpleNamespace()\n",
    "args.resize = 224\n",
    "args.augment = True\n",
    "args.noise = 0\n",
    "args.data_path = \"./Data\"\n",
    "args.batch_size = 512\n",
    "args.seed = 42\n",
    "args.num_epochs = 100\n",
    "args.criterion = \"CrossEntropy\"\n",
    "args.optimizer = \"adamw\"\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 2e-4\n",
    "args.scheduler = \"cosine\"\n",
    "args.device = \"cuda\"\n",
    "args.use_amp = False\n",
    "\n",
    "mknet = MKNet().to(args.device)\n",
    "\n",
    "dataset = CIFAR100(args)\n",
    "args.num_classes = dataset.num_classes \n",
    "\n",
    "# Training Modules \n",
    "train_eval_results = Train_Eval(args, \n",
    "                                    mknet, \n",
    "                                    dataset.train_loader, \n",
    "                                    dataset.test_loader\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e285f2-54f1-4232-8a48-bd61a4665502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
