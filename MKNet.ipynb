{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f769eab5-1ff0-45d7-9379-628bfcfefc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ImageNet, CIFAR10, CIFAR100\n",
    "from train_eval import Train_Eval\n",
    "from types import SimpleNamespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b07a11-c8f5-41a4-a50c-e0d845ed5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2d_NN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 K, \n",
    "                 stride, \n",
    "                 padding, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding,\n",
    "                 shuffle_pattern, \n",
    "                 shuffle_scale, \n",
    "                 magnitude_type, \n",
    "                 similarity_type, \n",
    "                 aggregation_type, \n",
    "                 lambda_param\n",
    "                ):\n",
    "\n",
    "        super(Conv2d_NN, self).__init__()\n",
    "\n",
    "        assert K == stride, \"K must be equal to stride for ConvNN\"\n",
    "        assert padding > 0 or padding == 0, \"Cannot have Negative Padding\"\n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Shuffle pattern must be: Before, After, Before After, Not Applicable\"\n",
    "        assert magnitude_type in [\"cosine\", \"euclidean\"], \"Similarity Matrix must be either cosine similarity or euclidean distance\"\n",
    "        assert sampling_type in [\"all\", \"random\", \"spatial\"], \"Consider all neighbors, random neighbors, or spatial neighbors\"\n",
    "        assert int(num_samples) > 0 or int(num_samples) == -1, \"Number of samples to consider must be greater than 0 or -1 for all samples\"\n",
    "        assert (sampling_type == \"all\" and int(num_samples) == -1) or (sampling_type != \"all\" and isinstance(num_samples, int)), \"Number of samples must be -1 for all samples or integer for random and spatial sampling\"\n",
    "\n",
    "        assert similarity_type in [\"Loc\", \"Col\", \"Loc_Col\"], \"Similarity Matrix based on Location, Color, or both\"\n",
    "        assert aggregation_type in [\"Col\", \"Loc_Col\"], \"Aggregation based on Color or Location and Color\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.in_channels = in_channels \n",
    "        self.out_channels = out_channels \n",
    "        self.K = K\n",
    "        self.stride = stride \n",
    "        self.padding = padding \n",
    "\n",
    "        # 3 Sampling Types: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples)\n",
    "        self.sample_padding = int(sample_padding) if sampling_type == \"spatial\" else 0\n",
    "\n",
    "        # Pixel Shuffling (optional) \n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "\n",
    "        # Similarity Metric\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if magnitude_type == \"cosine\" else False\n",
    "\n",
    "        # Similarity and Aggregation Types\n",
    "        self.similarity_type = similarity_type\n",
    "        self.aggregation_type = aggregation_type\n",
    "        \n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = True if (similarity_type in [\"Loc\", \"Loc_Col\"] or aggregation_type == \"Loc_Col\") else False\n",
    "        self.coordinate_cache = {}\n",
    "\n",
    "        # Pixel Shuffle Adjustments\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale) \n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "\n",
    "        self.in_channels_1d = self.in_channels * (self.shuffle_scale ** 2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_1d = self.out_channels * (self.shuffle_scale ** 2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        self.in_channels_1d = self.in_channels_1d + 2 if self.aggregation_type == \"Loc_Col\" else self.in_channels_1d\n",
    "\n",
    "        # Conv1d Layer\n",
    "        self.conv1d_layer = nn.Conv1d(\n",
    "            in_channels = self.in_channels_1d,\n",
    "            out_channels = self.out_channels_1d,\n",
    "            kernel_size = self.K, \n",
    "            stride = self.stride, \n",
    "            padding = 0, \n",
    "            # bias = False # Only if similarity_type is \"Loc\" (make ConvNN exactly same as Conv2d)\n",
    "        )\n",
    "\n",
    "        # Flatten * Unflatten layers \n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        self.unflatten = None\n",
    "\n",
    "        # Shapes\n",
    "        self.og_shape = None \n",
    "        self.padded_shape = None\n",
    "\n",
    "        # Utility Variables\n",
    "        self.INF = 1e5\n",
    "        self.NEG_INF = -1e5\n",
    "\n",
    "        self.lambda_param = lambda_param\n",
    "        # self.lambda_param = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):  \n",
    "        # 1. Pixel Unshuffle Layer\n",
    "        x = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "        self.og_shape = x.shape\n",
    "\n",
    "        # 2. Add Padding \n",
    "        if self.padding > 0:\n",
    "            x = F.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
    "            self.padded_shape = x.shape\n",
    "\n",
    "        # 3. Add Coordinate Encoding\n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "\n",
    "        # 4. Flatten Layer\n",
    "        x = self.flatten(x) \n",
    "\n",
    "        # 5. Similarity and Aggregation Type \n",
    "        if self.similarity_type == \"Loc\":\n",
    "            x_sim = x[:, -2:, :]\n",
    "        elif self.similarity_type == \"Loc_Col\":\n",
    "            x_sim = x\n",
    "        elif self.similarity_type == \"Col\" and self.aggregation_type == \"Col\":\n",
    "            x_sim = x\n",
    "        elif self.similarity_type == \"Col\" and self.aggregation_type == \"Loc_Col\":\n",
    "            x_sim = x[:, :-2, :]\n",
    "\n",
    "        if self.similarity_type in [\"Loc\", \"Loc_Col\"] and self.aggregation_type == \"Col\":\n",
    "            x = x[:, :-2, :]\n",
    "        else: \n",
    "            x = x\n",
    "\n",
    "        if self.similarity_type == \"Loc_Col\":      \n",
    "            # Normalize each modality to unit variance before combining\n",
    "            color_feats = x_sim[:, :-2, :]\n",
    "            color_std = torch.std(color_feats, dim=[1,2], keepdim=True) + 1e-6\n",
    "            color_norm = color_feats / color_std\n",
    "\n",
    "            coord_feats = x_sim[:, -2:, :]  # Already in [-1,1]\n",
    "            x_sim = torch.cat([self.lambda_param * color_norm, \n",
    "                            (1-self.lambda_param) * coord_feats], dim=1)\n",
    "            \n",
    "        # 6. Sampling + Similarity Calculation + Aggregation\n",
    "        if self.sampling_type == \"all\":\n",
    "            similarity_matrix = self._calculate_euclidean_matrix(x_sim) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix(x_sim)\n",
    "            prime = self._prime(x, similarity_matrix, self.K, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type == \"random\":\n",
    "            if self.num_samples > x.shape[-1]:\n",
    "                x_sample = x_sim\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "                torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(-0.1 if self.magnitude_type == \"euclidean\" else 1.1)\n",
    "                prime = self._prime(x, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "            else:\n",
    "                rand_idx = torch.randperm(x.shape[-1], device=x.device)[:self.num_samples]\n",
    "                x_sample = x_sim[:, :, rand_idx]\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "                range_idx = torch.arange(len(rand_idx), device=x.device)\n",
    "                similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == \"euclidean\" else self.NEG_INF\n",
    "                prime = self._prime_N(x, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "            \n",
    "\n",
    "        elif self.sampling_type == \"spatial\":\n",
    "            if self.num_samples > self.og_shape[-2]:\n",
    "                x_sample = x_sim\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "                torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(-0.1 if self.magnitude_type == \"euclidean\" else 1.1)\n",
    "                prime = self._prime(x, similarity_matrix, self.K, self.maximum)\n",
    "            else:\n",
    "                x_ind = torch.linspace(0 + self.sample_padding, self.og_shape[-2] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "                y_ind = torch.linspace(0 + self.sample_padding, self.og_shape[-1] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "                x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "                x_idx_flat, y_idx_flat = x_grid.flatten(), y_grid.flatten()\n",
    "                width = self.og_shape[-2]\n",
    "                flat_indices = y_idx_flat * width + x_idx_flat\n",
    "                x_sample = x_sim[:, :, flat_indices]\n",
    "\n",
    "                similarity_matrix = self._calculate_euclidean_matrix_N(x_sim, x_sample) if self.magnitude_type == \"euclidean\" else self._calculate_cosine_matrix_N(x_sim, x_sample)\n",
    "\n",
    "                range_idx = torch.arange(len(flat_indices), device=x.device)    \n",
    "                similarity_matrix[:, flat_indices, range_idx] = self.INF if self.magnitude_type == \"euclidean\" else self.NEG_INF\n",
    "                \n",
    "                prime = self._prime_N(x, similarity_matrix, self.K, flat_indices, self.maximum)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Sampling Type not Implemented\")\n",
    "        \n",
    "        # 7. Conv1d Layer\n",
    "        x = self.conv1d_layer(prime)\n",
    "\n",
    "        # 8. Unflatten Layer\n",
    "        if not self.unflatten: \n",
    "            self.unflatten = nn.Unflatten(dim=2, unflattened_size=self.og_shape[2:])\n",
    "        x = self.unflatten(x)\n",
    "\n",
    "        # 9. Pixel Shuffle Layer\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x \n",
    "        return x \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, matrix, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.matmul(matrix.transpose(1, 2), matrix)\n",
    "\n",
    "        dist_matrix = norm_squared.transpose(1, 2) + norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1)\n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_euclidean_matrix_N(self, matrix, matrix_sample, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        norm_squared_sample = torch.sum(matrix_sample ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.matmul(matrix.transpose(1, 2), matrix_sample)\n",
    "        \n",
    "        dist_matrix = norm_squared.transpose(1, 2) + norm_squared_sample - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0) \n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_cosine_matrix(self, matrix):\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_matrix.transpose(1, 2), norm_matrix)\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_cosine_matrix_N(self, matrix, matrix_sample):\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_matrix.transpose(1, 2), norm_sample)\n",
    "        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0) \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _prime(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "\n",
    "        if self.similarity_type == \"Loc\":\n",
    "            topk_values, topk_indices = torch.sort(magnitude_matrix, dim=2, descending=maximum, stable=True)\n",
    "            topk_indices = topk_indices[:, :, :K]\n",
    "            topk_indices, _ = torch.sort(topk_indices, dim=-1)\n",
    "        else:\n",
    "            topk_values, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "\n",
    "        if self.padding > 0: \n",
    "            prime = prime.view(b, c, self.padded_shape[-2], self.padded_shape[-1], K)\n",
    "            prime = prime[:, :, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "            prime = prime.reshape(b, c, K * self.og_shape[-2] * self.og_shape[-1])\n",
    "        else: \n",
    "            prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "        \n",
    "    def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        \n",
    "        topk_values, topk_indices = torch.topk(magnitude_matrix, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indices back to original matrix positions\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        if self.similarity_type == \"Loc\":\n",
    "            final_indices, _ = torch.sort(final_indices, dim=-1)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "\n",
    "        if self.padding > 0:\n",
    "            prime = prime.view(b, c, self.padded_shape[-2], self.padded_shape[-1], K)\n",
    "            prime = prime[:, :, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "            prime = prime.reshape(b, c, K * self.og_shape[-2] * self.og_shape[-1])\n",
    "        else:\n",
    "            prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "\n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords ### Last two channels are coordinate channels \n",
    "\n",
    "\n",
    "class MultiHeadConvNNAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 num_heads, \n",
    "                 attention_dropout,\n",
    "                 K, \n",
    "                 sampling_type, \n",
    "                 num_samples, \n",
    "                 sample_padding, \n",
    "                 magnitude_type, \n",
    "                 seq_length=197, \n",
    "                 coordinate_encoding=False\n",
    "                 ):\n",
    "        \n",
    "        super(MultiHeadConvNNAttention, self).__init__()\n",
    "        assert d_hidden % num_heads == 0, \"d_hidden must be divisible by num_heads\"\n",
    "\n",
    "        # Core Parameters\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.d_k = d_hidden // num_heads\n",
    "\n",
    "        # ConvNN Parameters\n",
    "        self.K = K\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # 3 types of sampling: all, random, spatial\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = int(num_samples) \n",
    "        self.sample_padding = int(sample_padding) if sampling_type == 'spatial' else 0    \n",
    "\n",
    "        # Similarity Metric \n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'cosine' else False\n",
    "\n",
    "        # Coordinate Encoding (optional) \n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {}\n",
    "        \n",
    "        # Linear projections for query, key, value\n",
    "        self.W_q = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_k = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_v = nn.Linear(d_hidden, d_hidden)\n",
    "        self.W_o = nn.Linear(d_hidden, d_hidden)   \n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.in_channels = (d_hidden // num_heads) + 1 if coordinate_encoding else (d_hidden // num_heads)\n",
    "        self.out_channels = (d_hidden // num_heads) \n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.K,\n",
    "            stride=self.K,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "        # Utility Variables \n",
    "        self.INF = 1.1\n",
    "        self.NEG_INF = -0.1 \n",
    "        \n",
    "    def split_head(self, x): \n",
    "        batch_size, seq_length, d_hidden = x.size()\n",
    "        self.batch_size = batch_size\n",
    "        # self.seq_length = seq_length\n",
    "        return x.contiguous().view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # (B, num_heads, seq_length, d_k)\n",
    "        \n",
    "    def combine_heads(self, x): \n",
    "        \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_hidden) \n",
    "    \n",
    "    def batch_split(self, x): \n",
    "        x = x.reshape(self.batch_size, -1, self.d_k, self.seq_length)\n",
    "        return x.permute(0, 1, 3, 2).contiguous()\n",
    "        \n",
    "    def batch_combine(self, x): \n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        x = x.permute(0, 1, 3, 2).contiguous() \n",
    "        return x.view(-1, self.d_k, seq_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Note: x shape: (B, seq_length, d_hidden)\n",
    "        # 1. Splithead & Batch Combine\n",
    "        k = self.batch_combine(self.split_head(self.W_k(x)))\n",
    "        v = self.batch_combine(self.split_head(self.W_v(x)))\n",
    "        \n",
    "\n",
    "        # 2. Add Coordinate Encoding \n",
    "        k = self._add_coordinate_encoding(k) if self.coordinate_encoding else k\n",
    "        v = self._add_coordinate_encoding(v) if self.coordinate_encoding else v\n",
    "\n",
    "\n",
    "        # 3. Sampling & Similarity Calculation\n",
    "        if self.sampling_type == 'all': # All Samples\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x)))\n",
    "            \n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix(k, q, sqrt=True)\n",
    "\n",
    "            \n",
    "            prime = self._prime(v, similarity_matrix, self.K, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'random': # Random Samples\n",
    "            rand_idx = torch.randperm(x.shape[1], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, rand_idx, :]            \n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(rand_idx), device=q.device)\n",
    "            similarity_matrix[:, rand_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, rand_idx, self.maximum)\n",
    "\n",
    "        elif self.sampling_type == 'spatial': # Spatial Samples\n",
    "            spat_idx = torch.linspace(0 + self.sample_padding, x.shape[1] - self.sample_padding - 1, self.num_samples, device=x.device).long()\n",
    "            x_sample = x[:, spat_idx, :]\n",
    "            q = self.batch_combine(self.split_head(self.W_q(x_sample)))\n",
    "            q = self._add_coordinate_encoding(q) if self.coordinate_encoding else q\n",
    "\n",
    "            similarity_matrix = self._calculate_cosine_matrix_N(k, q) if self.magnitude_type == 'cosine' else self._calculate_euclidean_matrix_N(k, q, sqrt=True)\n",
    "            range_idx = torch.arange(len(spat_idx), device=q.device)\n",
    "            similarity_matrix[:, spat_idx, range_idx] = self.INF if self.magnitude_type == 'euclidean' else self.NEG_INF\n",
    "\n",
    "\n",
    "            prime = self._prime_N(v, similarity_matrix, self.K, spat_idx, self.maximum)\n",
    "            \n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial']\")\n",
    "\n",
    "        # 4. Conv1d Layer\n",
    "        x = self.conv(prime)  \n",
    "\n",
    "        # 5. Dropout + Reshape (B, seq_length, d_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1) \n",
    "\n",
    "        # 6. Final Linear Projection\n",
    "        x = self.W_o(self.combine_heads(self.batch_split(x)))\n",
    "        return x       \n",
    "\n",
    "    def _calculate_euclidean_matrix(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        torch.diagonal(dist_matrix, dim1=1, dim2=2).fill_(-0.1) \n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_euclidean_matrix_N(self, K, Q, sqrt=False):\n",
    "        k_norm_squared = torch.sum(K**2, dim=1, keepdim=True)\n",
    "        q_norm_squared = torch.sum(Q**2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(K.transpose(1, 2), Q)\n",
    "\n",
    "        dist_matrix = k_norm_squared.transpose(1, 2) + q_norm_squared - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0.0)\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "        return dist_matrix \n",
    "\n",
    "    def _calculate_cosine_matrix(self, K, Q):\n",
    "        k_norm = F.normalize(K, p=2, dim=1)\n",
    "        q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(k_norm.transpose(1, 2), q_norm)\n",
    "        torch.diagonal(similarity_matrix, dim1=1, dim2=2).fill_(1.1)  # Fill diagonal with 1.1 to self-select\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _calculate_cosine_matrix_N(self, K, Q):\n",
    "        norm_k = F.normalize(K, p=2, dim=1)\n",
    "        norm_q = F.normalize(Q, p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(norm_k.transpose(1, 2), norm_q)\n",
    "        similarity_matrix = torch.softmax(similarity_matrix, dim=-1)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime(self, v, qk, K, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K, dim=2, largest=maximum)\n",
    "        topk_values = torch.softmax(topk_values, dim=-1)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "    \n",
    "\n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime \n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "\n",
    "        return prime\n",
    "\n",
    "    def _prime_N(self, v, qk, K, rand_idx, maximum):\n",
    "        b, c, t = v.shape\n",
    "        topk_values, topk_indices = torch.topk(qk, k=K-1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        # Map sample indicies back to original matrix positions \n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=v.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=-1)\n",
    "        topk_indices_exp = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        # Expand topk values to match the shape of indices\n",
    "        topk_values = torch.softmax(topk_values, dim=-1)\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        zeros = torch.zeros((b, c, t, 1), device=v.device)\n",
    "        topk_values_exp = torch.cat((zeros, topk_values_exp), dim=-1)\n",
    "\n",
    "        # Gather matrix values and apply similarity weighting \n",
    "        v_expanded = v.unsqueeze(-1).expand(b, c, t, K).contiguous()    \n",
    "        prime = torch.gather(v_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, c, t = x.shape \n",
    "        cache_key = f\"{b}_{t}_{x.device}\"\n",
    "        if cache_key in self.coordinate_cache: \n",
    "            expanded_coords = self.coordinate_cache[cache_key]\n",
    "        else: \n",
    "            coords_vec = torch.linspace(start=-1, end=1, steps=t, device=x.device).unsqueeze(0).expand(b, -1) \n",
    "            expanded_coords = coords_vec.unsqueeze(1).expand(b, -1, -1) \n",
    "            self.coordinate_cache[cache_key] = expanded_coords\n",
    "\n",
    "        x_with_coords = torch.cat([x, expanded_coords], dim=1) \n",
    "        return x_with_coords \n",
    "\n",
    "class MKNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_hidden = 128, \n",
    "                d_ffn = 512,\n",
    "                num_layers = 8, \n",
    "                dropout = 0.1, \n",
    "                convnn_args = {\n",
    "                    'K': 9,\n",
    "                    'stride': 9,\n",
    "                    'padding': 1,\n",
    "                    'sampling_type': 'all',\n",
    "                    'num_samples': -1,\n",
    "                    'sample_padding': 0,\n",
    "                    'shuffle_pattern': \"NA\", \n",
    "                    'shuffle_scale': 0,\n",
    "                    'magnitude_type': \"cosine\",\n",
    "                    'similarity_type': \"Col\",\n",
    "                    'aggregation_type': \"Col\",\n",
    "                    'lambda_param': 0.5\n",
    "                }, \n",
    "                convnn_attn_args = {\n",
    "                    \"num_heads\": 1,\n",
    "                    \"attention_dropout\": 0.1, \n",
    "                    \"K\": 9,\n",
    "                    \"sampling_type\": \"all\",\n",
    "                    \"num_samples\": -1,\n",
    "                    \"sample_padding\": 0,\n",
    "                    \"magnitude_type\": \"cosine\",\n",
    "                    \"seq_length\": 196, \n",
    "                    \"coordinate_encoding\": False\n",
    "                }\n",
    "                 ):\n",
    "\n",
    "        super(MKNet, self).__init__()\n",
    "        # Implementation of MKNet architecture goes here\n",
    "        \"\"\"\n",
    "        MKNet architecture implementation\n",
    "        \"\"\"\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.convnn_args = convnn_args\n",
    "        self.convnn_attn_args = convnn_attn_args\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            d_hidden=d_hidden, \n",
    "            img_size=224, \n",
    "            patch_size=16, \n",
    "            n_channels=3\n",
    "            )\n",
    "\n",
    "        self.cross_layers = nn.Sequential(*[MKCrossBlock(\n",
    "            d_hidden=self.d_hidden, \n",
    "            dropout = self.dropout, \n",
    "            convnn_args=self.convnn_args,\n",
    "            convnn_attn_args=self.convnn_attn_args\n",
    "            ) for _ in range(self.num_layers)])\n",
    "\n",
    "        self.ffn_layers = MKFFN(\n",
    "            d_hidden=self.d_hidden, \n",
    "            d_ffn=d_ffn, \n",
    "            dropout=self.dropout\n",
    "            )\n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]      \n",
    "        x = self.patch_embedding(x)  # [B, d_hidden, H/patch_size, W/patch_size]\n",
    "        x = self.cross_layers(x)     # [B, d_hidden, H/patch_size, W/patch_size]\n",
    "        x = self.ffn_layers(x)       # [B, d_hidden, H/patch_size, W/patch_size]\n",
    "        return x\n",
    "\n",
    "    def parameter_count(self): \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params\n",
    "\n",
    "\n",
    "class MKLocalBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_hidden, \n",
    "                convnn_args):\n",
    "        super(MKLocalBlock, self).__init__()\n",
    "        # Implementation of MKNet block goes here\n",
    "        \"\"\"\n",
    "        MKNet block implementation\n",
    "        \"\"\"\n",
    "        self.d_hidden = d_hidden\n",
    "        self.convnn = Conv2d_NN(\n",
    "            in_channels=d_hidden,\n",
    "            out_channels=d_hidden,\n",
    "            K=convnn_args['K'],\n",
    "            stride=convnn_args['stride'],\n",
    "            padding=convnn_args['padding'],\n",
    "            sampling_type=convnn_args['sampling_type'],\n",
    "            num_samples=convnn_args['num_samples'],\n",
    "            sample_padding=convnn_args['sample_padding'],\n",
    "            shuffle_pattern=convnn_args['shuffle_pattern'],\n",
    "            shuffle_scale=convnn_args['shuffle_scale'],\n",
    "            magnitude_type=convnn_args['magnitude_type'],\n",
    "            similarity_type=convnn_args['similarity_type'],\n",
    "            aggregation_type=convnn_args['aggregation_type'],\n",
    "            lambda_param=convnn_args['lambda_param']\n",
    "        )\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]\n",
    "        x = self.convnn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "class MKGlobalBlock(nn.Module):\n",
    "    def __init__(self, d_hidden, convnn_attn_args):\n",
    "        super(MKGlobalBlock, self).__init__()\n",
    "        # Implementation of MKNet global block goes here\n",
    "        \"\"\"\n",
    "        MKNet global block implementation\n",
    "        \"\"\"\n",
    "        self.convnn_attn = MultiHeadConvNNAttention(\n",
    "            d_hidden=d_hidden,\n",
    "            num_heads=convnn_attn_args[\"num_heads\"],\n",
    "            attention_dropout=convnn_attn_args[\"attention_dropout\"],\n",
    "            K=convnn_attn_args[\"K\"],\n",
    "            sampling_type=convnn_attn_args[\"sampling_type\"],\n",
    "            num_samples=convnn_attn_args[\"num_samples\"],\n",
    "            sample_padding=convnn_attn_args[\"sample_padding\"],\n",
    "            magnitude_type=convnn_attn_args[\"magnitude_type\"],\n",
    "            seq_length=convnn_attn_args[\"seq_length\"],\n",
    "            coordinate_encoding=convnn_attn_args[\"coordinate_encoding\"]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_hidden)\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]\n",
    "        B, C, H, W = x.size()\n",
    "        x_reshaped = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)  # [B, H*W, C]\n",
    "        x_attn = self.convnn_attn(x_reshaped)  # [B, H*W, C]\n",
    "        x_attn = self.norm(x_attn)\n",
    "        x_attn = self.dropout(x_attn)\n",
    "        x = x + x_attn.view(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "        return x\n",
    "\n",
    "class MKCrossBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_hidden, \n",
    "                 dropout, \n",
    "                 convnn_args, \n",
    "                 convnn_attn_args\n",
    "                 ):\n",
    "        super(MKCrossBlock, self).__init__()\n",
    "        # Implementation of MKNet cross block goes here\n",
    "        \"\"\"\n",
    "        MKNet cross block implementation\n",
    "        \"\"\"\n",
    "\n",
    "        self.local_block = MKLocalBlock(d_hidden=d_hidden, convnn_args=convnn_args)\n",
    "        self.global_block = MKGlobalBlock(d_hidden=d_hidden, convnn_attn_args=convnn_attn_args)\n",
    "\n",
    "        self.local_conv = nn.Conv2d(in_channels=d_hidden, \n",
    "                                    out_channels=d_hidden, \n",
    "                                    kernel_size=1,\n",
    "                                    stride=1,\n",
    "                                    padding=0, \n",
    "                                    bias=False)\n",
    "\n",
    "        self.global_conv = nn.Conv2d(in_channels=d_hidden, \n",
    "                                     out_channels=d_hidden, \n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding=0, \n",
    "                                     bias=False)\n",
    "\n",
    "        self.combine_conv = nn.Conv2d(in_channels=d_hidden, \n",
    "                                      out_channels=d_hidden, \n",
    "                                      kernel_size=1,\n",
    "                                      stride=1,\n",
    "                                      padding=0, \n",
    "                                      bias=False)\n",
    "\n",
    "        self.dropout_local = nn.Dropout(dropout)\n",
    "        self.dropout_global = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm_local = nn.LayerNorm(d_hidden)\n",
    "        self.norm_global = nn.LayerNorm(d_hidden)\n",
    "        self.norm_combine = nn.LayerNorm(d_hidden)\n",
    "\n",
    "    def forward(self, x): # input [B, C, H, W]\n",
    "        identity = x \n",
    "        \n",
    "        # Local Branch\n",
    "        x_local = self.norm_local(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) \n",
    "        x_local = self.local_block(x)\n",
    "        x_local = x + self.dropout_local(x_local)\n",
    "        x_local = self.local_conv(x_local)\n",
    "\n",
    "        # Global Branch\n",
    "        x_global = self.norm_global(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) \n",
    "        x_global = self.global_block(x)\n",
    "        x_global = x + self.dropout_global(x_global)\n",
    "        x_global = self.global_conv(x_global)\n",
    "\n",
    "        # Combine Local and Global\n",
    "        x_combine = x_local + x_global + identity\n",
    "        x_combine = self.norm_combine(x_combine.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        x_combine = self.combine_conv(x_combine)\n",
    "\n",
    "        x = x_combine\n",
    "        return x\n",
    "\n",
    "class MKFFN(nn.Module):\n",
    "    def __init__(self, d_hidden, d_ffn, num_classes=100, dropout=0.1):\n",
    "        super(MKFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_hidden, d_ffn)\n",
    "        self.fc2 = nn.Linear(d_ffn, d_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "        # Use adaptive pooling instead of flattening all spatial dimensions\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_layer = nn.Linear(d_hidden, num_classes) \n",
    "\n",
    "    def forward(self, x):  # [B, C, H, W]\n",
    "        # Apply FFN to spatial features\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).contiguous().view(B, H*W, C)  # [B, H*W, C]\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.pool(x)  # [B, C, 1, 1]\n",
    "        x = x.flatten(1)  # [B, C]\n",
    "        x = self.output_layer(x)  # [B, num_classes]\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_hidden, img_size, patch_size, n_channels=3):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(n_channels, d_hidden, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(d_hidden)\n",
    "        \n",
    "        # Add learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, d_hidden, \n",
    "                                                   img_size // patch_size, \n",
    "                                                   img_size // patch_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, d_hidden, H/P, W/P]\n",
    "        x = x + self.pos_embed  # Add positional encoding\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H/P, W/P, d_hidden]\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, d_hidden, H/P, W/P]\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a7965-762e-467b-899d-2380e6677770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/mnt/local/python3.11.8/lib/python3.11/site-packages/torch/autograd/profiler.py:228: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n",
      "STAGE:2025-10-24 23:52:03 1158278:1158278 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-10-24 23:52:03 1158278:1158278 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-10-24 23:52:03 1158278:1158278 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Trainable Parameters: 3.55952400 M\n",
      "Model Complexity (Profiler):\n",
      "   - GFLOPs: 0.66625946\n",
      "   - Trainable Parameters: 3.55952400 M\n",
      "After GFLOPS\n"
     ]
    }
   ],
   "source": [
    "mknet = MKNet()\n",
    "\n",
    "args = SimpleNamespace()\n",
    "args.resize = 224\n",
    "args.augment = True\n",
    "args.noise = 0\n",
    "args.data_path = \"./Data\"\n",
    "args.batch_size = 32\n",
    "args.seed = 42\n",
    "args.num_epochs = 50\n",
    "args.criterion = \"CrossEntropy\"\n",
    "args.optimizer = \"adamw\"\n",
    "args.lr = 1e-3\n",
    "args.weight_decay = 2e-4\n",
    "args.scheduler = \"cosine\"\n",
    "args.device = \"cuda\"\n",
    "args.use_amp = False\n",
    "\n",
    "mknet = MKNet().to(args.device)\n",
    "\n",
    "dataset = CIFAR100(args)\n",
    "args.num_classes = dataset.num_classes \n",
    "\n",
    "# Training Modules \n",
    "train_eval_results = Train_Eval(args, \n",
    "                                    mknet, \n",
    "                                    dataset.train_loader, \n",
    "                                    dataset.test_loader\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35a2b7-1c2e-44b4-b036-7f216aa1cf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926dbd61-8997-4d40-bce4-ae16bb9829c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
