{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dec 31 Branching Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch modules\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# ConvNN Modules\n",
    "from Conv2d_NN import * \n",
    "from Conv2d_NN_spatial import * \n",
    "from Conv1d_NN import * \n",
    "from Conv1d_NN_spatial import * \n",
    "from pixelshuffle import * \n",
    "\n",
    "# Data + Training\n",
    "from data import * \n",
    "from train import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branching2D_Layer(nn.module):\n",
    "    def __init__(self, in_ch, out_ch1, out_ch2, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch1 = out_ch1\n",
    "        self.out_ch2 = out_ch2\n",
    "        \n",
    "        self.Conv2d_Branch = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch1, kernel_size), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.Conv2d_NN_Branch = nn.Sequential(\n",
    "            Conv2d_NN(in_ch, out_ch2, K=kernel_size, stride = kernel_size), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.reduce_channels = nn.Conv2d(out_ch1+out_ch2, (out_ch1 + out_ch2) // 2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.Conv2d_Branch(x)\n",
    "        x2 = self.Conv2d_NN_Branch(x)\n",
    "        \n",
    "        # Debugging for shape\n",
    "        print(\"x1 shape: \", x1.shape)\n",
    "        print(\"x2 shape: \", x2.shape)\n",
    "        \n",
    "        ## Calculate expected Output size of x2\n",
    "        expected_x1_size = x2.size(2)\n",
    "        print(\"expected x1 size: \", expected_x1_size)\n",
    "        \n",
    "        ## Calculate padding for x1 to match x2's size \n",
    "        total_padding = expected_x1_size - x1.size(2) \n",
    "        print(\"total padding: \", total_padding)\n",
    "        \n",
    "        left_padding = total_padding //2 \n",
    "        right_padding = total_padding - left_padding\n",
    "        print(\"Right + Left padding: \", left_padding, right_padding)\n",
    "        \n",
    "        ## Apply dynamic padding to x2 \n",
    "        x1 = F.pad(x1, (left_padding, right_padding), 'constant', 0)\n",
    "        print(\"new x1 shape: \", x1.shape)\n",
    "        \n",
    "        ## Concatenate the outputs along the channel dimension \n",
    "        concat = torch.cat([x1, x2], dim=1)\n",
    "        print(\"concatenated shape: \", concat.shape)\n",
    "        \n",
    "        ## Reduce the number of channels \n",
    "        reduce = self.reduce_channels(concat) \n",
    "        print(\"reduced shape: \", reduce.shape)\n",
    "        \n",
    "        return reduce\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "### Playground Tests\n",
    "ex = torch.randn(32, 3, 64, 64)\n",
    "print(ex.shape)\n",
    "\n",
    "Conv2dNN = Conv2d_NN(\n",
    "    in_channels=3, \n",
    "    out_channels=16, \n",
    "    K=5, \n",
    "    stride=5, \n",
    "    padding=0, \n",
    "    samples='all', \n",
    "    magnitude_type='distance')\n",
    "\n",
    "Conv2d = nn.Conv2d(\n",
    "    in_channels=3, \n",
    "    out_channels=16, \n",
    "    kernel_size=5\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape:  torch.Size([32, 16, 60, 60])\n",
      "x2 shape:  torch.Size([32, 16, 64, 64])\n",
      "expected x1 size:  64\n",
      "total padding:  4\n",
      "Right + Left padding:  2 2\n",
      "new x1 shape:  torch.Size([32, 16, 60, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 60 but got size 64 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew x1 shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, x1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m## Concatenate the outputs along the channel dimension \u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m concat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, concat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m## Reduce the number of channels \u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 60 but got size 64 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "x1 = Conv2d.forward(ex)\n",
    "x2 = Conv2dNN.forward(ex)\n",
    "\n",
    "\n",
    "\n",
    "# Debugging for shape\n",
    "print(\"x1 shape: \", x1.shape)\n",
    "print(\"x2 shape: \", x2.shape)\n",
    "\n",
    "## Calculate expected Output size of x2\n",
    "expected_x1_size = x2.size(2)\n",
    "print(\"expected x1 size: \", expected_x1_size)\n",
    "\n",
    "## Calculate padding for x1 to match x2's size \n",
    "total_padding = expected_x1_size - x1.size(2) \n",
    "print(\"total padding: \", total_padding)\n",
    "\n",
    "left_padding = total_padding //2 \n",
    "right_padding = total_padding - left_padding\n",
    "print(\"Right + Left padding: \", left_padding, right_padding)\n",
    "\n",
    "## Apply dynamic padding to x2 \n",
    "x1 = F.pad(x1, (left_padding, right_padding), 'constant', 0)\n",
    "print(\"new x1 shape: \", x1.shape)\n",
    "\n",
    "## Concatenate the outputs along the channel dimension \n",
    "concat = torch.cat([x1, x2], dim=1)\n",
    "print(\"concatenated shape: \", concat.shape)\n",
    "\n",
    "## Reduce the number of channels \n",
    "reduce = self.reduce_channels(concat) \n",
    "print(\"reduced shape: \", reduce.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
