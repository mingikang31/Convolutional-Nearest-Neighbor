{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7fdeb5-b886-48c8-9991-6796f7dd3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40e7fc4-5236-44ae-ac90-ca6e0ec9e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Datasets \n",
    "from dataset import ImageNet, CIFAR10, CIFAR100\n",
    "from train_eval import Train_Eval\n",
    "\n",
    "# Models \n",
    "from models.allconvnet import AllConvNet \n",
    "\n",
    "# Utilities \n",
    "from utils import write_to_file, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fadb61-27d5-4d06-82a6-c2dbaae55b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mnt/research/j.farias/mkang2/Convolutional-Nearest-Neighbor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f5f14-92f5-42d9-b474-6397acaaf600",
   "metadata": {},
   "source": [
    "### I. CONTROL Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9059b00a-db01-4228-a4a2-8516c0d57734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"Conv2d\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=-1,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=False,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/Conv2d_Control\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de297f-0e39-4692-a851-d365bd994c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Upscale transform not defined. Skipping dataset upscale.\n",
      "Model: All Convolutional Network Conv2d\n",
      "Total Parameters: 6362\n",
      "Trainable Parameters: 6362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/mnt/local/python3.12/lib/python3.12/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Time: 6.9034s | [Train] Loss: 2.00888785 Accuracy: Top1: 25.8592%, Top5: 76.8642% | [Test] Loss: 1.79527853 Accuracy: Top1: 36.4351%, Top5: 86.0370%\n",
      "[Epoch 002] Time: 6.5123s | [Train] Loss: 1.81663057 Accuracy: Top1: 33.7096%, Top5: 84.2791% | [Test] Loss: 1.70774801 Accuracy: Top1: 38.3857%, Top5: 87.5896%\n",
      "[Epoch 003] Time: 6.4558s | [Train] Loss: 1.74704074 Accuracy: Top1: 36.0934%, Top5: 85.9255% | [Test] Loss: 1.63533909 Accuracy: Top1: 43.2126%, Top5: 89.1421%\n",
      "[Epoch 004] Time: 6.3957s | [Train] Loss: 1.70513760 Accuracy: Top1: 37.8437%, Top5: 86.9285% | [Test] Loss: 1.60613113 Accuracy: Top1: 43.2822%, Top5: 89.4606%\n",
      "[Epoch 005] Time: 6.5760s | [Train] Loss: 1.67092661 Accuracy: Top1: 38.9346%, Top5: 87.5819% | [Test] Loss: 1.56789567 Accuracy: Top1: 43.0533%, Top5: 90.8439%\n",
      "[Epoch 006] Time: 6.4342s | [Train] Loss: 1.64307343 Accuracy: Top1: 40.2514%, Top5: 88.1374% | [Test] Loss: 1.54402733 Accuracy: Top1: 44.2974%, Top5: 91.2818%\n",
      "[Epoch 007] Time: 6.4946s | [Train] Loss: 1.61662108 Accuracy: Top1: 41.2464%, Top5: 88.8647% | [Test] Loss: 1.53428582 Accuracy: Top1: 46.4968%, Top5: 90.8838%\n",
      "[Epoch 008] Time: 6.5520s | [Train] Loss: 1.58815975 Accuracy: Top1: 42.6950%, Top5: 89.4721% | [Test] Loss: 1.50295899 Accuracy: Top1: 47.6513%, Top5: 91.8093%\n",
      "[Epoch 009] Time: 6.4449s | [Train] Loss: 1.57010832 Accuracy: Top1: 43.0866%, Top5: 89.9477% | [Test] Loss: 1.48509477 Accuracy: Top1: 48.3678%, Top5: 91.5008%\n",
      "[Epoch 010] Time: 6.6195s | [Train] Loss: 1.55417473 Accuracy: Top1: 43.4643%, Top5: 90.1974% | [Test] Loss: 1.45997632 Accuracy: Top1: 49.2038%, Top5: 92.2572%\n",
      "[Epoch 011] Time: 6.4759s | [Train] Loss: 1.53259513 Accuracy: Top1: 44.5332%, Top5: 90.6630% | [Test] Loss: 1.44772424 Accuracy: Top1: 49.3929%, Top5: 92.5856%\n",
      "[Epoch 012] Time: 6.5490s | [Train] Loss: 1.52011153 Accuracy: Top1: 44.9488%, Top5: 90.7569% | [Test] Loss: 1.44493375 Accuracy: Top1: 49.6915%, Top5: 92.6652%\n",
      "[Epoch 013] Time: 6.5890s | [Train] Loss: 1.50682637 Accuracy: Top1: 45.6742%, Top5: 90.9567% | [Test] Loss: 1.41244430 Accuracy: Top1: 50.7464%, Top5: 93.2126%\n",
      "[Epoch 014] Time: 6.4921s | [Train] Loss: 1.49219092 Accuracy: Top1: 46.1497%, Top5: 91.2804% | [Test] Loss: 1.40874370 Accuracy: Top1: 51.0649%, Top5: 93.0036%\n",
      "[Epoch 015] Time: 6.5198s | [Train] Loss: 1.48125329 Accuracy: Top1: 46.3475%, Top5: 91.5341% | [Test] Loss: 1.41191836 Accuracy: Top1: 50.4479%, Top5: 92.5060%\n",
      "[Epoch 016] Time: 6.6377s | [Train] Loss: 1.47115664 Accuracy: Top1: 46.8131%, Top5: 91.5761% | [Test] Loss: 1.39938069 Accuracy: Top1: 51.0549%, Top5: 93.3818%\n",
      "[Epoch 017] Time: 6.5157s | [Train] Loss: 1.45930079 Accuracy: Top1: 47.5224%, Top5: 91.8039% | [Test] Loss: 1.40718432 Accuracy: Top1: 50.8360%, Top5: 92.9240%\n"
     ]
    }
   ],
   "source": [
    "    # Check if the output directory exists, if not create it\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Dataset \n",
    "    if args.dataset == \"cifar10\":\n",
    "        dataset = CIFAR10(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size \n",
    "    elif args.dataset == \"cifar100\":\n",
    "        dataset = CIFAR100(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size \n",
    "    elif args.dataset == \"imagenet\":\n",
    "        dataset = ImageNet(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "    \n",
    "    # Model \n",
    "    model = AllConvNet(args)\n",
    "    print(f\"Model: {model.name}\")\n",
    "    \n",
    "    # Parameters\n",
    "    total_params, trainable_params = model.parameter_count()\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    args.total_params = total_params\n",
    "    args.trainable_params = trainable_params\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    \n",
    "    # Training Modules \n",
    "    train_eval_results = Train_Eval(args, \n",
    "                                model, \n",
    "                                dataset.train_loader, \n",
    "                                dataset.test_loader\n",
    "                                )\n",
    "    \n",
    "    # Storing Results in output directory \n",
    "    write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "    write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "    write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b3b7a-7657-4585-ab6d-eaeb5dc1b95a",
   "metadata": {},
   "source": [
    "### II. Original ConvNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2c772f1-25a3-4d89-a6aa-4bfb4174ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"ConvNN\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=-1,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=False,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/ConvNN\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0692ee-9aff-469a-8d7c-b7f5186e0c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscale transform not defined. Skipping dataset upscale.\n",
      "Model: All Convolutional Network ConvNN\n",
      "Total Parameters: 97452\n",
      "Trainable Parameters: 97452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.12/lib/python3.12/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Time: 8.7831s | [Train] Loss: 1.94205378 Accuracy: Top1: 32.5547%, Top5: 82.4049% | [Test] Loss: 1.78601248 Accuracy: Top1: 41.3018%, Top5: 88.6744%\n",
      "[Epoch 002] Time: 7.8760s | [Train] Loss: 1.76870084 Accuracy: Top1: 41.6320%, Top5: 88.8507% | [Test] Loss: 1.70776096 Accuracy: Top1: 43.9490%, Top5: 91.1326%\n",
      "[Epoch 003] Time: 7.8357s | [Train] Loss: 1.71371125 Accuracy: Top1: 44.4933%, Top5: 90.1714% | [Test] Loss: 1.67413074 Accuracy: Top1: 47.0939%, Top5: 91.0529%\n",
      "[Epoch 004] Time: 7.7487s | [Train] Loss: 1.68356065 Accuracy: Top1: 46.1277%, Top5: 90.8967% | [Test] Loss: 1.64632788 Accuracy: Top1: 48.0195%, Top5: 92.1278%\n",
      "[Epoch 005] Time: 7.8389s | [Train] Loss: 1.65682312 Accuracy: Top1: 47.5304%, Top5: 91.3543% | [Test] Loss: 1.60940807 Accuracy: Top1: 49.8706%, Top5: 92.4562%\n",
      "[Epoch 006] Time: 8.0376s | [Train] Loss: 1.64171854 Accuracy: Top1: 48.2017%, Top5: 91.6900% | [Test] Loss: 1.60998311 Accuracy: Top1: 50.1891%, Top5: 92.1079%\n",
      "[Epoch 007] Time: 7.8523s | [Train] Loss: 1.62707703 Accuracy: Top1: 49.0549%, Top5: 91.9258% | [Test] Loss: 1.61468570 Accuracy: Top1: 49.6915%, Top5: 92.2572%\n",
      "[Epoch 008] Time: 7.8479s | [Train] Loss: 1.61294617 Accuracy: Top1: 49.6423%, Top5: 92.2554% | [Test] Loss: 1.59041293 Accuracy: Top1: 51.0549%, Top5: 92.6154%\n",
      "[Epoch 009] Time: 7.8001s | [Train] Loss: 1.60304382 Accuracy: Top1: 50.0919%, Top5: 92.2754% | [Test] Loss: 1.58010370 Accuracy: Top1: 51.6421%, Top5: 92.4363%\n",
      "[Epoch 010] Time: 7.9187s | [Train] Loss: 1.59230600 Accuracy: Top1: 50.5415%, Top5: 92.6810% | [Test] Loss: 1.58795747 Accuracy: Top1: 51.3336%, Top5: 92.6154%\n",
      "[Epoch 011] Time: 7.9044s | [Train] Loss: 1.58626871 Accuracy: Top1: 51.0870%, Top5: 92.5671% | [Test] Loss: 1.56344662 Accuracy: Top1: 52.2990%, Top5: 93.2325%\n",
      "[Epoch 012] Time: 7.9210s | [Train] Loss: 1.57428905 Accuracy: Top1: 51.5565%, Top5: 92.8309% | [Test] Loss: 1.55467542 Accuracy: Top1: 52.8762%, Top5: 93.2424%\n",
      "[Epoch 013] Time: 7.8466s | [Train] Loss: 1.56991877 Accuracy: Top1: 51.7004%, Top5: 93.0127% | [Test] Loss: 1.55205514 Accuracy: Top1: 53.2245%, Top5: 93.5410%\n",
      "[Epoch 014] Time: 7.8397s | [Train] Loss: 1.56305058 Accuracy: Top1: 52.1659%, Top5: 92.9927% | [Test] Loss: 1.56747328 Accuracy: Top1: 52.4582%, Top5: 92.8145%\n",
      "[Epoch 015] Time: 7.7548s | [Train] Loss: 1.55671290 Accuracy: Top1: 52.4496%, Top5: 93.1526% | [Test] Loss: 1.54654677 Accuracy: Top1: 53.4634%, Top5: 93.4912%\n",
      "[Epoch 016] Time: 7.8070s | [Train] Loss: 1.55064556 Accuracy: Top1: 52.6614%, Top5: 93.1586% | [Test] Loss: 1.55316588 Accuracy: Top1: 53.2245%, Top5: 93.6505%\n",
      "[Epoch 017] Time: 7.8032s | [Train] Loss: 1.54450231 Accuracy: Top1: 53.0551%, Top5: 93.3524% | [Test] Loss: 1.53006755 Accuracy: Top1: 54.0207%, Top5: 93.6604%\n",
      "[Epoch 018] Time: 7.6874s | [Train] Loss: 1.54016717 Accuracy: Top1: 53.4747%, Top5: 93.3624% | [Test] Loss: 1.54532361 Accuracy: Top1: 53.5928%, Top5: 93.8197%\n",
      "[Epoch 019] Time: 7.8582s | [Train] Loss: 1.53466172 Accuracy: Top1: 53.6685%, Top5: 93.4003% | [Test] Loss: 1.52986437 Accuracy: Top1: 53.8714%, Top5: 93.6007%\n",
      "[Epoch 020] Time: 7.8418s | [Train] Loss: 1.53208846 Accuracy: Top1: 53.9043%, Top5: 93.5442% | [Test] Loss: 1.52840291 Accuracy: Top1: 54.1799%, Top5: 93.9192%\n",
      "[Epoch 021] Time: 7.8731s | [Train] Loss: 1.48873613 Accuracy: Top1: 55.8524%, Top5: 94.2835% | [Test] Loss: 1.49760045 Accuracy: Top1: 55.7325%, Top5: 94.1680%\n",
      "[Epoch 022] Time: 7.6873s | [Train] Loss: 1.48310632 Accuracy: Top1: 56.0382%, Top5: 94.3055% | [Test] Loss: 1.49375974 Accuracy: Top1: 55.7424%, Top5: 94.2874%\n",
      "[Epoch 023] Time: 7.8212s | [Train] Loss: 1.48201130 Accuracy: Top1: 56.1161%, Top5: 94.5492% | [Test] Loss: 1.49241389 Accuracy: Top1: 55.5334%, Top5: 94.2576%\n",
      "[Epoch 024] Time: 8.0192s | [Train] Loss: 1.48080107 Accuracy: Top1: 56.1861%, Top5: 94.5053% | [Test] Loss: 1.49105535 Accuracy: Top1: 55.8121%, Top5: 94.2775%\n",
      "[Epoch 025] Time: 7.7123s | [Train] Loss: 1.47581982 Accuracy: Top1: 56.3599%, Top5: 94.4613% | [Test] Loss: 1.49126539 Accuracy: Top1: 55.7524%, Top5: 94.2377%\n",
      "[Epoch 026] Time: 7.9635s | [Train] Loss: 1.47820442 Accuracy: Top1: 56.2700%, Top5: 94.4353% | [Test] Loss: 1.49120855 Accuracy: Top1: 56.0211%, Top5: 94.1381%\n",
      "[Epoch 027] Time: 7.8385s | [Train] Loss: 1.47809575 Accuracy: Top1: 56.1001%, Top5: 94.4793% | [Test] Loss: 1.49240476 Accuracy: Top1: 55.7524%, Top5: 94.3272%\n",
      "[Epoch 028] Time: 7.8696s | [Train] Loss: 1.47444348 Accuracy: Top1: 56.6077%, Top5: 94.4873% | [Test] Loss: 1.49100667 Accuracy: Top1: 55.6031%, Top5: 94.3471%\n",
      "[Epoch 029] Time: 7.7016s | [Train] Loss: 1.47310430 Accuracy: Top1: 56.5437%, Top5: 94.4054% | [Test] Loss: 1.49054419 Accuracy: Top1: 55.7922%, Top5: 94.1481%\n",
      "[Epoch 030] Time: 7.9468s | [Train] Loss: 1.47456075 Accuracy: Top1: 56.1941%, Top5: 94.5332% | [Test] Loss: 1.49018703 Accuracy: Top1: 55.6728%, Top5: 94.3969%\n",
      "[Epoch 031] Time: 7.8401s | [Train] Loss: 1.47158742 Accuracy: Top1: 56.4518%, Top5: 94.4273% | [Test] Loss: 1.48803287 Accuracy: Top1: 56.0311%, Top5: 94.3073%\n",
      "[Epoch 032] Time: 7.9033s | [Train] Loss: 1.47125479 Accuracy: Top1: 56.6616%, Top5: 94.5932% | [Test] Loss: 1.48886656 Accuracy: Top1: 55.9614%, Top5: 94.3372%\n",
      "[Epoch 033] Time: 8.0747s | [Train] Loss: 1.46907862 Accuracy: Top1: 56.6017%, Top5: 94.6391% | [Test] Loss: 1.48828118 Accuracy: Top1: 55.8818%, Top5: 94.4168%\n",
      "[Epoch 034] Time: 7.9927s | [Train] Loss: 1.47022347 Accuracy: Top1: 56.8254%, Top5: 94.5912% | [Test] Loss: 1.48763664 Accuracy: Top1: 56.1803%, Top5: 94.2476%\n",
      "[Epoch 035] Time: 7.7107s | [Train] Loss: 1.46932842 Accuracy: Top1: 56.7136%, Top5: 94.6651% | [Test] Loss: 1.48614593 Accuracy: Top1: 55.9912%, Top5: 94.4068%\n",
      "[Epoch 036] Time: 8.0030s | [Train] Loss: 1.46922203 Accuracy: Top1: 56.7635%, Top5: 94.5332% | [Test] Loss: 1.48790989 Accuracy: Top1: 55.8320%, Top5: 94.3073%\n",
      "[Epoch 037] Time: 8.0075s | [Train] Loss: 1.46641394 Accuracy: Top1: 57.0552%, Top5: 94.6352% | [Test] Loss: 1.48615960 Accuracy: Top1: 55.9912%, Top5: 94.2675%\n",
      "[Epoch 038] Time: 7.8015s | [Train] Loss: 1.46595808 Accuracy: Top1: 56.9673%, Top5: 94.5552% | [Test] Loss: 1.48546691 Accuracy: Top1: 56.0709%, Top5: 94.2377%\n",
      "[Epoch 039] Time: 7.7483s | [Train] Loss: 1.46633468 Accuracy: Top1: 56.8514%, Top5: 94.6931% | [Test] Loss: 1.48560049 Accuracy: Top1: 56.0510%, Top5: 94.2675%\n",
      "[Epoch 040] Time: 7.7508s | [Train] Loss: 1.46579775 Accuracy: Top1: 56.8394%, Top5: 94.5792% | [Test] Loss: 1.48671345 Accuracy: Top1: 55.9514%, Top5: 94.3372%\n",
      "[Epoch 041] Time: 7.8162s | [Train] Loss: 1.46072409 Accuracy: Top1: 57.1292%, Top5: 94.6591% | [Test] Loss: 1.48330415 Accuracy: Top1: 56.1604%, Top5: 94.2974%\n",
      "[Epoch 042] Time: 7.9099s | [Train] Loss: 1.45878199 Accuracy: Top1: 57.2351%, Top5: 94.7291% | [Test] Loss: 1.48292189 Accuracy: Top1: 56.3197%, Top5: 94.3869%\n",
      "[Epoch 043] Time: 7.9223s | [Train] Loss: 1.46013905 Accuracy: Top1: 57.1831%, Top5: 94.7331% | [Test] Loss: 1.48285655 Accuracy: Top1: 56.0808%, Top5: 94.3869%\n",
      "[Epoch 044] Time: 7.8652s | [Train] Loss: 1.45717279 Accuracy: Top1: 57.1911%, Top5: 94.6571% | [Test] Loss: 1.48260904 Accuracy: Top1: 56.3893%, Top5: 94.2476%\n",
      "[Epoch 045] Time: 7.6009s | [Train] Loss: 1.45928614 Accuracy: Top1: 57.2490%, Top5: 94.6791% | [Test] Loss: 1.48276656 Accuracy: Top1: 56.0410%, Top5: 94.2974%\n",
      "[Epoch 046] Time: 7.7703s | [Train] Loss: 1.45641199 Accuracy: Top1: 57.2470%, Top5: 94.7690% | [Test] Loss: 1.48248349 Accuracy: Top1: 56.1604%, Top5: 94.3571%\n",
      "[Epoch 047] Time: 7.8875s | [Train] Loss: 1.45653826 Accuracy: Top1: 57.2830%, Top5: 94.7790% | [Test] Loss: 1.48287797 Accuracy: Top1: 56.0808%, Top5: 94.3571%\n",
      "[Epoch 048] Time: 7.9800s | [Train] Loss: 1.45555467 Accuracy: Top1: 57.3449%, Top5: 94.8729% | [Test] Loss: 1.48257188 Accuracy: Top1: 56.0609%, Top5: 94.3372%\n",
      "[Epoch 049] Time: 7.7294s | [Train] Loss: 1.45713797 Accuracy: Top1: 57.3989%, Top5: 94.7430% | [Test] Loss: 1.48260802 Accuracy: Top1: 55.9713%, Top5: 94.3670%\n",
      "[Epoch 050] Time: 7.6926s | [Train] Loss: 1.45829812 Accuracy: Top1: 57.1951%, Top5: 94.7231% | [Test] Loss: 1.48241499 Accuracy: Top1: 56.1405%, Top5: 94.3272%\n",
      "[Epoch 051] Time: 7.7030s | [Train] Loss: 1.45823877 Accuracy: Top1: 57.1651%, Top5: 94.6591% | [Test] Loss: 1.48226332 Accuracy: Top1: 56.0808%, Top5: 94.3372%\n",
      "[Epoch 052] Time: 7.6866s | [Train] Loss: 1.45717138 Accuracy: Top1: 57.3090%, Top5: 94.7530% | [Test] Loss: 1.48231244 Accuracy: Top1: 55.9713%, Top5: 94.3770%\n",
      "[Epoch 053] Time: 7.6462s | [Train] Loss: 1.45750035 Accuracy: Top1: 57.4988%, Top5: 94.7111% | [Test] Loss: 1.48189140 Accuracy: Top1: 56.1007%, Top5: 94.2974%\n",
      "[Epoch 054] Time: 7.6615s | [Train] Loss: 1.45747217 Accuracy: Top1: 57.2071%, Top5: 94.7710% | [Test] Loss: 1.48195560 Accuracy: Top1: 56.1007%, Top5: 94.3571%\n",
      "[Epoch 055] Time: 7.6065s | [Train] Loss: 1.45875894 Accuracy: Top1: 57.2211%, Top5: 94.7331% | [Test] Loss: 1.48222072 Accuracy: Top1: 56.2102%, Top5: 94.2974%\n",
      "[Epoch 056] Time: 7.8893s | [Train] Loss: 1.45796675 Accuracy: Top1: 57.1911%, Top5: 94.6831% | [Test] Loss: 1.48211640 Accuracy: Top1: 56.1505%, Top5: 94.3471%\n",
      "[Epoch 057] Time: 7.8673s | [Train] Loss: 1.45809970 Accuracy: Top1: 57.3669%, Top5: 94.8010% | [Test] Loss: 1.48213641 Accuracy: Top1: 56.0908%, Top5: 94.2974%\n",
      "[Epoch 058] Time: 8.0211s | [Train] Loss: 1.45700548 Accuracy: Top1: 57.5288%, Top5: 94.7410% | [Test] Loss: 1.48262184 Accuracy: Top1: 55.9614%, Top5: 94.3770%\n",
      "[Epoch 059] Time: 7.8957s | [Train] Loss: 1.45624887 Accuracy: Top1: 57.3090%, Top5: 94.7830% | [Test] Loss: 1.48196959 Accuracy: Top1: 56.0808%, Top5: 94.3869%\n",
      "[Epoch 060] Time: 7.9853s | [Train] Loss: 1.45629013 Accuracy: Top1: 57.2550%, Top5: 94.7590% | [Test] Loss: 1.48181162 Accuracy: Top1: 56.2201%, Top5: 94.3173%\n",
      "[Epoch 061] Time: 7.9726s | [Train] Loss: 1.45811096 Accuracy: Top1: 57.1391%, Top5: 94.7231% | [Test] Loss: 1.48176849 Accuracy: Top1: 56.2002%, Top5: 94.3073%\n",
      "[Epoch 062] Time: 8.1491s | [Train] Loss: 1.45647818 Accuracy: Top1: 57.4229%, Top5: 94.7550% | [Test] Loss: 1.48171319 Accuracy: Top1: 56.2102%, Top5: 94.2874%\n",
      "[Epoch 063] Time: 7.9281s | [Train] Loss: 1.45644488 Accuracy: Top1: 57.4209%, Top5: 94.8050% | [Test] Loss: 1.48172031 Accuracy: Top1: 56.2102%, Top5: 94.3272%\n",
      "[Epoch 064] Time: 7.9904s | [Train] Loss: 1.45599799 Accuracy: Top1: 57.3430%, Top5: 94.8889% | [Test] Loss: 1.48171357 Accuracy: Top1: 56.1903%, Top5: 94.2974%\n",
      "[Epoch 065] Time: 7.6963s | [Train] Loss: 1.45626252 Accuracy: Top1: 57.4089%, Top5: 94.7630% | [Test] Loss: 1.48169380 Accuracy: Top1: 56.2400%, Top5: 94.2974%\n",
      "[Epoch 066] Time: 7.9047s | [Train] Loss: 1.45672984 Accuracy: Top1: 57.1691%, Top5: 94.7391% | [Test] Loss: 1.48168421 Accuracy: Top1: 56.2201%, Top5: 94.3272%\n",
      "[Epoch 067] Time: 7.9189s | [Train] Loss: 1.45663611 Accuracy: Top1: 57.4728%, Top5: 94.6791% | [Test] Loss: 1.48163745 Accuracy: Top1: 56.1704%, Top5: 94.3272%\n",
      "[Epoch 068] Time: 7.8349s | [Train] Loss: 1.45533314 Accuracy: Top1: 57.3649%, Top5: 94.8709% | [Test] Loss: 1.48163035 Accuracy: Top1: 56.2102%, Top5: 94.3770%\n",
      "[Epoch 069] Time: 7.8847s | [Train] Loss: 1.45621071 Accuracy: Top1: 57.5328%, Top5: 94.7950% | [Test] Loss: 1.48163372 Accuracy: Top1: 56.1704%, Top5: 94.3372%\n",
      "[Epoch 070] Time: 7.7300s | [Train] Loss: 1.45734841 Accuracy: Top1: 57.2710%, Top5: 94.7111% | [Test] Loss: 1.48165993 Accuracy: Top1: 56.1704%, Top5: 94.3670%\n",
      "[Epoch 071] Time: 7.9694s | [Train] Loss: 1.45395179 Accuracy: Top1: 57.5987%, Top5: 94.6851% | [Test] Loss: 1.48166462 Accuracy: Top1: 56.0709%, Top5: 94.3372%\n",
      "[Epoch 072] Time: 7.7340s | [Train] Loss: 1.45495196 Accuracy: Top1: 57.5887%, Top5: 94.8390% | [Test] Loss: 1.48161229 Accuracy: Top1: 56.1505%, Top5: 94.3471%\n",
      "[Epoch 073] Time: 7.5882s | [Train] Loss: 1.45458295 Accuracy: Top1: 57.4808%, Top5: 94.8529% | [Test] Loss: 1.48152488 Accuracy: Top1: 56.2102%, Top5: 94.3372%\n",
      "[Epoch 074] Time: 7.8468s | [Train] Loss: 1.45664922 Accuracy: Top1: 57.2950%, Top5: 94.7131% | [Test] Loss: 1.48149342 Accuracy: Top1: 56.1803%, Top5: 94.3571%\n",
      "[Epoch 075] Time: 7.7420s | [Train] Loss: 1.45637009 Accuracy: Top1: 57.4788%, Top5: 94.8250% | [Test] Loss: 1.48156986 Accuracy: Top1: 56.1505%, Top5: 94.3372%\n",
      "[Epoch 076] Time: 7.7181s | [Train] Loss: 1.45437164 Accuracy: Top1: 57.2790%, Top5: 94.7151% | [Test] Loss: 1.48157780 Accuracy: Top1: 56.1505%, Top5: 94.3471%\n",
      "[Epoch 077] Time: 7.5830s | [Train] Loss: 1.45743920 Accuracy: Top1: 57.5228%, Top5: 94.7890% | [Test] Loss: 1.48157662 Accuracy: Top1: 56.1405%, Top5: 94.3372%\n",
      "[Epoch 078] Time: 7.7971s | [Train] Loss: 1.45569057 Accuracy: Top1: 57.2091%, Top5: 94.8050% | [Test] Loss: 1.48161935 Accuracy: Top1: 56.1704%, Top5: 94.3173%\n",
      "[Epoch 079] Time: 7.7949s | [Train] Loss: 1.45632649 Accuracy: Top1: 57.2730%, Top5: 94.7550% | [Test] Loss: 1.48153608 Accuracy: Top1: 56.1803%, Top5: 94.3571%\n",
      "[Epoch 080] Time: 7.8369s | [Train] Loss: 1.45377788 Accuracy: Top1: 57.4149%, Top5: 94.7910% | [Test] Loss: 1.48155087 Accuracy: Top1: 56.1405%, Top5: 94.3670%\n",
      "[Epoch 081] Time: 7.5549s | [Train] Loss: 1.45572025 Accuracy: Top1: 57.3509%, Top5: 94.7490% | [Test] Loss: 1.48153447 Accuracy: Top1: 56.1306%, Top5: 94.3471%\n",
      "[Epoch 082] Time: 7.8769s | [Train] Loss: 1.45536559 Accuracy: Top1: 57.2970%, Top5: 94.7790% | [Test] Loss: 1.48154702 Accuracy: Top1: 56.1405%, Top5: 94.3471%\n",
      "[Epoch 083] Time: 7.9508s | [Train] Loss: 1.45660865 Accuracy: Top1: 57.3829%, Top5: 94.6751% | [Test] Loss: 1.48154702 Accuracy: Top1: 56.1405%, Top5: 94.3471%\n",
      "[Epoch 084] Time: 7.7462s | [Train] Loss: 1.45600941 Accuracy: Top1: 57.3649%, Top5: 94.8449% | [Test] Loss: 1.48153283 Accuracy: Top1: 56.1505%, Top5: 94.3571%\n",
      "[Epoch 085] Time: 7.6226s | [Train] Loss: 1.45721074 Accuracy: Top1: 57.4169%, Top5: 94.7331% | [Test] Loss: 1.48155838 Accuracy: Top1: 56.1803%, Top5: 94.3571%\n",
      "[Epoch 086] Time: 7.7907s | [Train] Loss: 1.45599228 Accuracy: Top1: 57.3370%, Top5: 94.8729% | [Test] Loss: 1.48155308 Accuracy: Top1: 56.1604%, Top5: 94.3571%\n",
      "[Epoch 087] Time: 7.8840s | [Train] Loss: 1.45451384 Accuracy: Top1: 57.2610%, Top5: 94.8030% | [Test] Loss: 1.48154921 Accuracy: Top1: 56.1604%, Top5: 94.3471%\n",
      "[Epoch 088] Time: 7.8483s | [Train] Loss: 1.45445577 Accuracy: Top1: 57.4089%, Top5: 94.7331% | [Test] Loss: 1.48154134 Accuracy: Top1: 56.1505%, Top5: 94.3571%\n",
      "[Epoch 089] Time: 7.8687s | [Train] Loss: 1.45607209 Accuracy: Top1: 57.3469%, Top5: 94.7430% | [Test] Loss: 1.48157283 Accuracy: Top1: 56.1405%, Top5: 94.3471%\n",
      "[Epoch 090] Time: 7.7064s | [Train] Loss: 1.45417003 Accuracy: Top1: 57.4968%, Top5: 94.7351% | [Test] Loss: 1.48154174 Accuracy: Top1: 56.1604%, Top5: 94.3372%\n",
      "[Epoch 091] Time: 7.7073s | [Train] Loss: 1.45574343 Accuracy: Top1: 57.3130%, Top5: 94.7351% | [Test] Loss: 1.48156074 Accuracy: Top1: 56.1405%, Top5: 94.3372%\n",
      "[Epoch 092] Time: 7.5831s | [Train] Loss: 1.45500372 Accuracy: Top1: 57.4748%, Top5: 94.8649% | [Test] Loss: 1.48156737 Accuracy: Top1: 56.1505%, Top5: 94.3471%\n",
      "[Epoch 093] Time: 7.6527s | [Train] Loss: 1.45698711 Accuracy: Top1: 57.2311%, Top5: 94.7810% | [Test] Loss: 1.48152793 Accuracy: Top1: 56.1405%, Top5: 94.3372%\n",
      "[Epoch 094] Time: 7.9639s | [Train] Loss: 1.45717962 Accuracy: Top1: 57.3629%, Top5: 94.7430% | [Test] Loss: 1.48153277 Accuracy: Top1: 56.1405%, Top5: 94.3372%\n",
      "[Epoch 095] Time: 7.6910s | [Train] Loss: 1.45429410 Accuracy: Top1: 57.5388%, Top5: 94.9109% | [Test] Loss: 1.48151160 Accuracy: Top1: 56.1107%, Top5: 94.3372%\n",
      "[Epoch 096] Time: 7.8260s | [Train] Loss: 1.45502100 Accuracy: Top1: 57.5428%, Top5: 94.7730% | [Test] Loss: 1.48152667 Accuracy: Top1: 56.1206%, Top5: 94.3571%\n",
      "[Epoch 097] Time: 7.6666s | [Train] Loss: 1.45552381 Accuracy: Top1: 57.4648%, Top5: 94.7490% | [Test] Loss: 1.48153984 Accuracy: Top1: 56.1405%, Top5: 94.3471%\n",
      "[Epoch 098] Time: 7.8234s | [Train] Loss: 1.45735918 Accuracy: Top1: 57.4888%, Top5: 94.7470% | [Test] Loss: 1.48154782 Accuracy: Top1: 56.1704%, Top5: 94.3571%\n",
      "[Epoch 099] Time: 7.9451s | [Train] Loss: 1.45514981 Accuracy: Top1: 57.3390%, Top5: 94.7730% | [Test] Loss: 1.48154357 Accuracy: Top1: 56.1903%, Top5: 94.3372%\n",
      "[Epoch 100] Time: 7.6825s | [Train] Loss: 1.45413115 Accuracy: Top1: 57.3330%, Top5: 94.7251% | [Test] Loss: 1.48153457 Accuracy: Top1: 56.1704%, Top5: 94.3471%\n"
     ]
    }
   ],
   "source": [
    "    # Check if the output directory exists, if not create it\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Dataset \n",
    "    if args.dataset == \"cifar10\":\n",
    "        dataset = CIFAR10(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size \n",
    "    elif args.dataset == \"cifar100\":\n",
    "        dataset = CIFAR100(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size \n",
    "    elif args.dataset == \"imagenet\":\n",
    "        dataset = ImageNet(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "    \n",
    "    # Model \n",
    "    model = AllConvNet(args)\n",
    "    print(f\"Model: {model.name}\")\n",
    "    \n",
    "    # Parameters\n",
    "    total_params, trainable_params = model.parameter_count()\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    args.total_params = total_params\n",
    "    args.trainable_params = trainable_params\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    \n",
    "    # Training Modules \n",
    "    train_eval_results = Train_Eval(args, \n",
    "                                model, \n",
    "                                dataset.train_loader, \n",
    "                                dataset.test_loader\n",
    "                                )\n",
    "    \n",
    "    # Storing Results in output directory \n",
    "    write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "    write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "    write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7063905-1419-4c54-b67f-f335b65a6b5b",
   "metadata": {},
   "source": [
    "# NEW _PRIME_NEW function = multiply topk values with topk indexed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b461a3fb-f802-4867-ba4f-a23142c445f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Conv2d_NN(nn.Module): \n",
    "    \"\"\"Convolution 2D Nearest Neighbor Layer\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                K,\n",
    "                stride, \n",
    "                sampling_type, \n",
    "                num_samples, \n",
    "                sample_padding,\n",
    "                shuffle_pattern, \n",
    "                shuffle_scale, \n",
    "                magnitude_type,\n",
    "                coordinate_encoding=False\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            K (int): Number of Nearest Neighbors for consideration.\n",
    "            stride (int): Stride size.\n",
    "            sampling_type (str): Sampling type: \"all\", \"random\", \"spatial\".\n",
    "            num_samples (int): Number of samples to consider. -1 for all samples.\n",
    "            shuffle_pattern (str): Shuffle pattern: \"B\", \"A\", \"BA\".\n",
    "            shuffle_scale (int): Shuffle scale factor.\n",
    "            magnitude_type (str): Distance or Similarity.\n",
    "        \"\"\"\n",
    "        super(Conv2d_NN, self).__init__()\n",
    "        \n",
    "        # Assertions \n",
    "        assert K == stride, \"Error: K must be same as stride. K == stride.\"\n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Error: shuffle_pattern must be one of ['B', 'A', 'BA', 'NA']\"\n",
    "        assert magnitude_type in [\"distance\", \"similarity\"], \"Error: magnitude_type must be one of ['distance', 'similarity']\"\n",
    "        assert sampling_type in [\"all\", \"random\", \"spatial\"], \"Error: sampling_type must be one of ['all', 'random', 'spatial']\"\n",
    "        assert int(num_samples) > 0 or int(num_samples) == -1, \"Error: num_samples must be greater than 0 or -1 for all samples\"\n",
    "        assert (sampling_type == \"all\" and int(num_samples) == -1) or (sampling_type != \"all\" and isinstance(num_samples, int)), \"Error: num_samples must be -1 for 'all' sampling or an integer for 'random' and 'spatial' sampling\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.stride = stride\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples if num_samples != -1 else 'all'  # -1 for all samples\n",
    "        self.sample_padding = sample_padding if sampling_type == \"spatial\" else 0\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'similarity' else False\n",
    "\n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        self.in_channels = in_channels + 2 if self.coordinate_encoding else in_channels\n",
    "        self.out_channels = out_channels + 2 if self.coordinate_encoding else out_channels\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Adjust Channels for PixelShuffle\n",
    "        self.in_channels_1d = self.in_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_1d = self.out_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        # Conv1d Layer\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=self.in_channels_1d, \n",
    "                                      out_channels=self.out_channels_1d, \n",
    "                                      kernel_size=self.K, \n",
    "                                      stride=self.stride, \n",
    "                                      padding=0)\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "\n",
    "        # Pointwise Convolution Layer\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels=self.out_channels,\n",
    "                                         out_channels=self.out_channels - 2,\n",
    "                                         kernel_size=1,\n",
    "                                         stride=1,\n",
    "                                         padding=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        # Coordinate Channels (optional) + Unshuffle + Flatten \n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        x_2d = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "        x = self.flatten(x_2d)\n",
    "\n",
    "        if self.sampling_type == \"all\":    \n",
    "            # ConvNN Algorithm \n",
    "            matrix_magnitude = self._calculate_distance_matrix(x, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix(x)\n",
    "            prime = self._prime_new(x, matrix_magnitude, self.K, self.maximum)\n",
    "             \n",
    "        elif self.sampling_type == \"random\":\n",
    "            # Select random samples\n",
    "            rand_idx = torch.randperm(x.shape[2], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, :, rand_idx]\n",
    "\n",
    "            # ConvNN Algorithm \n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(rand_idx), device=x.device)\n",
    "            matrix_magnitude[:, rand_idx, range_idx] = float('inf') if self.magnitude_type == 'distance' else float('-inf')\n",
    "            prime = self._prime_N(x, matrix_magnitude, self.K, rand_idx, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type == \"spatial\":\n",
    "            # Get spatial sampled indices\n",
    "            x_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[2] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            y_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[3] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "            x_idx_flat, y_idx_flat = x_grid.flatten(), y_grid.flatten()\n",
    "            width = x_2d.shape[2] \n",
    "            flat_indices = y_idx_flat * width + x_idx_flat  \n",
    "            x_sample = x[:, :, flat_indices]\n",
    "\n",
    "            # ConvNN Algorithm\n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(flat_indices), device=x.device)\n",
    "            matrix_magnitude[:, flat_indices, range_idx] = float('inf') if self.magnitude_type == 'distance' else float('-inf')\n",
    "            prime = self._prime_N(x, matrix_magnitude, self.K, flat_indices, self.maximum)\n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial'].\")\n",
    "\n",
    "        # Post-Processing \n",
    "        x_conv = self.conv1d_layer(prime) \n",
    "        \n",
    "        # Unflatten + Shuffle\n",
    "        unflatten = nn.Unflatten(dim=2, unflattened_size=x_2d.shape[2:])\n",
    "        x = unflatten(x_conv)  # [batch_size, out_channels\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x\n",
    "        x = self.pointwise_conv(x) if self.coordinate_encoding else x\n",
    "        return x\n",
    "\n",
    "    def _calculate_distance_matrix(self, matrix, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared.transpose(2, 1) - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0) # remove negative values\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix # take square root if needed\n",
    "        \n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_distance_matrix_N(self, matrix, matrix_sample, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True).permute(0, 2, 1)\n",
    "        norm_squared_sample = torch.sum(matrix_sample ** 2, dim=1, keepdim=True).transpose(2, 1).permute(0, 2, 1)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix_sample)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared_sample - 2 * dot_product\n",
    "        dist_matrix = torch.clamp(dist_matrix, min=0) # remove negative values\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "\n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix(self, matrix):\n",
    "        # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix_N(self, matrix, matrix_sample):\n",
    "        # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_sample)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime_new(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        topk_values, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)    \n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        _, topk_indices = torch.topk(magnitude_matrix, k=K - 1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "\n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f16e75c1-defa-4198-a14d-11fb2d8fcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class AllConvNet(nn.Module): \n",
    "    def __init__(self, args): \n",
    "        super(AllConvNet, self).__init__()\n",
    "        self.args = args\n",
    "        self.model = \"All Convolutional Network\"\n",
    "        self.name = f\"{self.model} {self.args.layer}\"\n",
    "        \n",
    "        layers = []\n",
    "        in_ch = self.args.img_size[0] \n",
    "\n",
    "        for i in range(self.args.num_layers):\n",
    "            out_ch = self.args.channels[i]\n",
    "\n",
    "            # A dictionary to hold parameters for the current layer\n",
    "            layer_params = {\n",
    "                \"in_channels\": in_ch,\n",
    "                \"out_channels\": out_ch,\n",
    "                \"shuffle_pattern\": self.args.shuffle_pattern,\n",
    "                \"shuffle_scale\": self.args.shuffle_scale,\n",
    "            }\n",
    "\n",
    "            if self.args.layer == \"Conv2d\":\n",
    "                layer = nn.Conv2d(\n",
    "                    in_channels=in_ch, \n",
    "                    out_channels=out_ch, \n",
    "                    kernel_size=self.args.kernel_size, \n",
    "                    stride=1, \n",
    "                    padding='same'\n",
    "                )\n",
    "            \n",
    "            elif self.args.layer == \"ConvNN\":\n",
    "                layer_params.update({\n",
    "                    \"K\": self.args.K,\n",
    "                    \"stride\": self.args.K, # Stride is always K\n",
    "                    \"sampling_type\": self.args.sampling_type,\n",
    "                    \"num_samples\": self.args.num_samples,\n",
    "                    \"sample_padding\": self.args.sample_padding,\n",
    "                    \"magnitude_type\": self.args.magnitude_type,\n",
    "                    \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                })\n",
    "                layer = Conv2d_NN(**layer_params)\n",
    "\n",
    "            elif self.args.layer == \"ConvNN_Attn\":\n",
    "                layer_params.update({\n",
    "                    \"K\": self.args.K,\n",
    "                    \"stride\": self.args.K,\n",
    "                    \"sampling_type\": self.args.sampling_type,\n",
    "                    \"num_samples\": self.args.num_samples,\n",
    "                    \"sample_padding\": self.args.sample_padding,\n",
    "                    \"magnitude_type\": self.args.magnitude_type,\n",
    "                    \"img_size\": self.args.img_size[1:], # Pass H, W\n",
    "                    \"attention_dropout\": self.args.attention_dropout,\n",
    "                    \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                })\n",
    "                layer = Conv2d_NN_Attn(**layer_params)\n",
    "            \n",
    "            elif self.args.layer == \"Attention\":\n",
    "                layer_params.update({\n",
    "                    \"num_heads\": self.args.num_heads,\n",
    "                })\n",
    "                layer = Attention2d(**layer_params)\n",
    "            elif \"/\" in self.args.layer: # Handle all branching cases\n",
    "                ch1 = out_ch // 2 if out_ch % 2 == 0 else out_ch // 2 + 1\n",
    "                ch2 = out_ch - ch1\n",
    "                \n",
    "                layer_params.update({\"channel_ratio\": (ch1, ch2)})\n",
    "                \n",
    "                # --- Check all sub-cases for branching layers ---\n",
    "                if self.args.layer == \"Conv2d/ConvNN\":\n",
    "                    layer_params.update({\n",
    "                        \"kernel_size\": self.args.kernel_size,\n",
    "                        \"K\": self.args.K, \"stride\": self.args.K,\n",
    "                        \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "                        \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "                        \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                    })\n",
    "                    layer = Conv2d_ConvNN_Branching(**layer_params)\n",
    "                \n",
    "                elif self.args.layer == \"Conv2d/ConvNN_Attn\":\n",
    "                    layer_params.update({\n",
    "                        \"kernel_size\": self.args.kernel_size,\n",
    "                        \"K\": self.args.K, \"stride\": self.args.K,\n",
    "                        \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "                        \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "                        \"img_size\": self.args.img_size[1:],\n",
    "                        \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                    })\n",
    "                    layer = Conv2d_ConvNN_Attn_Branching(**layer_params)\n",
    "                \n",
    "                elif self.args.layer == \"Attention/ConvNN\":\n",
    "                    layer_params.update({\n",
    "                        \"num_heads\": self.args.num_heads,\n",
    "                        \"K\": self.args.K, \"stride\": self.args.K,\n",
    "                        \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "                        \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "                        \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                    })\n",
    "                    layer = Attention_ConvNN_Branching(**layer_params)\n",
    "\n",
    "                elif self.args.layer == \"Attention/ConvNN_Attn\":\n",
    "                    layer_params.update({\n",
    "                        \"num_heads\": self.args.num_heads,\n",
    "                        \"K\": self.args.K, \"stride\": self.args.K,\n",
    "                        \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "                        \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "                        \"img_size\": self.args.img_size[1:],\n",
    "                        \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                    })\n",
    "                    layer = Attention_ConvNN_Attn_Branching(**layer_params)\n",
    "                \n",
    "                # This is the specific case that was failing\n",
    "                elif self.args.layer == \"Conv2d/Attention\":\n",
    "                    layer_params.update({\n",
    "                        \"num_heads\": self.args.num_heads,\n",
    "                        \"kernel_size\": self.args.kernel_size, \n",
    "                        \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                    })\n",
    "                    layer = Attention_Conv2d_Branching(**layer_params)\n",
    "                \n",
    "                else:\n",
    "                    # This else now only catches unknown branching types\n",
    "                    raise ValueError(f\"Unknown branching layer type: {self.args.layer}\")\n",
    "\n",
    "            else:\n",
    "                # This is the final else for non-branching types\n",
    "                raise ValueError(f\"Layer type {self.args.layer} not supported in AllConvNet\")\n",
    "\n",
    "            layers.append(nn.InstanceNorm2d(num_features=out_ch)) # Pre-layer normalization\n",
    "            layers.append(layer)\n",
    "            if self.args.layer == \"ConvNN_Attn\":\n",
    "                pass #layers.append(nn.Dropout(p=self.args.attention_dropout))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "            # Update in_ch for the next layer\n",
    "            in_ch = out_ch\n",
    "            \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_ch, self.args.num_classes) # Use the final in_ch value\n",
    "        )\n",
    "        \n",
    "        self.to(self.args.device)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def summary(self): \n",
    "        original_device = next(self.parameters()).device\n",
    "        try:\n",
    "            self.to(\"cpu\")\n",
    "            print(f\"--- Summary for {self.name} ---\")\n",
    "            # torchsummary expects batch dimension, but img_size doesn't include it\n",
    "            summary(self, input_size=self.img_size, device=\"cpu\") \n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate summary: {e}\")\n",
    "        finally:\n",
    "            # Move model back to its original device\n",
    "            self.to(original_device)\n",
    "        \n",
    "    def parameter_count(self): \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e88ef343-b9e7-4877-875f-10f3c0d3133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"ConvNN\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=-1,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=False,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/ConvNN_New_Prime\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420f8bd-ed2b-4260-84b4-fec270b475d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Upscale transform not defined. Skipping dataset upscale.\n",
      "Model: All Convolutional Network ConvNN\n",
      "Total Parameters: 97452\n",
      "Trainable Parameters: 97452\n",
      "[Epoch 001] Time: 8.1835s | [Train] Loss: 1.96924643 Accuracy: Top1: 31.2480%, Top5: 80.7425% | [Test] Loss: 1.78848823 Accuracy: Top1: 40.5354%, Top5: 88.5151%\n",
      "[Epoch 002] Time: 7.9834s | [Train] Loss: 1.77757054 Accuracy: Top1: 41.0806%, Top5: 88.6929% | [Test] Loss: 1.70315316 Accuracy: Top1: 44.0585%, Top5: 90.9833%\n",
      "[Epoch 003] Time: 8.1019s | [Train] Loss: 1.72715530 Accuracy: Top1: 43.5262%, Top5: 90.0036% | [Test] Loss: 1.71253108 Accuracy: Top1: 45.4220%, Top5: 90.4757%\n",
      "[Epoch 004] Time: 8.1902s | [Train] Loss: 1.70131773 Accuracy: Top1: 45.2226%, Top5: 90.6730% | [Test] Loss: 1.64676818 Accuracy: Top1: 48.0394%, Top5: 92.2671%\n",
      "[Epoch 005] Time: 7.7990s | [Train] Loss: 1.67557305 Accuracy: Top1: 46.3535%, Top5: 91.0706% | [Test] Loss: 1.63855213 Accuracy: Top1: 47.8702%, Top5: 91.9586%\n",
      "[Epoch 006] Time: 7.8833s | [Train] Loss: 1.65980485 Accuracy: Top1: 47.4924%, Top5: 91.3503% | [Test] Loss: 1.62737602 Accuracy: Top1: 49.1342%, Top5: 92.3268%\n",
      "[Epoch 007] Time: 8.1218s | [Train] Loss: 1.64375307 Accuracy: Top1: 48.1957%, Top5: 91.8099% | [Test] Loss: 1.61702452 Accuracy: Top1: 49.8607%, Top5: 92.5657%\n",
      "[Epoch 008] Time: 8.1748s | [Train] Loss: 1.63166614 Accuracy: Top1: 48.6473%, Top5: 91.8538% | [Test] Loss: 1.61222019 Accuracy: Top1: 49.6417%, Top5: 92.4064%\n",
      "[Epoch 009] Time: 8.1808s | [Train] Loss: 1.62009130 Accuracy: Top1: 49.2347%, Top5: 92.1995% | [Test] Loss: 1.59557096 Accuracy: Top1: 51.1445%, Top5: 92.5955%\n",
      "[Epoch 010] Time: 7.8452s | [Train] Loss: 1.60805531 Accuracy: Top1: 49.9980%, Top5: 92.3833% | [Test] Loss: 1.59455208 Accuracy: Top1: 50.4180%, Top5: 92.7448%\n",
      "[Epoch 011] Time: 8.2664s | [Train] Loss: 1.60046835 Accuracy: Top1: 50.2737%, Top5: 92.5512% | [Test] Loss: 1.57221177 Accuracy: Top1: 51.9606%, Top5: 93.2623%\n",
      "[Epoch 012] Time: 8.4439s | [Train] Loss: 1.59144957 Accuracy: Top1: 50.6494%, Top5: 92.8149% | [Test] Loss: 1.56179787 Accuracy: Top1: 52.2691%, Top5: 93.2922%\n",
      "[Epoch 013] Time: 8.3270s | [Train] Loss: 1.58423035 Accuracy: Top1: 51.3087%, Top5: 92.8369% | [Test] Loss: 1.56010365 Accuracy: Top1: 52.7269%, Top5: 93.4813%\n",
      "[Epoch 014] Time: 8.3750s | [Train] Loss: 1.57910960 Accuracy: Top1: 51.1009%, Top5: 92.8708% | [Test] Loss: 1.58101525 Accuracy: Top1: 51.8611%, Top5: 92.2572%\n",
      "[Epoch 015] Time: 8.2051s | [Train] Loss: 1.57268428 Accuracy: Top1: 51.6804%, Top5: 92.9068% | [Test] Loss: 1.54346553 Accuracy: Top1: 53.5430%, Top5: 93.5808%\n",
      "[Epoch 016] Time: 8.4259s | [Train] Loss: 1.56319757 Accuracy: Top1: 52.1879%, Top5: 93.1586% | [Test] Loss: 1.55669994 Accuracy: Top1: 52.8364%, Top5: 93.6107%\n",
      "[Epoch 017] Time: 8.3126s | [Train] Loss: 1.56155149 Accuracy: Top1: 52.3258%, Top5: 93.0287% | [Test] Loss: 1.55941610 Accuracy: Top1: 52.4881%, Top5: 93.2225%\n",
      "[Epoch 018] Time: 7.6534s | [Train] Loss: 1.55348492 Accuracy: Top1: 52.4077%, Top5: 93.1965% | [Test] Loss: 1.54740933 Accuracy: Top1: 53.7122%, Top5: 93.6604%\n",
      "[Epoch 019] Time: 8.1721s | [Train] Loss: 1.55077375 Accuracy: Top1: 52.8413%, Top5: 93.3044% | [Test] Loss: 1.53197378 Accuracy: Top1: 54.4586%, Top5: 93.6206%\n",
      "[Epoch 020] Time: 8.2904s | [Train] Loss: 1.54691608 Accuracy: Top1: 53.0291%, Top5: 93.4843% | [Test] Loss: 1.53334408 Accuracy: Top1: 53.5729%, Top5: 93.7301%\n",
      "[Epoch 021] Time: 8.4056s | [Train] Loss: 1.50658406 Accuracy: Top1: 54.9353%, Top5: 94.0497% | [Test] Loss: 1.50475250 Accuracy: Top1: 55.5135%, Top5: 94.0187%\n",
      "[Epoch 022] Time: 8.3938s | [Train] Loss: 1.49885596 Accuracy: Top1: 55.2310%, Top5: 94.2295% | [Test] Loss: 1.50140084 Accuracy: Top1: 55.1453%, Top5: 94.1282%\n",
      "[Epoch 023] Time: 8.4691s | [Train] Loss: 1.49692036 Accuracy: Top1: 55.4907%, Top5: 94.3574% | [Test] Loss: 1.49911457 Accuracy: Top1: 55.5036%, Top5: 94.1580%\n",
      "[Epoch 024] Time: 7.7140s | [Train] Loss: 1.49642254 Accuracy: Top1: 55.5107%, Top5: 94.3494% | [Test] Loss: 1.49783318 Accuracy: Top1: 55.5434%, Top5: 94.3173%\n"
     ]
    }
   ],
   "source": [
    "    # Check if the output directory exists, if not create it\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Dataset \n",
    "    if args.dataset == \"cifar10\":\n",
    "        dataset = CIFAR10(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size \n",
    "    elif args.dataset == \"cifar100\":\n",
    "        dataset = CIFAR100(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size \n",
    "    elif args.dataset == \"imagenet\":\n",
    "        dataset = ImageNet(args)\n",
    "        args.num_classes = dataset.num_classes \n",
    "        args.img_size = dataset.img_size\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not supported\")\n",
    "    \n",
    "    # Model \n",
    "    model = AllConvNet(args)\n",
    "    print(f\"Model: {model.name}\")\n",
    "    \n",
    "    # Parameters\n",
    "    total_params, trainable_params = model.parameter_count()\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "    args.total_params = total_params\n",
    "    args.trainable_params = trainable_params\n",
    "    \n",
    "    # Set the seed for reproducibility\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    \n",
    "    # Training Modules \n",
    "    train_eval_results = Train_Eval(args, \n",
    "                                model, \n",
    "                                dataset.train_loader, \n",
    "                                dataset.test_loader\n",
    "                                )\n",
    "    \n",
    "    # Storing Results in output directory \n",
    "    write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "    write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "    write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10516ec6-711d-4c62-8fec-278c06c36fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
