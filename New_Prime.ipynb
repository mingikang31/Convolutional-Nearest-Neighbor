{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7fdeb5-b886-48c8-9991-6796f7dd3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40e7fc4-5236-44ae-ac90-ca6e0ec9e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "# Datasets \n",
    "from dataset import ImageNet, CIFAR10, CIFAR100\n",
    "from train_eval import Train_Eval\n",
    "\n",
    "# Models \n",
    "from models.allconvnet import AllConvNet \n",
    "\n",
    "# Utilities \n",
    "from utils import write_to_file, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8fadb61-27d5-4d06-82a6-c2dbaae55b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mnt/research/j.farias/mkang2/Convolutional-Nearest-Neighbor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f5f14-92f5-42d9-b474-6397acaaf600",
   "metadata": {},
   "source": [
    "### I. CONTROL Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059b00a-db01-4228-a4a2-8516c0d57734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"Conv2d\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=-1,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=False,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/Conv2d_Control\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de297f-0e39-4692-a851-d365bd994c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the output directory exists, if not create it\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset \n",
    "if args.dataset == \"cifar10\":\n",
    "    dataset = CIFAR10(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"cifar100\":\n",
    "    dataset = CIFAR100(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"imagenet\":\n",
    "    dataset = ImageNet(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "# Model \n",
    "model = AllConvNet(args)\n",
    "print(f\"Model: {model.name}\")\n",
    "\n",
    "# Parameters\n",
    "total_params, trainable_params = model.parameter_count()\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "args.total_params = total_params\n",
    "args.trainable_params = trainable_params\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "# Training Modules \n",
    "train_eval_results = Train_Eval(args, \n",
    "                            model, \n",
    "                            dataset.train_loader, \n",
    "                            dataset.test_loader\n",
    "                            )\n",
    "\n",
    "# Storing Results in output directory \n",
    "write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b3b7a-7657-4585-ab6d-eaeb5dc1b95a",
   "metadata": {},
   "source": [
    "### II. Original ConvNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c772f1-25a3-4d89-a6aa-4bfb4174ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"ConvNN\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"spatial\",\n",
    "    num_samples=6,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=False,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/ConvNN_Spat\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0692ee-9aff-469a-8d7c-b7f5186e0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the output directory exists, if not create it\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset \n",
    "if args.dataset == \"cifar10\":\n",
    "    dataset = CIFAR10(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"cifar100\":\n",
    "    dataset = CIFAR100(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"imagenet\":\n",
    "    dataset = ImageNet(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "# Model \n",
    "model = AllConvNet(args)\n",
    "print(f\"Model: {model.name}\")\n",
    "\n",
    "# Parameters\n",
    "total_params, trainable_params = model.parameter_count()\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "args.total_params = total_params\n",
    "args.trainable_params = trainable_params\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "# Training Modules \n",
    "train_eval_results = Train_Eval(args, \n",
    "                            model, \n",
    "                            dataset.train_loader, \n",
    "                            dataset.test_loader\n",
    "                            )\n",
    "\n",
    "# Storing Results in output directory \n",
    "write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7063905-1419-4c54-b67f-f335b65a6b5b",
   "metadata": {},
   "source": [
    "# NEW _PRIME_NEW function = multiply topk values with topk indexed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b461a3fb-f802-4867-ba4f-a23142c445f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Conv2d_NN(nn.Module): \n",
    "    \"\"\"Convolution 2D Nearest Neighbor Layer\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                K,\n",
    "                stride, \n",
    "                sampling_type, \n",
    "                num_samples, \n",
    "                sample_padding,\n",
    "                shuffle_pattern, \n",
    "                shuffle_scale, \n",
    "                magnitude_type,\n",
    "                coordinate_encoding=False\n",
    "                ): \n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            K (int): Number of Nearest Neighbors for consideration.\n",
    "            stride (int): Stride size.\n",
    "            sampling_type (str): Sampling type: \"all\", \"random\", \"spatial\".\n",
    "            num_samples (int): Number of samples to consider. -1 for all samples.\n",
    "            shuffle_pattern (str): Shuffle pattern: \"B\", \"A\", \"BA\".\n",
    "            shuffle_scale (int): Shuffle scale factor.\n",
    "            magnitude_type (str): Distance or Similarity.\n",
    "        \"\"\"\n",
    "        super(Conv2d_NN, self).__init__()\n",
    "        \n",
    "        # Assertions \n",
    "        assert K == stride, \"Error: K must be same as stride. K == stride.\"\n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Error: shuffle_pattern must be one of ['B', 'A', 'BA', 'NA']\"\n",
    "        assert magnitude_type in [\"distance\", \"similarity\"], \"Error: magnitude_type must be one of ['distance', 'similarity']\"\n",
    "        assert sampling_type in [\"all\", \"random\", \"spatial\"], \"Error: sampling_type must be one of ['all', 'random', 'spatial']\"\n",
    "        assert int(num_samples) > 0 or int(num_samples) == -1, \"Error: num_samples must be greater than 0 or -1 for all samples\"\n",
    "        assert (sampling_type == \"all\" and int(num_samples) == -1) or (sampling_type != \"all\" and isinstance(num_samples, int)), \"Error: num_samples must be -1 for 'all' sampling or an integer for 'random' and 'spatial' sampling\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.stride = stride\n",
    "        self.sampling_type = sampling_type\n",
    "        self.num_samples = num_samples if num_samples != -1 else 'all'  # -1 for all samples\n",
    "        self.sample_padding = sample_padding if sampling_type == \"spatial\" else 0\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "        self.magnitude_type = magnitude_type\n",
    "        self.maximum = True if self.magnitude_type == 'similarity' else False\n",
    "\n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        self.in_channels = in_channels + 2 if self.coordinate_encoding else in_channels\n",
    "        self.out_channels = out_channels + 2 if self.coordinate_encoding else out_channels\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Adjust Channels for PixelShuffle\n",
    "        self.in_channels_1d = self.in_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_1d = self.out_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        # Conv1d Layer\n",
    "        self.conv1d_layer = nn.Conv1d(in_channels=self.in_channels_1d, \n",
    "                                      out_channels=self.out_channels_1d, \n",
    "                                      kernel_size=self.K, \n",
    "                                      stride=self.stride, \n",
    "                                      padding=0)\n",
    "\n",
    "        # Flatten Layer\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "\n",
    "        # Pointwise Convolution Layer\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels=self.out_channels,\n",
    "                                         out_channels=self.out_channels - 2,\n",
    "                                         kernel_size=1,\n",
    "                                         stride=1,\n",
    "                                         padding=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x): \n",
    "        # Coordinate Channels (optional) + Unshuffle + Flatten \n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        x_2d = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "        x = self.flatten(x_2d)\n",
    "\n",
    "        if self.sampling_type == \"all\":    \n",
    "            # ConvNN Algorithm \n",
    "            matrix_magnitude = self._calculate_distance_matrix(x, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix(x)\n",
    "            \n",
    "            prime = self._prime_new(x, matrix_magnitude, self.K, self.maximum) ### CHANGED\n",
    "             \n",
    "        elif self.sampling_type == \"random\":\n",
    "            # Select random samples\n",
    "            rand_idx = torch.randperm(x.shape[2], device=x.device)[:self.num_samples]\n",
    "            x_sample = x[:, :, rand_idx]\n",
    "\n",
    "            # ConvNN Algorithm \n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(rand_idx), device=x.device)\n",
    "            matrix_magnitude[:, rand_idx, range_idx] = float('inf') if self.magnitude_type == 'distance' else float('-inf')\n",
    "            \n",
    "            prime = self._prime_N_new(x, matrix_magnitude, self.K, rand_idx, self.maximum)\n",
    "            \n",
    "        elif self.sampling_type == \"spatial\":\n",
    "            # Get spatial sampled indices\n",
    "            x_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[2] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            y_ind = torch.linspace(0 + self.sample_padding, x_2d.shape[3] - self.sample_padding - 1, self.num_samples, device=x.device).to(torch.long)\n",
    "            x_grid, y_grid = torch.meshgrid(x_ind, y_ind, indexing='ij')\n",
    "            x_idx_flat, y_idx_flat = x_grid.flatten(), y_grid.flatten()\n",
    "            width = x_2d.shape[2] \n",
    "            flat_indices = y_idx_flat * width + x_idx_flat  \n",
    "            x_sample = x[:, :, flat_indices]\n",
    "\n",
    "            # ConvNN Algorithm\n",
    "            matrix_magnitude = self._calculate_distance_matrix_N(x, x_sample, sqrt=True) if self.magnitude_type == 'distance' else self._calculate_similarity_matrix_N(x, x_sample)\n",
    "            range_idx = torch.arange(len(flat_indices), device=x.device)\n",
    "            matrix_magnitude[:, flat_indices, range_idx] = float('inf') if self.magnitude_type == 'distance' else float('-inf')\n",
    "            prime = self._prime_N_new(x, matrix_magnitude, self.K, flat_indices, self.maximum)\n",
    "        else: \n",
    "            raise ValueError(\"Invalid sampling_type. Must be one of ['all', 'random', 'spatial'].\")\n",
    "\n",
    "        # Post-Processing \n",
    "        x_conv = self.conv1d_layer(prime) \n",
    "        \n",
    "        # Unflatten + Shuffle\n",
    "        unflatten = nn.Unflatten(dim=2, unflattened_size=x_2d.shape[2:])\n",
    "        x = unflatten(x_conv)  # [batch_size, out_channels\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x\n",
    "        x = self.pointwise_conv(x) if self.coordinate_encoding else x\n",
    "        return x\n",
    "\n",
    "    def _calculate_distance_matrix(self, matrix, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared.transpose(2, 1) - 2 * dot_product\n",
    "        # dist_matrix = torch.clamp(dist_matrix, min=0) # remove negative values\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix # take square root if needed\n",
    "        \n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_distance_matrix_N(self, matrix, matrix_sample, sqrt=False):\n",
    "        norm_squared = torch.sum(matrix ** 2, dim=1, keepdim=True).permute(0, 2, 1)\n",
    "        norm_squared_sample = torch.sum(matrix_sample ** 2, dim=1, keepdim=True).transpose(2, 1).permute(0, 2, 1)\n",
    "        dot_product = torch.bmm(matrix.transpose(2, 1), matrix_sample)\n",
    "        \n",
    "        dist_matrix = norm_squared + norm_squared_sample - 2 * dot_product\n",
    "        # dist_matrix = torch.clamp(dist_matrix, min=0) # remove negative values\n",
    "        dist_matrix = torch.sqrt(dist_matrix) if sqrt else dist_matrix\n",
    "\n",
    "        return dist_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix(self, matrix):\n",
    "        # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_matrix)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def _calculate_similarity_matrix_N(self, matrix, matrix_sample):\n",
    "        # p=2 (L2 Norm - Euclidean Distance), dim=1 (across the channels)\n",
    "        norm_matrix = F.normalize(matrix, p=2, dim=1) \n",
    "        norm_sample = F.normalize(matrix_sample, p=2, dim=1)\n",
    "        similarity_matrix = torch.bmm(norm_matrix.transpose(2, 1), norm_sample)\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _prime_new(self, matrix, magnitude_matrix, K, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        topk_values, topk_indices = torch.topk(magnitude_matrix, k=K, dim=2, largest=maximum)\n",
    "        topk_indices_exp = topk_indices.unsqueeze(1).expand(b, c, t, K)    \n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K)    \n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=topk_indices_exp)\n",
    "        prime = topk_values_exp * prime\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _prime_N(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        _, topk_indices = torch.topk(magnitude_matrix, k=K - 1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "    \n",
    "    def _prime_N_new(self, matrix, magnitude_matrix, K, rand_idx, maximum):\n",
    "        b, c, t = matrix.shape\n",
    "        topk_values, topk_indices = torch.topk(magnitude_matrix, k=K - 1, dim=2, largest=maximum)\n",
    "        tk = topk_indices.shape[-1]\n",
    "        assert K == tk + 1, \"Error: K must be same as tk + 1. K == tk + 1.\"\n",
    "\n",
    "        mapped_tensor = rand_idx[topk_indices]\n",
    "        token_indices = torch.arange(t, device=matrix.device).view(1, t, 1).expand(b, t, 1)\n",
    "        final_indices = torch.cat([token_indices, mapped_tensor], dim=2)\n",
    "        indices_expanded = final_indices.unsqueeze(1).expand(b, c, t, K)\n",
    "\n",
    "        topk_values_exp = topk_values.unsqueeze(1).expand(b, c, t, K-1)\n",
    "        ones = torch.ones((b, c, t, 1), device=matrix.device)\n",
    "        topk_values_exp = torch.cat((ones, topk_values_exp), dim=-1)\n",
    "        \n",
    "\n",
    "        matrix_expanded = matrix.unsqueeze(-1).expand(b, c, t, K).contiguous()\n",
    "        prime = torch.gather(matrix_expanded, dim=2, index=indices_expanded)  \n",
    "        prime = topk_values_exp * prime\n",
    "        prime = prime.view(b, c, -1)\n",
    "        return prime\n",
    "\n",
    "    \n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b47c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv2d_New(nn.Module): \n",
    "    \"\"\"Convolution 2D Nearest Neighbor Layer\"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                kernel_size,\n",
    "                stride, \n",
    "                shuffle_pattern, \n",
    "                shuffle_scale, \n",
    "                coordinate_encoding\n",
    "                ): \n",
    "        \n",
    "        super(Conv2d_New, self).__init__()\n",
    "        \n",
    "        # Assertions \n",
    "        assert shuffle_pattern in [\"B\", \"A\", \"BA\", \"NA\"], \"Error: shuffle_pattern must be one of ['B', 'A', 'BA', 'NA']\"\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.shuffle_pattern = shuffle_pattern\n",
    "        self.shuffle_scale = shuffle_scale\n",
    "\n",
    "        # Positional Encoding (optional)\n",
    "        self.coordinate_encoding = coordinate_encoding\n",
    "        self.coordinate_cache = {} \n",
    "        self.in_channels = in_channels + 2 if self.coordinate_encoding else in_channels\n",
    "        self.out_channels = out_channels + 2 if self.coordinate_encoding else out_channels\n",
    "\n",
    "        # Shuffle2D/Unshuffle2D Layers\n",
    "        self.shuffle_layer = nn.PixelShuffle(upscale_factor=self.shuffle_scale)\n",
    "        self.unshuffle_layer = nn.PixelUnshuffle(downscale_factor=self.shuffle_scale)\n",
    "        \n",
    "        # Adjust Channels for PixelShuffle\n",
    "        self.in_channels_shuff = self.in_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"B\", \"BA\"] else self.in_channels\n",
    "        self.out_channels_shuff = self.out_channels * (self.shuffle_scale**2) if self.shuffle_pattern in [\"A\", \"BA\"] else self.out_channels\n",
    "\n",
    "        # Conv2d Layer\n",
    "        self.conv2d_layer = nn.Conv2d(in_channels=self.in_channels_shuff, \n",
    "                                      out_channels=self.out_channels_shuff, \n",
    "                                      kernel_size=self.kernel_size, \n",
    "                                      stride=self.stride, \n",
    "                                      padding=\"same\")\n",
    "\n",
    "\n",
    "        # Pointwise Convolution Layer\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels=self.out_channels,\n",
    "                                         out_channels=self.out_channels - 2,\n",
    "                                         kernel_size=1,\n",
    "                                         stride=1,\n",
    "                                         padding=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Coordinate Channels (optional) + Unshuffle + Flatten \n",
    "        x = self._add_coordinate_encoding(x) if self.coordinate_encoding else x\n",
    "        x_2d = self.unshuffle_layer(x) if self.shuffle_pattern in [\"B\", \"BA\"] else x\n",
    "\n",
    "        # Conv2d Layer\n",
    "        x = self.conv2d_layer(x_2d)\n",
    "\n",
    "        x = self.shuffle_layer(x) if self.shuffle_pattern in [\"A\", \"BA\"] else x\n",
    "        x = self.pointwise_conv(x) if self.coordinate_encoding else x\n",
    "        return x\n",
    "\n",
    "    def _add_coordinate_encoding(self, x):\n",
    "        b, _, h, w = x.shape\n",
    "        cache_key = f\"{b}_{h}_{w}_{x.device}\"\n",
    "\n",
    "        if cache_key in self.coordinate_cache:\n",
    "            expanded_grid = self.coordinate_cache[cache_key]\n",
    "        else:\n",
    "            y_coords_vec = torch.linspace(start=-1, end=1, steps=h, device=x.device)\n",
    "            x_coords_vec = torch.linspace(start=-1, end=1, steps=w, device=x.device)\n",
    "\n",
    "            y_grid, x_grid = torch.meshgrid(y_coords_vec, x_coords_vec, indexing='ij')\n",
    "            grid = torch.stack((x_grid, y_grid), dim=0).unsqueeze(0)\n",
    "            expanded_grid = grid.expand(b, -1, -1, -1)\n",
    "            self.coordinate_cache[cache_key] = expanded_grid\n",
    "\n",
    "        x_with_coords = torch.cat((x, expanded_grid), dim=1)\n",
    "        return x_with_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16e75c1-defa-4198-a14d-11fb2d8fcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllConvNet(nn.Module): \n",
    "    def __init__(self, args): \n",
    "        super(AllConvNet, self).__init__()\n",
    "        self.args = args\n",
    "        self.model = \"All Convolutional Network\"\n",
    "        self.name = f\"{self.model} {self.args.layer}\"\n",
    "        \n",
    "        layers = []\n",
    "        in_ch = self.args.img_size[0] \n",
    "\n",
    "        for i in range(self.args.num_layers):\n",
    "            out_ch = self.args.channels[i]\n",
    "\n",
    "            # A dictionary to hold parameters for the current layer\n",
    "            layer_params = {\n",
    "                \"in_channels\": in_ch,\n",
    "                \"out_channels\": out_ch,\n",
    "                \"shuffle_pattern\": self.args.shuffle_pattern,\n",
    "                \"shuffle_scale\": self.args.shuffle_scale,\n",
    "            }\n",
    "\n",
    "            if self.args.layer == \"Conv2d\":\n",
    "                layer = Conv2d_New(\n",
    "                    in_channels=in_ch, \n",
    "                    out_channels=out_ch, \n",
    "                    kernel_size=self.args.kernel_size, \n",
    "                    stride=1, \n",
    "                    shuffle_pattern=self.args.shuffle_pattern,\n",
    "                    shuffle_scale=self.args.shuffle_scale,\n",
    "                    coordinate_encoding=self.args.coordinate_encoding\n",
    "                )\n",
    "            \n",
    "            elif self.args.layer == \"ConvNN\":\n",
    "                layer_params.update({\n",
    "                    \"K\": self.args.K,\n",
    "                    \"stride\": self.args.K, # Stride is always K\n",
    "                    \"sampling_type\": self.args.sampling_type,\n",
    "                    \"num_samples\": self.args.num_samples,\n",
    "                    \"sample_padding\": self.args.sample_padding,\n",
    "                    \"magnitude_type\": self.args.magnitude_type,\n",
    "                    \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "                })\n",
    "                layer = Conv2d_NN(**layer_params)\n",
    "\n",
    "            # elif self.args.layer == \"ConvNN_Attn\":\n",
    "            #     layer_params.update({\n",
    "            #         \"K\": self.args.K,\n",
    "            #         \"stride\": self.args.K,\n",
    "            #         \"sampling_type\": self.args.sampling_type,\n",
    "            #         \"num_samples\": self.args.num_samples,\n",
    "            #         \"sample_padding\": self.args.sample_padding,\n",
    "            #         \"magnitude_type\": self.args.magnitude_type,\n",
    "            #         \"img_size\": self.args.img_size[1:], # Pass H, W\n",
    "            #         \"attention_dropout\": self.args.attention_dropout,\n",
    "            #         \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "            #     })\n",
    "            #     layer = Conv2d_NN_Attn(**layer_params)\n",
    "            \n",
    "            # elif self.args.layer == \"Attention\":\n",
    "            #     layer_params.update({\n",
    "            #         \"num_heads\": self.args.num_heads,\n",
    "            #     })\n",
    "            #     layer = Attention2d(**layer_params)\n",
    "            # elif \"/\" in self.args.layer: # Handle all branching cases\n",
    "            #     ch1 = out_ch // 2 if out_ch % 2 == 0 else out_ch // 2 + 1\n",
    "            #     ch2 = out_ch - ch1\n",
    "                \n",
    "            #     layer_params.update({\"channel_ratio\": (ch1, ch2)})\n",
    "                \n",
    "            #     # --- Check all sub-cases for branching layers ---\n",
    "            #     if self.args.layer == \"Conv2d/ConvNN\":\n",
    "            #         layer_params.update({\n",
    "            #             \"kernel_size\": self.args.kernel_size,\n",
    "            #             \"K\": self.args.K, \"stride\": self.args.K,\n",
    "            #             \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "            #             \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "            #             \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "            #         })\n",
    "            #         layer = Conv2d_ConvNN_Branching(**layer_params)\n",
    "                \n",
    "            #     elif self.args.layer == \"Conv2d/ConvNN_Attn\":\n",
    "            #         layer_params.update({\n",
    "            #             \"kernel_size\": self.args.kernel_size,\n",
    "            #             \"K\": self.args.K, \"stride\": self.args.K,\n",
    "            #             \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "            #             \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "            #             \"img_size\": self.args.img_size[1:],\n",
    "            #             \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "            #         })\n",
    "            #         layer = Conv2d_ConvNN_Attn_Branching(**layer_params)\n",
    "                \n",
    "            #     elif self.args.layer == \"Attention/ConvNN\":\n",
    "            #         layer_params.update({\n",
    "            #             \"num_heads\": self.args.num_heads,\n",
    "            #             \"K\": self.args.K, \"stride\": self.args.K,\n",
    "            #             \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "            #             \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "            #             \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "            #         })\n",
    "            #         layer = Attention_ConvNN_Branching(**layer_params)\n",
    "\n",
    "            #     elif self.args.layer == \"Attention/ConvNN_Attn\":\n",
    "            #         layer_params.update({\n",
    "            #             \"num_heads\": self.args.num_heads,\n",
    "            #             \"K\": self.args.K, \"stride\": self.args.K,\n",
    "            #             \"sampling_type\": self.args.sampling_type, \"num_samples\": self.args.num_samples,\n",
    "            #             \"sample_padding\": self.args.sample_padding, \"magnitude_type\": self.args.magnitude_type,\n",
    "            #             \"img_size\": self.args.img_size[1:],\n",
    "            #             \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "            #         })\n",
    "            #         layer = Attention_ConvNN_Attn_Branching(**layer_params)\n",
    "                \n",
    "            #     # This is the specific case that was failing\n",
    "            #     elif self.args.layer == \"Conv2d/Attention\":\n",
    "            #         layer_params.update({\n",
    "            #             \"num_heads\": self.args.num_heads,\n",
    "            #             \"kernel_size\": self.args.kernel_size, \n",
    "            #             \"coordinate_encoding\": self.args.coordinate_encoding\n",
    "            #         })\n",
    "            #         layer = Attention_Conv2d_Branching(**layer_params)\n",
    "                \n",
    "            #     else:\n",
    "            #         # This else now only catches unknown branching types\n",
    "            #         raise ValueError(f\"Unknown branching layer type: {self.args.layer}\")\n",
    "\n",
    "            else:\n",
    "                # This is the final else for non-branching types\n",
    "                raise ValueError(f\"Layer type {self.args.layer} not supported in AllConvNet\")\n",
    "\n",
    "            layers.append(nn.InstanceNorm2d(num_features=out_ch)) # Pre-layer normalization\n",
    "            layers.append(layer)\n",
    "            if self.args.layer == \"ConvNN_Attn\":\n",
    "                pass #layers.append(nn.Dropout(p=self.args.attention_dropout))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "            # Update in_ch for the next layer\n",
    "            in_ch = out_ch\n",
    "            \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_ch, self.args.num_classes) # Use the final in_ch value\n",
    "        )\n",
    "        \n",
    "        self.to(self.args.device)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def summary(self): \n",
    "        original_device = next(self.parameters()).device\n",
    "        try:\n",
    "            self.to(\"cpu\")\n",
    "            print(f\"--- Summary for {self.name} ---\")\n",
    "            # torchsummary expects batch dimension, but img_size doesn't include it\n",
    "            summary(self, input_size=self.img_size, device=\"cpu\") \n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate summary: {e}\")\n",
    "        finally:\n",
    "            # Move model back to its original device\n",
    "            self.to(original_device)\n",
    "        \n",
    "    def parameter_count(self): \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ef343-b9e7-4877-875f-10f3c0d3133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"ConvNN\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=-1,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=True,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/ConvNN_Coord_New_Prime_No_Clamp\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420f8bd-ed2b-4260-84b4-fec270b475d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the output directory exists, if not create it\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset \n",
    "if args.dataset == \"cifar10\":\n",
    "    dataset = CIFAR10(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"cifar100\":\n",
    "    dataset = CIFAR100(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"imagenet\":\n",
    "    dataset = ImageNet(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "# Model \n",
    "model = AllConvNet(args)\n",
    "print(f\"Model: {model.name}\")\n",
    "\n",
    "# Parameters\n",
    "total_params, trainable_params = model.parameter_count()\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "args.total_params = total_params\n",
    "args.trainable_params = trainable_params\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "# Training Modules \n",
    "train_eval_results = Train_Eval(args, \n",
    "                            model, \n",
    "                            dataset.train_loader, \n",
    "                            dataset.test_loader\n",
    "                            )\n",
    "\n",
    "# Storing Results in output directory \n",
    "write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b69b44",
   "metadata": {},
   "source": [
    "# New Conv2d with pixel shuffle n coordinate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1f46db-02f3-4441-a49b-96789053f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Create default args\n",
    "args = SimpleNamespace(\n",
    "    layer=\"Conv2d\",\n",
    "    num_layers=3,\n",
    "    channels=[8, 16, 32],\n",
    "    K=9,\n",
    "    kernel_size=3,\n",
    "    sampling_type=\"all\",\n",
    "    num_samples=-1,\n",
    "    sample_padding=0,\n",
    "    num_heads=4,\n",
    "    attention_dropout=0.1,\n",
    "    shuffle_pattern=\"BA\",\n",
    "    shuffle_scale=2,\n",
    "    magnitude_type=\"similarity\",\n",
    "    coordinate_encoding=True,\n",
    "    dataset=\"cifar10\",\n",
    "    data_path=\"./Data\",\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    use_amp=False,\n",
    "    clip_grad_norm=None,\n",
    "    criterion=\"CrossEntropy\",\n",
    "    optimizer=\"adamw\",\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-6,\n",
    "    lr=1e-3,\n",
    "    lr_step=20,\n",
    "    lr_gamma=0.1,\n",
    "    scheduler=\"step\",\n",
    "    device=\"cuda\",\n",
    "    seed=0,\n",
    "    output_dir=\"./Output/Simple/Conv2d_New\", \n",
    "    resize=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d58405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.11.8/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upscale transform not defined. Skipping dataset upscale.\n",
      "Model: All Convolutional Network Conv2d\n",
      "Total Parameters: 123338\n",
      "Trainable Parameters: 123338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/python3.11.8/lib/python3.11/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Time: 20.3228s | [Train] Loss: 1.70366861 Accuracy: Top1: 37.0744%, Top5: 86.1353% | [Test] Loss: 1.40582694 Accuracy: Top1: 49.6019%, Top5: 92.7548%\n",
      "[Epoch 002] Time: 6.7030s | [Train] Loss: 1.37470604 Accuracy: Top1: 50.1718%, Top5: 92.9907% | [Test] Loss: 1.25601769 Accuracy: Top1: 54.6775%, Top5: 94.9841%\n",
      "[Epoch 003] Time: 6.8047s | [Train] Loss: 1.23253586 Accuracy: Top1: 55.2230%, Top5: 94.8130% | [Test] Loss: 1.15901932 Accuracy: Top1: 58.3499%, Top5: 95.8101%\n",
      "[Epoch 004] Time: 6.7449s | [Train] Loss: 1.14003444 Accuracy: Top1: 58.9154%, Top5: 95.6222% | [Test] Loss: 1.04497332 Accuracy: Top1: 63.1967%, Top5: 96.1485%\n",
      "[Epoch 005] Time: 6.8157s | [Train] Loss: 1.07158454 Accuracy: Top1: 61.6328%, Top5: 96.1537% | [Test] Loss: 1.01364355 Accuracy: Top1: 64.7691%, Top5: 96.4271%\n",
      "[Epoch 006] Time: 6.9874s | [Train] Loss: 1.02562689 Accuracy: Top1: 63.2633%, Top5: 96.4194% | [Test] Loss: 1.00111601 Accuracy: Top1: 65.5056%, Top5: 96.4769%\n",
      "[Epoch 007] Time: 6.8017s | [Train] Loss: 0.98393935 Accuracy: Top1: 64.8597%, Top5: 96.8750% | [Test] Loss: 0.96403260 Accuracy: Top1: 66.5904%, Top5: 96.8451%\n",
      "[Epoch 008] Time: 6.8763s | [Train] Loss: 0.95164665 Accuracy: Top1: 66.0886%, Top5: 97.0608% | [Test] Loss: 0.92207174 Accuracy: Top1: 67.9439%, Top5: 97.1835%\n",
      "[Epoch 009] Time: 6.7422s | [Train] Loss: 0.91912495 Accuracy: Top1: 67.2255%, Top5: 97.2626% | [Test] Loss: 0.89898262 Accuracy: Top1: 68.0932%, Top5: 97.3229%\n",
      "[Epoch 010] Time: 6.9102s | [Train] Loss: 0.89991584 Accuracy: Top1: 68.0127%, Top5: 97.3326% | [Test] Loss: 0.88603258 Accuracy: Top1: 69.2775%, Top5: 97.5119%\n",
      "[Epoch 011] Time: 6.8736s | [Train] Loss: 0.87228460 Accuracy: Top1: 69.1077%, Top5: 97.4844% | [Test] Loss: 0.87073959 Accuracy: Top1: 69.2576%, Top5: 97.4622%\n",
      "[Epoch 012] Time: 6.8627s | [Train] Loss: 0.85286529 Accuracy: Top1: 69.6911%, Top5: 97.7282% | [Test] Loss: 0.91050314 Accuracy: Top1: 68.4116%, Top5: 97.2731%\n",
      "[Epoch 013] Time: 6.9532s | [Train] Loss: 0.83720345 Accuracy: Top1: 70.2825%, Top5: 97.7442% | [Test] Loss: 0.85199485 Accuracy: Top1: 70.1035%, Top5: 97.5518%\n",
      "[Epoch 014] Time: 6.9086s | [Train] Loss: 0.82214407 Accuracy: Top1: 70.7161%, Top5: 97.9300% | [Test] Loss: 0.83013764 Accuracy: Top1: 71.1385%, Top5: 97.8006%\n",
      "[Epoch 015] Time: 7.0948s | [Train] Loss: 0.80351554 Accuracy: Top1: 71.3615%, Top5: 97.9739% | [Test] Loss: 0.80240753 Accuracy: Top1: 72.2432%, Top5: 97.9498%\n",
      "[Epoch 016] Time: 6.9033s | [Train] Loss: 0.78399067 Accuracy: Top1: 71.9629%, Top5: 98.1018% | [Test] Loss: 0.79934179 Accuracy: Top1: 72.0641%, Top5: 97.9498%\n",
      "[Epoch 017] Time: 6.8895s | [Train] Loss: 0.77190152 Accuracy: Top1: 72.5923%, Top5: 98.1618% | [Test] Loss: 0.79451595 Accuracy: Top1: 72.2233%, Top5: 98.0096%\n",
      "[Epoch 018] Time: 6.7857s | [Train] Loss: 0.75980056 Accuracy: Top1: 72.9120%, Top5: 98.1658% | [Test] Loss: 0.80722318 Accuracy: Top1: 71.9546%, Top5: 97.9100%\n",
      "[Epoch 019] Time: 6.7260s | [Train] Loss: 0.74697490 Accuracy: Top1: 73.5834%, Top5: 98.2856% | [Test] Loss: 0.79502247 Accuracy: Top1: 72.2631%, Top5: 98.1091%\n",
      "[Epoch 020] Time: 6.9155s | [Train] Loss: 0.73369786 Accuracy: Top1: 73.9290%, Top5: 98.3556% | [Test] Loss: 0.81737021 Accuracy: Top1: 71.9347%, Top5: 97.6513%\n",
      "[Epoch 021] Time: 6.9376s | [Train] Loss: 0.65382370 Accuracy: Top1: 76.8682%, Top5: 98.7512% | [Test] Loss: 0.72085685 Accuracy: Top1: 74.8408%, Top5: 98.3579%\n",
      "[Epoch 022] Time: 7.0744s | [Train] Loss: 0.64017209 Accuracy: Top1: 77.2938%, Top5: 98.8012% | [Test] Loss: 0.72000637 Accuracy: Top1: 75.2687%, Top5: 98.2683%\n",
      "[Epoch 023] Time: 7.7238s | [Train] Loss: 0.63190896 Accuracy: Top1: 77.5695%, Top5: 98.8211% | [Test] Loss: 0.71581586 Accuracy: Top1: 75.1891%, Top5: 98.2783%\n",
      "[Epoch 024] Time: 7.6654s | [Train] Loss: 0.62935782 Accuracy: Top1: 77.8313%, Top5: 98.8151% | [Test] Loss: 0.71922485 Accuracy: Top1: 75.1095%, Top5: 98.3280%\n",
      "[Epoch 025] Time: 8.5064s | [Train] Loss: 0.62748845 Accuracy: Top1: 77.6894%, Top5: 98.8551% | [Test] Loss: 0.71841077 Accuracy: Top1: 75.0697%, Top5: 98.4275%\n",
      "[Epoch 026] Time: 8.9936s | [Train] Loss: 0.62315143 Accuracy: Top1: 77.8173%, Top5: 98.8651% | [Test] Loss: 0.71775117 Accuracy: Top1: 75.2886%, Top5: 98.4176%\n",
      "[Epoch 027] Time: 7.3021s | [Train] Loss: 0.62152148 Accuracy: Top1: 78.0051%, Top5: 98.8031% | [Test] Loss: 0.71952089 Accuracy: Top1: 75.0100%, Top5: 98.3977%\n",
      "[Epoch 028] Time: 7.2233s | [Train] Loss: 0.61879361 Accuracy: Top1: 78.0231%, Top5: 98.8611% | [Test] Loss: 0.71318591 Accuracy: Top1: 75.2588%, Top5: 98.4475%\n",
      "[Epoch 029] Time: 7.4245s | [Train] Loss: 0.61671050 Accuracy: Top1: 77.9851%, Top5: 98.9130% | [Test] Loss: 0.71371654 Accuracy: Top1: 75.2289%, Top5: 98.3977%\n",
      "[Epoch 030] Time: 7.4209s | [Train] Loss: 0.61549091 Accuracy: Top1: 78.1650%, Top5: 98.9190% | [Test] Loss: 0.71989415 Accuracy: Top1: 75.3483%, Top5: 98.2484%\n",
      "[Epoch 031] Time: 7.3601s | [Train] Loss: 0.61210783 Accuracy: Top1: 78.1989%, Top5: 98.9830% | [Test] Loss: 0.71948219 Accuracy: Top1: 75.1791%, Top5: 98.4275%\n",
      "[Epoch 032] Time: 7.3444s | [Train] Loss: 0.61072890 Accuracy: Top1: 78.2908%, Top5: 98.9250% | [Test] Loss: 0.71637510 Accuracy: Top1: 74.9900%, Top5: 98.4972%\n",
      "[Epoch 033] Time: 7.6475s | [Train] Loss: 0.60729785 Accuracy: Top1: 78.4227%, Top5: 98.8871% | [Test] Loss: 0.71549547 Accuracy: Top1: 75.3881%, Top5: 98.3977%\n",
      "[Epoch 034] Time: 7.1057s | [Train] Loss: 0.60696019 Accuracy: Top1: 78.4507%, Top5: 98.9430% | [Test] Loss: 0.71493618 Accuracy: Top1: 75.5673%, Top5: 98.4375%\n",
      "[Epoch 035] Time: 6.7241s | [Train] Loss: 0.60227166 Accuracy: Top1: 78.4647%, Top5: 99.0109% | [Test] Loss: 0.71605019 Accuracy: Top1: 75.4976%, Top5: 98.3479%\n",
      "[Epoch 036] Time: 6.8735s | [Train] Loss: 0.60460998 Accuracy: Top1: 78.4747%, Top5: 98.8891% | [Test] Loss: 0.71532110 Accuracy: Top1: 75.4379%, Top5: 98.4176%\n",
      "[Epoch 037] Time: 6.9146s | [Train] Loss: 0.60028432 Accuracy: Top1: 78.5606%, Top5: 98.9410% | [Test] Loss: 0.71803814 Accuracy: Top1: 75.2289%, Top5: 98.3181%\n",
      "[Epoch 038] Time: 6.8302s | [Train] Loss: 0.59715707 Accuracy: Top1: 78.7544%, Top5: 98.9530% | [Test] Loss: 0.71273442 Accuracy: Top1: 75.5673%, Top5: 98.4574%\n",
      "[Epoch 039] Time: 6.9691s | [Train] Loss: 0.59672345 Accuracy: Top1: 78.6825%, Top5: 98.9850% | [Test] Loss: 0.71440490 Accuracy: Top1: 75.7066%, Top5: 98.4475%\n",
      "[Epoch 040] Time: 6.8542s | [Train] Loss: 0.59616565 Accuracy: Top1: 78.8503%, Top5: 98.9890% | [Test] Loss: 0.71448741 Accuracy: Top1: 75.3384%, Top5: 98.4275%\n",
      "[Epoch 041] Time: 7.2660s | [Train] Loss: 0.58507213 Accuracy: Top1: 79.1141%, Top5: 99.0289% | [Test] Loss: 0.70917522 Accuracy: Top1: 75.4479%, Top5: 98.4674%\n",
      "[Epoch 042] Time: 6.9422s | [Train] Loss: 0.58694666 Accuracy: Top1: 79.3638%, Top5: 99.0509% | [Test] Loss: 0.70926215 Accuracy: Top1: 75.7066%, Top5: 98.4076%\n",
      "[Epoch 043] Time: 6.7939s | [Train] Loss: 0.58122309 Accuracy: Top1: 79.3618%, Top5: 99.0509% | [Test] Loss: 0.71011528 Accuracy: Top1: 75.5175%, Top5: 98.4275%\n",
      "[Epoch 044] Time: 7.0005s | [Train] Loss: 0.57965512 Accuracy: Top1: 79.3898%, Top5: 99.0070% | [Test] Loss: 0.71016322 Accuracy: Top1: 75.7066%, Top5: 98.4375%\n",
      "[Epoch 045] Time: 7.6769s | [Train] Loss: 0.58150227 Accuracy: Top1: 79.2459%, Top5: 99.0309% | [Test] Loss: 0.71000879 Accuracy: Top1: 75.6469%, Top5: 98.3778%\n",
      "[Epoch 046] Time: 7.3512s | [Train] Loss: 0.58060758 Accuracy: Top1: 79.4138%, Top5: 99.0429% | [Test] Loss: 0.71011498 Accuracy: Top1: 75.6369%, Top5: 98.4375%\n",
      "[Epoch 047] Time: 7.3776s | [Train] Loss: 0.58101070 Accuracy: Top1: 79.2020%, Top5: 99.0529% | [Test] Loss: 0.70963098 Accuracy: Top1: 75.6668%, Top5: 98.4375%\n",
      "[Epoch 048] Time: 6.7145s | [Train] Loss: 0.58388078 Accuracy: Top1: 79.3658%, Top5: 98.9950% | [Test] Loss: 0.70980814 Accuracy: Top1: 75.5573%, Top5: 98.4176%\n",
      "[Epoch 049] Time: 7.3997s | [Train] Loss: 0.58223913 Accuracy: Top1: 79.0741%, Top5: 99.0409% | [Test] Loss: 0.70979860 Accuracy: Top1: 75.5971%, Top5: 98.4275%\n",
      "[Epoch 050] Time: 6.6628s | [Train] Loss: 0.58152986 Accuracy: Top1: 79.3478%, Top5: 99.0609% | [Test] Loss: 0.71012155 Accuracy: Top1: 75.5573%, Top5: 98.4574%\n",
      "[Epoch 051] Time: 6.8128s | [Train] Loss: 0.57785703 Accuracy: Top1: 79.4178%, Top5: 99.0309% | [Test] Loss: 0.70979830 Accuracy: Top1: 75.6469%, Top5: 98.4475%\n",
      "[Epoch 052] Time: 6.8480s | [Train] Loss: 0.58292656 Accuracy: Top1: 79.3678%, Top5: 98.9970% | [Test] Loss: 0.70942608 Accuracy: Top1: 75.6170%, Top5: 98.4574%\n",
      "[Epoch 053] Time: 6.8628s | [Train] Loss: 0.58121077 Accuracy: Top1: 79.2759%, Top5: 99.0409% | [Test] Loss: 0.71046062 Accuracy: Top1: 75.6469%, Top5: 98.4375%\n",
      "[Epoch 054] Time: 6.9572s | [Train] Loss: 0.58219267 Accuracy: Top1: 79.2799%, Top5: 98.9870% | [Test] Loss: 0.70956150 Accuracy: Top1: 75.7464%, Top5: 98.3977%\n",
      "[Epoch 055] Time: 6.8269s | [Train] Loss: 0.58352766 Accuracy: Top1: 79.2559%, Top5: 99.0529% | [Test] Loss: 0.70998686 Accuracy: Top1: 75.6768%, Top5: 98.4176%\n",
      "[Epoch 056] Time: 6.9895s | [Train] Loss: 0.57976496 Accuracy: Top1: 79.3658%, Top5: 99.0649% | [Test] Loss: 0.70922716 Accuracy: Top1: 75.7564%, Top5: 98.3977%\n",
      "[Epoch 057] Time: 6.9536s | [Train] Loss: 0.57805046 Accuracy: Top1: 79.4457%, Top5: 98.9690% | [Test] Loss: 0.71006228 Accuracy: Top1: 75.5971%, Top5: 98.4475%\n",
      "[Epoch 058] Time: 7.4976s | [Train] Loss: 0.57765220 Accuracy: Top1: 79.3278%, Top5: 99.1129% | [Test] Loss: 0.71045992 Accuracy: Top1: 75.6270%, Top5: 98.3678%\n",
      "[Epoch 059] Time: 8.0858s | [Train] Loss: 0.57925632 Accuracy: Top1: 79.2779%, Top5: 99.0529% | [Test] Loss: 0.70972210 Accuracy: Top1: 75.6469%, Top5: 98.4176%\n",
      "[Epoch 060] Time: 9.9557s | [Train] Loss: 0.58001837 Accuracy: Top1: 79.3558%, Top5: 99.1009% | [Test] Loss: 0.70957660 Accuracy: Top1: 75.6568%, Top5: 98.4275%\n",
      "[Epoch 061] Time: 9.6888s | [Train] Loss: 0.57785179 Accuracy: Top1: 79.4577%, Top5: 99.0070% | [Test] Loss: 0.70979614 Accuracy: Top1: 75.6867%, Top5: 98.4574%\n",
      "[Epoch 062] Time: 9.4634s | [Train] Loss: 0.57730228 Accuracy: Top1: 79.5436%, Top5: 99.0030% | [Test] Loss: 0.70979364 Accuracy: Top1: 75.6967%, Top5: 98.4574%\n",
      "[Epoch 063] Time: 9.4182s | [Train] Loss: 0.57891955 Accuracy: Top1: 79.3678%, Top5: 98.9910% | [Test] Loss: 0.70990064 Accuracy: Top1: 75.7166%, Top5: 98.4475%\n",
      "[Epoch 064] Time: 9.5694s | [Train] Loss: 0.57830994 Accuracy: Top1: 79.4837%, Top5: 99.0589% | [Test] Loss: 0.70975502 Accuracy: Top1: 75.6867%, Top5: 98.4475%\n",
      "[Epoch 065] Time: 9.5220s | [Train] Loss: 0.57958609 Accuracy: Top1: 79.4317%, Top5: 99.0489% | [Test] Loss: 0.70978038 Accuracy: Top1: 75.7066%, Top5: 98.4475%\n",
      "[Epoch 066] Time: 9.3570s | [Train] Loss: 0.57539813 Accuracy: Top1: 79.5316%, Top5: 99.0249% | [Test] Loss: 0.70980600 Accuracy: Top1: 75.7365%, Top5: 98.4275%\n",
      "[Epoch 067] Time: 9.3538s | [Train] Loss: 0.57619984 Accuracy: Top1: 79.6016%, Top5: 99.0249% | [Test] Loss: 0.70989034 Accuracy: Top1: 75.7663%, Top5: 98.4275%\n",
      "[Epoch 068] Time: 9.5428s | [Train] Loss: 0.57975051 Accuracy: Top1: 79.4637%, Top5: 99.0629% | [Test] Loss: 0.70979781 Accuracy: Top1: 75.7166%, Top5: 98.4475%\n",
      "[Epoch 069] Time: 9.5318s | [Train] Loss: 0.57640270 Accuracy: Top1: 79.4277%, Top5: 99.0429% | [Test] Loss: 0.70984491 Accuracy: Top1: 75.7265%, Top5: 98.4375%\n",
      "[Epoch 070] Time: 9.3532s | [Train] Loss: 0.57789177 Accuracy: Top1: 79.5356%, Top5: 99.0050% | [Test] Loss: 0.70990601 Accuracy: Top1: 75.7464%, Top5: 98.4275%\n",
      "[Epoch 071] Time: 9.2555s | [Train] Loss: 0.57858851 Accuracy: Top1: 79.2799%, Top5: 99.0909% | [Test] Loss: 0.70992429 Accuracy: Top1: 75.7166%, Top5: 98.4275%\n",
      "[Epoch 072] Time: 9.3872s | [Train] Loss: 0.57447831 Accuracy: Top1: 79.5676%, Top5: 99.0729% | [Test] Loss: 0.70997737 Accuracy: Top1: 75.7166%, Top5: 98.4375%\n",
      "[Epoch 073] Time: 8.9660s | [Train] Loss: 0.57638921 Accuracy: Top1: 79.3778%, Top5: 99.0489% | [Test] Loss: 0.70993687 Accuracy: Top1: 75.7265%, Top5: 98.4375%\n",
      "[Epoch 074] Time: 9.1872s | [Train] Loss: 0.57779615 Accuracy: Top1: 79.3898%, Top5: 99.0869% | [Test] Loss: 0.70990983 Accuracy: Top1: 75.7265%, Top5: 98.4375%\n",
      "[Epoch 075] Time: 8.2955s | [Train] Loss: 0.57767063 Accuracy: Top1: 79.6755%, Top5: 99.0489% | [Test] Loss: 0.70992672 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 076] Time: 8.1129s | [Train] Loss: 0.57674101 Accuracy: Top1: 79.4777%, Top5: 99.0409% | [Test] Loss: 0.70991201 Accuracy: Top1: 75.7166%, Top5: 98.4375%\n",
      "[Epoch 077] Time: 8.1855s | [Train] Loss: 0.57709914 Accuracy: Top1: 79.4617%, Top5: 99.1029% | [Test] Loss: 0.70995901 Accuracy: Top1: 75.7564%, Top5: 98.4275%\n",
      "[Epoch 078] Time: 7.9316s | [Train] Loss: 0.57772087 Accuracy: Top1: 79.4118%, Top5: 99.0369% | [Test] Loss: 0.70981943 Accuracy: Top1: 75.7066%, Top5: 98.4176%\n",
      "[Epoch 079] Time: 7.8581s | [Train] Loss: 0.57734242 Accuracy: Top1: 79.6076%, Top5: 99.0070% | [Test] Loss: 0.70990187 Accuracy: Top1: 75.6967%, Top5: 98.4375%\n",
      "[Epoch 080] Time: 8.2311s | [Train] Loss: 0.57773670 Accuracy: Top1: 79.4178%, Top5: 99.0629% | [Test] Loss: 0.70996549 Accuracy: Top1: 75.7464%, Top5: 98.4475%\n",
      "[Epoch 081] Time: 8.1871s | [Train] Loss: 0.57535550 Accuracy: Top1: 79.6016%, Top5: 99.0449% | [Test] Loss: 0.70996615 Accuracy: Top1: 75.7464%, Top5: 98.4475%\n",
      "[Epoch 082] Time: 8.2490s | [Train] Loss: 0.57419960 Accuracy: Top1: 79.4238%, Top5: 99.1109% | [Test] Loss: 0.70997063 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 083] Time: 8.1045s | [Train] Loss: 0.57773105 Accuracy: Top1: 79.4977%, Top5: 99.0549% | [Test] Loss: 0.70995813 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 084] Time: 8.0897s | [Train] Loss: 0.57511048 Accuracy: Top1: 79.5916%, Top5: 99.0829% | [Test] Loss: 0.70996424 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 085] Time: 8.0484s | [Train] Loss: 0.57460230 Accuracy: Top1: 79.4218%, Top5: 99.0229% | [Test] Loss: 0.70996635 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 086] Time: 8.1736s | [Train] Loss: 0.57474903 Accuracy: Top1: 79.5316%, Top5: 99.0349% | [Test] Loss: 0.70996751 Accuracy: Top1: 75.7166%, Top5: 98.4475%\n",
      "[Epoch 087] Time: 7.2419s | [Train] Loss: 0.57851888 Accuracy: Top1: 79.4098%, Top5: 98.9750% | [Test] Loss: 0.70995882 Accuracy: Top1: 75.7464%, Top5: 98.4475%\n",
      "[Epoch 088] Time: 7.3209s | [Train] Loss: 0.57443591 Accuracy: Top1: 79.4198%, Top5: 99.0649% | [Test] Loss: 0.70995294 Accuracy: Top1: 75.7166%, Top5: 98.4375%\n",
      "[Epoch 089] Time: 7.4336s | [Train] Loss: 0.57587722 Accuracy: Top1: 79.4797%, Top5: 99.0729% | [Test] Loss: 0.70995349 Accuracy: Top1: 75.7166%, Top5: 98.4375%\n",
      "[Epoch 090] Time: 7.4186s | [Train] Loss: 0.57782096 Accuracy: Top1: 79.3398%, Top5: 99.0169% | [Test] Loss: 0.70995374 Accuracy: Top1: 75.7265%, Top5: 98.4475%\n",
      "[Epoch 091] Time: 7.4302s | [Train] Loss: 0.57627653 Accuracy: Top1: 79.4637%, Top5: 99.0969% | [Test] Loss: 0.70994752 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 092] Time: 7.5211s | [Train] Loss: 0.57448667 Accuracy: Top1: 79.3278%, Top5: 99.0969% | [Test] Loss: 0.70994649 Accuracy: Top1: 75.7365%, Top5: 98.4475%\n",
      "[Epoch 093] Time: 7.6111s | [Train] Loss: 0.58070362 Accuracy: Top1: 79.2399%, Top5: 99.0969% | [Test] Loss: 0.70993760 Accuracy: Top1: 75.7166%, Top5: 98.4475%\n",
      "[Epoch 094] Time: 7.5497s | [Train] Loss: 0.57392678 Accuracy: Top1: 79.5916%, Top5: 99.0729% | [Test] Loss: 0.70994137 Accuracy: Top1: 75.7166%, Top5: 98.4475%\n",
      "[Epoch 095] Time: 7.4063s | [Train] Loss: 0.57748661 Accuracy: Top1: 79.6156%, Top5: 99.0169% | [Test] Loss: 0.70992635 Accuracy: Top1: 75.6967%, Top5: 98.4475%\n",
      "[Epoch 096] Time: 7.4233s | [Train] Loss: 0.57469585 Accuracy: Top1: 79.4397%, Top5: 99.1089% | [Test] Loss: 0.70992272 Accuracy: Top1: 75.7365%, Top5: 98.4375%\n",
      "[Epoch 097] Time: 7.4431s | [Train] Loss: 0.57955550 Accuracy: Top1: 79.2199%, Top5: 99.0669% | [Test] Loss: 0.70991350 Accuracy: Top1: 75.7166%, Top5: 98.4375%\n",
      "[Epoch 098] Time: 7.3650s | [Train] Loss: 0.57801855 Accuracy: Top1: 79.4138%, Top5: 99.0309% | [Test] Loss: 0.70990568 Accuracy: Top1: 75.7166%, Top5: 98.4475%\n",
      "[Epoch 099] Time: 7.4563s | [Train] Loss: 0.57726920 Accuracy: Top1: 79.2919%, Top5: 99.0649% | [Test] Loss: 0.70991377 Accuracy: Top1: 75.7166%, Top5: 98.4375%\n",
      "[Epoch 100] Time: 7.3979s | [Train] Loss: 0.57943307 Accuracy: Top1: 79.4497%, Top5: 99.0589% | [Test] Loss: 0.70991483 Accuracy: Top1: 75.7365%, Top5: 98.4375%\n"
     ]
    }
   ],
   "source": [
    "# Check if the output directory exists, if not create it\n",
    "if args.output_dir:\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset \n",
    "if args.dataset == \"cifar10\":\n",
    "    dataset = CIFAR10(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"cifar100\":\n",
    "    dataset = CIFAR100(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size \n",
    "elif args.dataset == \"imagenet\":\n",
    "    dataset = ImageNet(args)\n",
    "    args.num_classes = dataset.num_classes \n",
    "    args.img_size = dataset.img_size\n",
    "else:\n",
    "    raise ValueError(\"Dataset not supported\")\n",
    "\n",
    "# Model \n",
    "model = AllConvNet(args)\n",
    "print(f\"Model: {model.name}\")\n",
    "\n",
    "# Parameters\n",
    "total_params, trainable_params = model.parameter_count()\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\")\n",
    "args.total_params = total_params\n",
    "args.trainable_params = trainable_params\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "# Training Modules \n",
    "train_eval_results = Train_Eval(args, \n",
    "                            model, \n",
    "                            dataset.train_loader, \n",
    "                            dataset.test_loader\n",
    "                            )\n",
    "\n",
    "# Storing Results in output directory \n",
    "write_to_file(os.path.join(args.output_dir, \"args.txt\"), args)\n",
    "write_to_file(os.path.join(args.output_dir, \"model.txt\"), model)\n",
    "write_to_file(os.path.join(args.output_dir, \"train_eval_results.txt\"), train_eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
